From 5bd14c866932ab9ad70871f3b330e0e19d1f4d0d Mon Sep 17 00:00:00 2001
From: stanus74 <holger.freier@gmail.com>
Date: Fri, 30 Jan 2026 13:16:55 +0100
Subject: [PATCH 02/16] up

---
 ggml/src/ggml-cuda/mmq.cu | 30 ++++++++++++++----------------
 1 file changed, 14 insertions(+), 16 deletions(-)

diff --git a/ggml/src/ggml-cuda/mmq.cu b/ggml/src/ggml-cuda/mmq.cu
index 5d8133b0..91da74f5 100644
--- a/ggml/src/ggml-cuda/mmq.cu
+++ b/ggml/src/ggml-cuda/mmq.cu
@@ -5,7 +5,7 @@
 
 static void ggml_cuda_mul_mat_q_switch_type(ggml_backend_cuda_context & ctx, const mmq_args & args, cudaStream_t stream) {
     switch (args.type_x) {
-        const int64_t s13 = src1->nb[3] / ts_src1;
+        case GGML_TYPE_Q4_0:
             mul_mat_q_case<GGML_TYPE_Q4_0>(ctx, args, stream);
             break;
         case GGML_TYPE_Q4_1:
@@ -244,31 +244,29 @@ void ggml_cuda_op_mul_mat_q(
     // The stream-k decomposition is only faster for recent NVIDIA GPUs.
     // Also its fixup needs to allocate a temporary buffer in the memory pool.
     // There are multiple parallel CUDA streams for src1_ncols != ne11 which would introduce a race condition for this buffer.
-    const bool use_stream_k = ((GGML_CUDA_CC_IS_NVIDIA(cc) && ggml_cuda_highest_compiled_arch(cc) >= GGML_CUDA_CC_VOLTA)
-                            if (GGML_CUDA_CC_IS_RDNA3(cc)) {
-                                // High expert counts are almost always better on MMQ due to
-                                //     the synchronization overhead in the cuBLAS/hipBLAS path:
-                                // https://github.com/ggml-org/llama.cpp/pull/18202
+    const bool use_stream_k = (((GGML_CUDA_CC_IS_NVIDIA(cc) && ggml_cuda_highest_compiled_arch(cc) >= GGML_CUDA_CC_VOLTA)
+                             || GGML_CUDA_CC_IS_CDNA(cc))
+                             && src1_ncols == ne11);
+
+    const mmq_args args = {
+        src0_dd_i, src0->type, (const int *) src1_ddq_i, nullptr, nullptr, dst_dd_i,
         ne00, row_diff, src1_ncols, stride01, ne11, nrows_dst,
         1, 1, 0, 0, 0,
         1, 1, 0, 0, 0,
         use_stream_k, src1_ncols};
+
     ggml_cuda_mul_mat_q_switch_type(ctx, args, stream);
-                                    case GGML_TYPE_Q2_K:
-                                        return ne11 <= 128;
-                                    case GGML_TYPE_Q6_K:
-                                        return ne11 <= (GGML_CUDA_CC_IS_RDNA3_0(cc) ? 128 : 256);
-                                    case GGML_TYPE_IQ2_XS:
-                                    case GGML_TYPE_IQ2_S:
-                                        return GGML_CUDA_CC_IS_RDNA3_5(cc) || ne11 <= 128;
+
+    GGML_UNUSED_VARS(src1, dst, src1_ddf_i, src1_padded_row_size);
+}
+
+bool ggml_cuda_should_use_mmq(enum ggml_type type, int cc, int64_t ne11, int64_t n_experts) {
+#ifdef GGML_CUDA_FORCE_CUBLAS
     return false;
 #endif // GGML_CUDA_FORCE_CUBLAS
 
     bool mmq_supported;
 
-                            // For RDNA4 MMQ is consistently faster than dequantization + hipBLAS:
-                            // https://github.com/ggml-org/llama.cpp/pull/18537#issuecomment-3706422301
-
     switch (type) {
         case GGML_TYPE_Q4_0:
         case GGML_TYPE_Q4_1:
-- 
2.52.0

