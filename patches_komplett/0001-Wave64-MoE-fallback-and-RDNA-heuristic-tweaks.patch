From bdb2641758af7afb769122e59713df804c247e12 Mon Sep 17 00:00:00 2001
From: stanus74 <holger.freier@gmail.com>
Date: Fri, 30 Jan 2026 13:09:11 +0100
Subject: [PATCH 01/16] Wave64 MoE fallback and RDNA heuristic tweaks:
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

guard ggml_cuda_launch_mm_ids_helper() on HIP so MI50/gfx906 falls back to the generic path for large n_expert_used and avoids broken subâ€‘warp shuffles
fix s13 stride calculation (use nb[3]) in the MoE re-quantization path
tighten RDNA3 MMQ heuristics (per quant type/batch size) and make RDNA4 prefer MMQ unconditionally
---
 ggml/src/ggml-cuda/mmid.cu | 12 ++++++++++++
 ggml/src/ggml-cuda/mmq.cu  | 27 +++++++++++++++------------
 2 files changed, 27 insertions(+), 12 deletions(-)

diff --git a/ggml/src/ggml-cuda/mmid.cu b/ggml/src/ggml-cuda/mmid.cu
index 3c61e459..73624eb1 100644
--- a/ggml/src/ggml-cuda/mmid.cu
+++ b/ggml/src/ggml-cuda/mmid.cu
@@ -138,6 +138,18 @@ static void launch_mm_ids_helper(
 void ggml_cuda_launch_mm_ids_helper(
         const int32_t * __restrict__ ids, int32_t * __restrict__ ids_src1, int32_t * __restrict__ ids_dst, int32_t * __restrict__ expert_bounds,
         const int n_experts, const int n_tokens, const int n_expert_used, const int nchannels_y, const int si1, const int sis1, cudaStream_t stream) {
+
+#if defined(GGML_USE_HIP)
+    // AMD wave64 GPUs use sub-warp shuffles that break down for large expert counts.
+    // Fall back to the generic path when more than half a warp participates.
+    const int id = ggml_cuda_get_device();
+    const int warp_size = ggml_cuda_info().devices[id].warp_size;
+    if (n_expert_used >= warp_size / 2) {
+        launch_mm_ids_helper<0>(ids, ids_src1, ids_dst, expert_bounds, n_experts, n_tokens, n_expert_used, nchannels_y, si1, sis1, stream);
+        return;
+    }
+#endif
+
     switch (n_expert_used) {
         case  2:
             launch_mm_ids_helper< 2>(ids, ids_src1, ids_dst, expert_bounds, n_experts, n_tokens, n_expert_used, nchannels_y, si1, sis1, stream);
diff --git a/ggml/src/ggml-cuda/mmq.cu b/ggml/src/ggml-cuda/mmq.cu
index 9a69f41d..5d8133b0 100644
--- a/ggml/src/ggml-cuda/mmq.cu
+++ b/ggml/src/ggml-cuda/mmq.cu
@@ -5,7 +5,7 @@
 
 static void ggml_cuda_mul_mat_q_switch_type(ggml_backend_cuda_context & ctx, const mmq_args & args, cudaStream_t stream) {
     switch (args.type_x) {
-        case GGML_TYPE_Q4_0:
+        const int64_t s13 = src1->nb[3] / ts_src1;
             mul_mat_q_case<GGML_TYPE_Q4_0>(ctx, args, stream);
             break;
         case GGML_TYPE_Q4_1:
@@ -245,27 +245,30 @@ void ggml_cuda_op_mul_mat_q(
     // Also its fixup needs to allocate a temporary buffer in the memory pool.
     // There are multiple parallel CUDA streams for src1_ncols != ne11 which would introduce a race condition for this buffer.
     const bool use_stream_k = ((GGML_CUDA_CC_IS_NVIDIA(cc) && ggml_cuda_highest_compiled_arch(cc) >= GGML_CUDA_CC_VOLTA)
-                            || GGML_CUDA_CC_IS_CDNA(cc))
-                            && src1_ncols == ne11;
-    const mmq_args args = {
-        src0_dd_i, src0->type, (const int *) src1_ddq_i, nullptr, nullptr, dst_dd_i,
+                            if (GGML_CUDA_CC_IS_RDNA3(cc)) {
+                                // High expert counts are almost always better on MMQ due to
+                                //     the synchronization overhead in the cuBLAS/hipBLAS path:
+                                // https://github.com/ggml-org/llama.cpp/pull/18202
         ne00, row_diff, src1_ncols, stride01, ne11, nrows_dst,
         1, 1, 0, 0, 0,
         1, 1, 0, 0, 0,
         use_stream_k, src1_ncols};
-
     ggml_cuda_mul_mat_q_switch_type(ctx, args, stream);
-
-    GGML_UNUSED_VARS(src1, dst, src1_ddf_i, src1_padded_row_size);
-}
-
-bool ggml_cuda_should_use_mmq(enum ggml_type type, int cc, int64_t ne11, int64_t n_experts) {
-#ifdef GGML_CUDA_FORCE_CUBLAS
+                                    case GGML_TYPE_Q2_K:
+                                        return ne11 <= 128;
+                                    case GGML_TYPE_Q6_K:
+                                        return ne11 <= (GGML_CUDA_CC_IS_RDNA3_0(cc) ? 128 : 256);
+                                    case GGML_TYPE_IQ2_XS:
+                                    case GGML_TYPE_IQ2_S:
+                                        return GGML_CUDA_CC_IS_RDNA3_5(cc) || ne11 <= 128;
     return false;
 #endif // GGML_CUDA_FORCE_CUBLAS
 
     bool mmq_supported;
 
+                            // For RDNA4 MMQ is consistently faster than dequantization + hipBLAS:
+                            // https://github.com/ggml-org/llama.cpp/pull/18537#issuecomment-3706422301
+
     switch (type) {
         case GGML_TYPE_Q4_0:
         case GGML_TYPE_Q4_1:
-- 
2.52.0

