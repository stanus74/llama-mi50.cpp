diff --git a/SCRIPT_compile_MI50.sh b/SCRIPT_compile_MI50.sh
new file mode 100755
index 000000000..f9ecda128
--- /dev/null
+++ b/SCRIPT_compile_MI50.sh
@@ -0,0 +1,203 @@
+#!/bin/bash
+cat << 'EOF'
+
+   ██╗     ██╗      █████╗ ███╗   ███╗ █████╗    ██████╗██████╗ ██████╗
+   ██║     ██║     ██╔══██╗████╗ ████║██╔══██╗  ██╔════╝██╔══██╗██╔══██╗
+   ██║     ██║     ███████║██╔████╔██║███████║  ██║     ██████╔╝██████╔╝
+   ██║     ██║     ██╔══██║██║╚██╔╝██║██╔══██║  ██║     ██╔═══╝ ██╔═══╝
+   ███████╗███████╗██║  ██║██║ ╚═╝ ██║██║  ██║  ╚██████╗██║     ██║
+   ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝     ╚═╝╚═╝  ╚═╝   ╚═════╝╚═╝     ╚═╝
+            ██████╗ ███████╗██╗  ██╗ █████╗  ██████╗  ██████╗
+           ██╔════╝ ██╔════╝╚██╗██╔╝██╔══██╗██╔═████╗██╔════╝
+           ██║  ███╗█████╗   ╚███╔╝ ╚██████║██║██╔██║███████╗
+           ██║   ██║██╔══╝   ██╔██╗  ╚═══██║████╔╝██║██╔═══██╗
+           ╚██████╔╝██║     ██╔╝ ██╗ █████╔╝╚██████╔╝╚██████╔╝
+            ╚═════╝ ╚═╝     ╚═╝  ╚═╝ ╚════╝  ╚═════╝  ╚═════╝           
+
+
+EOF
+
+set -e
+
+# 1. Check location
+[[ ! -f "CMakeLists.txt" ]] && echo "Error: Not in llama.cpp root directory" && exit 1
+
+# 2. Setup ROCm Environment Variables
+export ROCM_PATH=${ROCM_PATH:-/opt/rocm}
+export HIP_PATH=$ROCM_PATH
+export HIP_PLATFORM=amd
+export HIP_CLANG_PATH=$ROCM_PATH/llvm/bin
+export PATH=$ROCM_PATH/bin:$ROCM_PATH/llvm/bin:$PATH
+export LD_LIBRARY_PATH=$ROCM_PATH/lib:$ROCM_PATH/lib64:$ROCM_PATH/llvm/lib:${LD_LIBRARY_PATH:-}
+
+if command -v amdgpu-arch &> /dev/null; then
+    AMDGPU_ARCH=$(amdgpu-arch | head -n 1)
+    echo "Detected AMD GPU Architecture: $AMDGPU_ARCH"
+else
+    echo "Warning: amdgpu-arch tool not found. Defaulting to 'native'."
+    AMDGPU_ARCH="native"
+fi
+
+rm -rf build && mkdir -p build && cd build
+
+# ============================================================================
+# CMAKE FLAGS DOCUMENTATION 
+# ============================================================================
+# 
+#  CMAKE BUILD CONFIGURATION ===                                            
+#
+# CMAKE_BUILD_TYPE                 Build type: Release, Debug, RelWithDebInfo, MinSizeRel
+# CMAKE_C_COMPILER                 C compiler path (use ROCm clang for HIP)
+# CMAKE_CXX_COMPILER               C++ compiler path (use ROCm clang++ for HIP)
+# CMAKE_HIP_ARCHITECTURES          Target GPU arch: gfx906 (MI50/60)
+# CMAKE_HIP_COMPILER_FORCED=1      Skip HIP compiler detection. Fixes bfloat16 duplicate symbol
+#                                  error on ROCm 6.x (hip_bf16.h multi-include bug) - see issue 
+#
+#  COMPILER FLAGS ===                                                             
+#
+# -O3                              Maximum optimization level
+# -march=native                    Optimize for host CPU architecture
+# -mtune=native                    Tune instruction scheduling for host CPU
+# -DNDEBUG                         Disable assert() checks (release mode)
+# -Wno-ignored-attributes          Suppress CUDA __host__/__device__ attribute warnings
+# -Wno-cuda-compat                 Suppress CUDA compatibility warnings in HIP
+# -Wno-unused-result               Suppress unused return value warnings
+#
+#  GGML GENERAL OPTIONS ===                                                       
+#
+# GGML_STATIC=OFF                  Static link libraries (ON=static, OFF=shared/dynamic)
+# GGML_NATIVE=ON                   Enable CPU-native optimizations (AVX, AVX2, etc)
+# GGML_LTO=OFF                     Link Time Optimization (slower build, faster binary)
+# GGML_CCACHE=ON                   Use ccache for faster rebuilds if available
+# GGML_OPENMP=ON                   Enable OpenMP for CPU parallelization
+# GGML_CPU=ON                      Enable CPU backend
+# GGML_CPU_HBM=OFF                 Use memkind for High Bandwidth Memory (HBM)
+# GGML_CPU_REPACK=ON               Runtime weight conversion Q4_0 -> Q4_X_X
+# GGML_BACKEND_DL=OFF              Build backends as dynamic libraries
+# GGML_SCHED_NO_REALLOC=OFF        Disable reallocations in ggml-alloc (debug)
+#
+#  CPU SIMD INSTRUCTION SETS ===                                                  
+#
+# GGML_SSE42=ON                    Enable SSE 4.2 instructions
+# GGML_AVX=ON                      Enable AVX instructions
+# GGML_AVX2=ON                     Enable AVX2 instructions
+# GGML_AVX_VNNI=OFF                Enable AVX-VNNI (Alder Lake+)
+# GGML_AVX512=OFF                  Enable AVX-512F instructions
+# GGML_AVX512_VBMI=OFF             Enable AVX-512 VBMI
+# GGML_AVX512_VNNI=OFF             Enable AVX-512 VNNI
+# GGML_AVX512_BF16=OFF             Enable AVX-512 BF16
+# GGML_FMA=ON                      Enable FMA (Fused Multiply-Add)
+# GGML_F16C=ON                     Enable F16C (half-float conversions)
+# GGML_BMI2=ON                     Enable BMI2 bit manipulation
+# GGML_AMX_TILE=OFF                Enable Intel AMX tile instructions
+# GGML_AMX_INT8=OFF                Enable Intel AMX INT8
+# GGML_AMX_BF16=OFF                Enable Intel AMX BF16
+#
+#  AMD HIP/ROCm BACKEND ===                                                       
+#
+# GGML_HIP=ON                      Enable AMD ROCm/HIP backend
+# GGML_HIP_GRAPHS=OFF              Use HIP graphs for kernel batching (experimental)
+# GGML_HIP_NO_VMM=ON               Disable Virtual Memory Management (required for MI50)
+# GGML_HIP_ROCWMMA_FATTN=OFF       Use rocWMMA for Flash Attention (CDNA2+ only)
+# GGML_HIP_MMQ_MFMA=ON             Use MFMA matrix instructions for MMQ (CDNA GPUs)
+# GGML_HIP_EXPORT_METRICS=OFF      Export kernel performance metrics
+# GGML_HIP_NO_HIPBLASLT=OFF        Disable hipBLASLt (enable if crashes on your ROCm)
+#
+#  NVIDIA CUDA BACKEND ===                                                        
+#
+# GGML_CUDA=OFF                    Enable NVIDIA CUDA backend
+# GGML_CUDA_FORCE_MMQ=OFF          Force MMQ kernels instead of cuBLAS
+# GGML_CUDA_FORCE_CUBLAS=OFF       Force cuBLAS instead of MMQ kernels
+# GGML_CUDA_NO_PEER_COPY=OFF       Disable peer-to-peer GPU copies (multi-GPU)
+# GGML_CUDA_NO_VMM=OFF             Disable CUDA Virtual Memory Management
+# GGML_CUDA_GRAPHS=ON              Use CUDA graphs for kernel batching
+#
+#  FLASH ATTENTION  ===                                                           
+#
+# GGML_CUDA_FA=ON                  Enable Flash Attention CUDA/HIP kernels
+# GGML_CUDA_FA_ALL_QUANTS=OFF      Compile FA for all quant types (Q4, Q5, Q8, etc)
+#                                  ON = slower build, supports all quants
+#                                  OFF = faster build, only F16 FA
+#
+#  OTHER GPU BACKENDS ===                                                         
+#
+# GGML_VULKAN=OFF                  Enable Vulkan backend (cross-platform GPU)
+# GGML_VULKAN_DEBUG=OFF            Enable Vulkan debug output
+# GGML_VULKAN_VALIDATE=OFF         Enable Vulkan validation layers
+# GGML_METAL=OFF                   Enable Apple Metal backend (macOS/iOS)
+# GGML_METAL_EMBED_LIBRARY=ON      Embed Metal shaders in binary
+# GGML_SYCL=OFF                    Enable Intel SYCL backend (oneAPI)
+# GGML_OPENCL=OFF                  Enable OpenCL backend (Adreno GPUs)
+# GGML_MUSA=OFF                    Enable Moore Threads MUSA backend
+# GGML_WEBGPU=OFF                  Enable WebGPU backend (browsers)
+# GGML_RPC=OFF                     Enable RPC for distributed inference
+#
+#  OTHER ACCELERATORS ===                                                         
+#
+# GGML_BLAS=OFF                    Use BLAS library (OpenBLAS, MKL, etc)
+# GGML_ACCELERATE=ON               Use Apple Accelerate framework (macOS)
+# GGML_LLAMAFILE=ON                Use llamafile SGEMM kernels
+# GGML_HEXAGON=OFF                 Enable Qualcomm Hexagon DSP backend
+# GGML_ZENDNN=OFF                  Enable AMD ZenDNN for Zen CPUs
+# GGML_ZDNN=OFF                    Enable IBM zDNN for Z mainframes
+#
+#  LLAMA.CPP BUILD TARGETS ===                                                    
+#
+# LLAMA_BUILD_SERVER=ON            Build llama-server (OpenAI-compatible HTTP API)
+# LLAMA_BUILD_EXAMPLES=ON          Build example programs (simple, batched, etc)
+# LLAMA_BUILD_TOOLS=ON             Build tools (quantize, bench, perplexity, etc)
+# LLAMA_BUILD_TESTS=OFF            Build test suite (slower, for development)
+# LLAMA_BUILD_COMMON=ON            Build common utilities library
+# LLAMA_TOOLS_INSTALL=ON           Install tools to system
+#
+#  LLAMA.CPP FEATURES ===                                                         
+#
+# LLAMA_CURL=ON                    Enable libcurl for HuggingFace downloads (-hf flag)
+# LLAMA_HTTPLIB=ON                 Use cpp-httplib if curl disabled
+# LLAMA_OPENSSL=OFF                Use OpenSSL for HTTPS support
+# LLAMA_LLGUIDANCE=OFF             Include LLGuidance for structured output
+#
+#  DEBUG & SANITIZERS ===                                                         
+#
+# GGML_ALL_WARNINGS=ON             Enable all compiler warnings
+# GGML_FATAL_WARNINGS=OFF          Treat warnings as errors (-Werror)
+# GGML_SANITIZE_THREAD=OFF         Enable ThreadSanitizer (race detection)
+# GGML_SANITIZE_ADDRESS=OFF        Enable AddressSanitizer (memory errors)
+# GGML_SANITIZE_UNDEFINED=OFF      Enable UndefinedBehaviorSanitizer
+# GGML_GPROF=OFF                   Enable gprof profiling
+#
+# ============================================================================
+
+{
+cmake .. \
+    -DCMAKE_BUILD_TYPE=Release \
+    -DCMAKE_C_COMPILER="$ROCM_PATH"/llvm/bin/clang \
+    -DCMAKE_CXX_COMPILER="$ROCM_PATH"/llvm/bin/clang++ \
+    -DCMAKE_HIP_ARCHITECTURES="$AMDGPU_ARCH" \
+    -DCMAKE_HIP_COMPILER_FORCED=1 \
+    -DCMAKE_C_FLAGS="-O3 -march=native -mtune=native -DNDEBUG -ffast-math -fno-finite-math-only -ffp-contract=fast" \
+    -DCMAKE_CXX_FLAGS="-O3 -march=native -mtune=native -DNDEBUG" \
+    -DCMAKE_HIP_FLAGS="-Wno-ignored-attributes -Wno-cuda-compat -Wno-unused-result" \
+    -DGGML_HIP=ON \
+    -DGGML_HIP_GRAPHS=ON \
+    -DGGML_HIP_NO_VMM=ON \
+    -DGGML_HIP_EXPORT_METRICS=ON \
+    -DGGML_NATIVE=ON \
+    -DGGML_CUDA_FA=ON \
+    -DGGML_CUDA_FA_ALL_QUANTS=ON \
+    -DGGML_CUDA_FORCE_MMQ=OFF \
+    -DGGML_CUDA_FORCE_CUBLAS=OFF \
+    -DGGML_CUDA_NO_PEER_COPY=ON \
+    -DLLAMA_BUILD_SERVER=ON \
+    -DLLAMA_BUILD_EXAMPLES=ON \
+    -DLLAMA_BUILD_TOOLS=ON \
+    -DLLAMA_BUILD_TESTS=OFF \
+    -DLLAMA_CURL=ON \
+    -DLLAMA_STATIC=OFF
+
+
+make -j"$(nproc)"
+
+echo ""
+echo "Build complete: ./build/bin/llama-cli, llama-server, llama-bench"
+} 2>&1 | tee ../compilation_log.txt
diff --git a/SCRIPT_launch_server_MI50.sh b/SCRIPT_launch_server_MI50.sh
new file mode 100755
index 000000000..bec450520
--- /dev/null
+++ b/SCRIPT_launch_server_MI50.sh
@@ -0,0 +1,71 @@
+#!/bin/bash
+# shellcheck disable=SC1143,SC2215
+cat << 'EOF'
+
+   ██╗     ██╗      █████╗ ███╗   ███╗ █████╗    ██████╗██████╗ ██████╗
+   ██║     ██║     ██╔══██╗████╗ ████║██╔══██╗  ██╔════╝██╔══██╗██╔══██╗
+   ██║     ██║     ███████║██╔████╔██║███████║  ██║     ██████╔╝██████╔╝
+   ██║     ██║     ██╔══██║██║╚██╔╝██║██╔══██║  ██║     ██╔═══╝ ██╔═══╝
+   ███████╗███████╗██║  ██║██║ ╚═╝ ██║██║  ██║  ╚██████╗██║     ██║
+   ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝     ╚═╝╚═╝  ╚═╝   ╚═════╝╚═╝     ╚═╝
+            ██████╗ ███████╗██╗  ██╗ █████╗  ██████╗  ██████╗
+           ██╔════╝ ██╔════╝╚██╗██╔╝██╔══██╗██╔═████╗██╔════╝
+           ██║  ███╗█████╗   ╚███╔╝ ╚██████║██║██╔██║███████╗
+           ██║   ██║██╔══╝   ██╔██╗  ╚═══██║████╔╝██║██╔═══██╗
+           ╚██████╔╝██║     ██╔╝ ██╗ █████╔╝╚██████╔╝╚██████╔╝
+            ╚═════╝ ╚═╝     ╚═╝  ╚═╝ ╚════╝  ╚═════╝  ╚═════╝            
+
+
+EOF
+
+export HSA_OVERRIDE_GFX_VERSION=9.0.6
+export HIP_VISIBLE_DEVICES=0
+export CUDA_VISIBLE_DEVICES=0
+export ROCR_VISIBLE_DEVICES=0
+export GGML_BACKEND_HIP=1
+export HCC_AMDGPU_TARGET=gfx906
+
+# Model path 
+MODEL_PATH="/path/..."
+
+# Vision projector path (uncomment for multimodal models)
+#MMPROJ_PATH="/path/..."
+
+# Model path .................. -m
+# Vision projector ............ --mmproj
+# GPU layers (99 = all) ....... -ngl
+# Flash attention ............. -fa
+# KV cache key type ........... -ctk
+# KV cache value type ......... -ctv
+# Listen interface ............ --host
+# Server port ................. --port
+# Context size (tokens) ....... -c
+# Jinja templating ............ --jinja
+
+#ngram settings:
+#        --spec-type ngram-mod \
+#        --spec-ngram-size-n 24 \
+#        --draft-min 48 \
+#        --draft-max 64 \
+
+
+./build/bin/llama-server \
+    -m "$MODEL_PATH" \
+        #--spec-type ngram-mod \
+        #--spec-ngram-size-n 24 \
+        #--draft-min 48 \
+        #--draft-max 64 \
+    -ngl 99 \
+    -fa on \
+    -ctk f16 \
+    -ctv f16 \
+    --host 0.0.0.0 \
+    --port 8080 \
+    -c 800 \
+    -b 2048 \
+    -ub 2048 \
+    --jinja
+    # --mmproj "$MMPROJ_PATH"
+    
+    
+    
\ No newline at end of file
diff --git a/SCRIPT_llama_bench.sh b/SCRIPT_llama_bench.sh
new file mode 100755
index 000000000..bbfb2c793
--- /dev/null
+++ b/SCRIPT_llama_bench.sh
@@ -0,0 +1,59 @@
+#!/bin/bash
+cat << 'EOF'
+
+   ██╗     ██╗      █████╗ ███╗   ███╗ █████╗    ██████╗██████╗ ██████╗
+   ██║     ██║     ██╔══██╗████╗ ████║██╔══██╗  ██╔════╝██╔══██╗██╔══██╗
+   ██║     ██║     ███████║██╔████╔██║███████║  ██║     ██████╔╝██████╔╝
+   ██║     ██║     ██╔══██║██║╚██╔╝██║██╔══██║  ██║     ██╔═══╝ ██╔═══╝
+   ███████╗███████╗██║  ██║██║ ╚═╝ ██║██║  ██║  ╚██████╗██║     ██║
+   ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝     ╚═╝╚═╝  ╚═╝   ╚═════╝╚═╝     ╚═╝
+            ██████╗ ███████╗██╗  ██╗ █████╗  ██████╗  ██████╗
+           ██╔════╝ ██╔════╝╚██╗██╔╝██╔══██╗██╔═████╗██╔════╝
+           ██║  ███╗█████╗   ╚███╔╝ ╚██████║██║██╔██║███████╗
+           ██║   ██║██╔══╝   ██╔██╗  ╚═══██║████╔╝██║██╔═══██╗
+           ╚██████╔╝██║     ██╔╝ ██╗ █████╔╝╚██████╔╝╚██████╔╝
+            ╚═════╝ ╚═╝     ╚═╝  ╚═╝ ╚════╝  ╚═════╝  ╚═════╝
+
+
+EOF
+
+export HSA_OVERRIDE_GFX_VERSION=9.0.6
+export HIP_VISIBLE_DEVICES=0
+
+MODEL_PATH="/path/..."
+LOG_FILE="bench_results.md"
+
+
+BENCH_PARAMS=(
+    -m "$MODEL_PATH"       # Model path
+    -ngl 99                # Number of GPU layers (all on GPU)
+    -t "$(nproc)"          # Number of CPU threads
+    -fa 1                  # Flash attention (1=on, 0=off)
+    -ctk q8_0              # KV cache key type (q8_0 quantization)
+    -ctv f16               # KV cache value type (f16 precision)
+    --main-gpu 0           # Main GPU device ID
+    --progress             # Show progress during benchmark
+    -r 1                   # Number of repetitions
+    -b 2048                # Batch size
+    -ub 2048               # Micro-batch size
+    #-d 8192               # Context size
+    #-v                     # Verbose output (show llama.cpp internal logs)
+)
+
+
+#BENCH_TESTS="-p 0 -n 2048"
+#BENCH_TESTS="-p 2048 -n 0"
+BENCH_TESTS="-p 512,2048,8192 -n 1,128,2048"
+#BENCH_TESTS="-p 512 -n 128"
+
+echo "=== Benchmark ==="
+echo "Model: $(basename "$MODEL_PATH")"
+echo ""
+
+cd "$(dirname "$0")" || exit
+[ ! -f "./build/bin/llama-bench" ] && echo "Error: llama-bench not found" && exit 1
+
+./build/bin/llama-bench "${BENCH_PARAMS[@]}" "$BENCH_TESTS" "$@" 2>&1 | tee -a "$LOG_FILE"
+
+echo ""
+echo "Output saved to: $LOG_FILE"
diff --git a/SCRIPT_llama_bench2.sh b/SCRIPT_llama_bench2.sh
new file mode 100755
index 000000000..c72611a8e
--- /dev/null
+++ b/SCRIPT_llama_bench2.sh
@@ -0,0 +1,57 @@
+#!/bin/bash
+cat << 'EOF'
+
+   ██╗     ██╗      █████╗ ███╗   ███╗ █████╗    ██████╗██████╗ ██████╗
+   ██║     ██║     ██╔══██╗████╗ ████║██╔══██╗  ██╔════╝██╔══██╗██╔══██╗
+   ██║     ██║     ███████║██╔████╔██║███████║  ██║     ██████╔╝██████╔╝
+   ██║     ██║     ██╔══██║██║╚██╔╝██║██╔══██║  ██║     ██╔═══╝ ██╔═══╝
+   ███████╗███████╗██║  ██║██║ ╚═╝ ██║██║  ██║  ╚██████╗██║     ██║
+   ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝     ╚═╝╚═╝  ╚═╝   ╚═════╝╚═╝     ╚═╝
+            ██████╗ ███████╗██╗  ██╗ █████╗  ██████╗  ██████╗
+           ██╔════╝ ██╔════╝╚██╗██╔╝██╔══██╗██╔═████╗██╔════╝
+           ██║  ███╗█████╗   ╚███╔╝ ╚██████║██║██╔██║███████╗
+           ██║   ██║██╔══╝   ██╔██╗  ╚═══██║████╔╝██║██╔═══██╗
+           ╚██████╔╝██║     ██╔╝ ██╗ █████╔╝╚██████╔╝╚██████╔╝
+            ╚═════╝ ╚═╝     ╚═╝  ╚═╝ ╚════╝  ╚═════╝  ╚═════╝
+
+
+EOF
+
+export HSA_OVERRIDE_GFX_VERSION=9.0.6
+export HIP_VISIBLE_DEVICES=0
+MODEL_PATH="/media/iacoppbk/80F42C9BF42C96061/llms/Qwen3-VL-30B-A3B-Thinking-Q4_1.gguf"
+
+LOG_FILE="bench_results.md"
+
+
+BENCH_PARAMS=(
+    -m "$MODEL_PATH"       # Model path
+    -ngl 99                # Number of GPU layers (all on GPU)
+    -t "$(nproc)"          # Number of CPU threads
+    -fa 1                  # Flash attention (1=on, 0=off)
+    -ctk q8_0              # KV cache key type (q8_0 quantization)
+    -ctv f16               # KV cache value type (f16 precision)
+    --main-gpu 0           # Main GPU device ID
+    --progress             # Show progress during benchmark
+    -r 1                   # Number of repetitions
+    -b 2048                # Batch size
+    -ub 2048               # Micro-batch size
+    #-d 8192               # Context size 
+)
+
+
+#BENCH_TESTS="-p 0 -n 2048"
+#BENCH_TESTS="-p 2048 -n 0"
+BENCH_TESTS="-p 512 -n 1,128,2048"
+
+echo "=== Benchmark ==="
+echo "Model: $(basename "$MODEL_PATH")"
+echo ""
+
+cd "$(dirname "$0")" || exit
+[ ! -f "./build/bin/llama-bench" ] && echo "Error: llama-bench not found" && exit 1
+
+./build/bin/llama-bench "${BENCH_PARAMS[@]}" "$BENCH_TESTS" "$@" 2>&1 | tee -a "$LOG_FILE"
+
+echo ""
+echo "Output saved to: $LOG_FILE"
diff --git a/SCRIPT_overclock_upp_MI50.sh b/SCRIPT_overclock_upp_MI50.sh
new file mode 100755
index 000000000..edd3edd54
--- /dev/null
+++ b/SCRIPT_overclock_upp_MI50.sh
@@ -0,0 +1,45 @@
+#!/bin/bash
+cat << 'EOF'
+
+   ██╗     ██╗      █████╗ ███╗   ███╗ █████╗    ██████╗██████╗ ██████╗
+   ██║     ██║     ██╔══██╗████╗ ████║██╔══██╗  ██╔════╝██╔══██╗██╔══██╗
+   ██║     ██║     ███████║██╔████╔██║███████║  ██║     ██████╔╝██████╔╝
+   ██║     ██║     ██╔══██║██║╚██╔╝██║██╔══██║  ██║     ██╔═══╝ ██╔═══╝
+   ███████╗███████╗██║  ██║██║ ╚═╝ ██║██║  ██║  ╚██████╗██║     ██║
+   ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝     ╚═╝╚═╝  ╚═╝   ╚═════╝╚═╝     ╚═╝
+            ██████╗ ███████╗██╗  ██╗ █████╗  ██████╗  ██████╗
+           ██╔════╝ ██╔════╝╚██╗██╔╝██╔══██╗██╔═████╗██╔════╝
+           ██║  ███╗█████╗   ╚███╔╝ ╚██████║██║██╔██║███████╗
+           ██║   ██║██╔══╝   ██╔██╗  ╚═══██║████╔╝██║██╔═══██╗
+           ╚██████╔╝██║     ██╔╝ ██╗ █████╔╝╚██████╔╝╚██████╔╝
+            ╚═════╝ ╚═╝     ╚═╝  ╚═╝ ╚════╝  ╚═════╝  ╚═════╝                     
+
+EOF
+
+#This oveclocking script uses upp to tune clocks voltages and power limits.
+#Use at your own risk!
+
+MI50_POWER=225    #Power limit:do not go over 300W
+MI50_SCLK=2000    #Core clock speed
+MI50_MCLK=1100    #Memory clock speed
+MI50_TDC_GFX=260  #TDC limit for GFX (default 330)
+MI50_CARD=1       #For multi gpu config, choose your MI50!
+
+DEVICE="/sys/class/drm/card${MI50_CARD}/device"
+UPP_PYTHON="/home/iacoppbk/upp/bin/python3"
+
+[ ! -f "$UPP_PYTHON" ] && echo "Error: UPP not found at $UPP_PYTHON" && exit 1
+
+echo "Applying: Core=${MI50_SCLK}MHz, Mem=${MI50_MCLK}MHz, Power=${MI50_POWER}W, TDC_GFX=${MI50_TDC_GFX}"
+
+sudo "$UPP_PYTHON" -m upp.upp -p ${DEVICE}/pp_table set --write \
+    smcPPTable/SocketPowerLimitAc0=$MI50_POWER \
+    smcPPTable/SocketPowerLimitDc=$MI50_POWER \
+    smcPPTable/TdcLimitGfx=$MI50_TDC_GFX
+
+sudo "$UPP_PYTHON" -m upp.upp -p ${DEVICE}/pp_table set --write \
+    smcPPTable/FreqTableGfx/8=$MI50_SCLK \
+    smcPPTable/FreqTableUclk/2=$MI50_MCLK \
+    smcPPTable/FreqTableUclk/3=$MI50_MCLK
+
+echo "Done"
diff --git a/ggml/src/ggml-cuda/CMakeLists.txt b/ggml/src/ggml-cuda/CMakeLists.txt
index d313c1ac9..80e86ac2b 100644
--- a/ggml/src/ggml-cuda/CMakeLists.txt
+++ b/ggml/src/ggml-cuda/CMakeLists.txt
@@ -47,10 +47,7 @@ if (CUDAToolkit_FOUND)
                 #     check Modules/Internal/CMakeCUDAArchitecturesValidate.cmake in the CMake git repository instead.
                 # However, the architectures 120a-real and 121a-real should work with basically any CMake version and
                 #     until the release of e.g. Rubin there is no benefit to shipping virtual architectures for Blackwell.
-                list(APPEND CMAKE_CUDA_ARCHITECTURES 120a-real)
-            endif()
-            if (CUDAToolkit_VERSION VERSION_GREATER_EQUAL "12.9")
-                list(APPEND CMAKE_CUDA_ARCHITECTURES 121a-real)
+                list(APPEND CMAKE_CUDA_ARCHITECTURES 120a-real 121a-real)
             endif()
         endif()
     endif()
@@ -110,6 +107,12 @@ if (CUDAToolkit_FOUND)
     list(APPEND GGML_SOURCES_CUDA ${SRCS})
     file(GLOB   SRCS "template-instances/mmf*.cu")
     list(APPEND GGML_SOURCES_CUDA ${SRCS})
+    file(GLOB   SRCS "gfx906/fused/*.cu")
+    list(APPEND GGML_SOURCES_CUDA ${SRCS})
+    file(GLOB   SRCS "gfx906/attention/*.cu")
+    list(APPEND GGML_SOURCES_CUDA ${SRCS})
+    file(GLOB   SRCS "gfx906/attention/instances/*.cu")
+    list(APPEND GGML_SOURCES_CUDA ${SRCS})
 
     if (GGML_CUDA_FA_ALL_QUANTS)
         file(GLOB   SRCS "template-instances/fattn-vec*.cu")
diff --git a/ggml/src/ggml-cuda/add-id.cu b/ggml/src/ggml-cuda/add-id.cu
index 8d9cf692b..16047f488 100644
--- a/ggml/src/ggml-cuda/add-id.cu
+++ b/ggml/src/ggml-cuda/add-id.cu
@@ -1,13 +1,12 @@
 #include "add-id.cuh"
 
-static __global__ void add_id_kernel(
+static __global__ void add_id_kernel_reference(
         const float * src0, const float * src1, const int32_t * src2, float * dst,
         int64_t ne0, int64_t ne1,
         size_t nb01, size_t nb02,
         size_t nb11,
         size_t nb21
     ) {
-
     const int64_t i1 = blockIdx.x;
     const int64_t i2 = blockIdx.y;
 
@@ -25,6 +24,66 @@ static __global__ void add_id_kernel(
     }
 }
 
+static __global__ void add_id_kernel_vec4(
+        const float * __restrict__ src0,
+        const float * __restrict__ src1,
+        const int32_t * __restrict__ src2,
+        float * __restrict__ dst,
+        const int ne0,
+        const int ne01,
+        const int s0_stride,
+        const int s0_stride2,
+        const int s1_stride,
+        const int s2_stride
+    ) {
+    const int i1 = blockIdx.x;
+    const int i2 = blockIdx.y;
+
+    const int i11 = src2[i1 + i2 * s2_stride];
+
+    const int src0_offset = i1 * s0_stride + i2 * s0_stride2;
+    const int src1_offset = i11 * s1_stride;
+    const int dst_offset = i1 * ne0 + i2 * ne01 * ne0;
+
+    const float4 * __restrict__ src0_vec = reinterpret_cast<const float4 *>(src0 + src0_offset);
+    const float4 * __restrict__ src1_vec = reinterpret_cast<const float4 *>(src1 + src1_offset);
+    float4 * __restrict__ dst_vec = reinterpret_cast<float4 *>(dst + dst_offset);
+
+    const int ne0_vec = ne0 >> 2;
+
+    for (int i0 = threadIdx.x; i0 < ne0_vec; i0 += blockDim.x) {
+        const float4 a = src0_vec[i0];
+        const float4 b = src1_vec[i0];
+        dst_vec[i0] = make_float4(a.x + b.x, a.y + b.y, a.z + b.z, a.w + b.w);
+    }
+}
+
+static __global__ void add_id_kernel_contiguous(
+        const float * __restrict__ src0,
+        const float * __restrict__ src1,
+        const int32_t * __restrict__ src2,
+        float * __restrict__ dst,
+        const int ne0,
+        const int ne01,
+        const int s0_stride,
+        const int s0_stride2,
+        const int s1_stride,
+        const int s2_stride
+    ) {
+    const int i1 = blockIdx.x;
+    const int i2 = blockIdx.y;
+
+    const int i11 = src2[i1 + i2 * s2_stride];
+
+    const float * __restrict__ src0_row = src0 + i1 * s0_stride + i2 * s0_stride2;
+    const float * __restrict__ src1_row = src1 + i11 * s1_stride;
+    float * __restrict__ dst_row = dst + i1 * ne0 + i2 * ne01 * ne0;
+
+    for (int i0 = threadIdx.x; i0 < ne0; i0 += blockDim.x) {
+        dst_row[i0] = src0_row[i0] + src1_row[i0];
+    }
+}
+
 void ggml_cuda_op_add_id(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {
     const ggml_tensor * src0 = dst->src[0];
     const ggml_tensor * src1 = dst->src[1];
@@ -46,13 +105,49 @@ void ggml_cuda_op_add_id(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {
     const int32_t * src2_d = (const int32_t *)src2->data;
     float * dst_d = (float *)dst->data;
 
-    int threads = std::min((int)ne00, 768); // cols
-    dim3 blocks(ne01, ne02); // n_experts_used, n_tokens
-    add_id_kernel<<<blocks, threads, 0, ctx.stream()>>>(
-        src0_d, src1_d, src2_d, dst_d,
-        ne0, ne1,
-        nb01, nb02,
-        nb11,
-        nb21
-    );
+    cudaStream_t stream = ctx.stream();
+
+    const bool is_contiguous = (nb01 == ne00 * sizeof(float)) &&
+                               (nb11 == ne10 * sizeof(float));
+
+    const bool is_aligned = ((uintptr_t)src0_d % 16 == 0) &&
+                            ((uintptr_t)src1_d % 16 == 0) &&
+                            ((uintptr_t)dst_d  % 16 == 0);
+
+    const bool can_vectorize = is_contiguous && is_aligned && (ne00 % 4 == 0) && (ne00 <= INT_MAX);
+
+    const dim3 blocks(ne01, ne02);
+
+    if (can_vectorize) {
+        const int threads_vec4 = std::min((int)(ne00 / 4), 768);
+        add_id_kernel_vec4<<<blocks, threads_vec4, 0, stream>>>(
+            src0_d, src1_d, src2_d, dst_d,
+            (int)ne00,
+            (int)ne01,
+            (int)(nb01 / sizeof(float)),
+            (int)(nb02 / sizeof(float)),
+            (int)(nb11 / sizeof(float)),
+            (int)(nb21 / sizeof(int32_t))
+        );
+    } else if (is_contiguous && ne00 <= INT_MAX) {
+        const int threads = std::min((int)ne00, 768);
+        add_id_kernel_contiguous<<<blocks, threads, 0, stream>>>(
+            src0_d, src1_d, src2_d, dst_d,
+            (int)ne00,
+            (int)ne01,
+            (int)(nb01 / sizeof(float)),
+            (int)(nb02 / sizeof(float)),
+            (int)(nb11 / sizeof(float)),
+            (int)(nb21 / sizeof(int32_t))
+        );
+    } else {
+        const int threads = std::min((int)ne00, 768);
+        add_id_kernel_reference<<<blocks, threads, 0, stream>>>(
+            src0_d, src1_d, src2_d, dst_d,
+            ne0, ne1,
+            nb01, nb02,
+            nb11,
+            nb21
+        );
+    }
 }
diff --git a/ggml/src/ggml-cuda/common.cuh b/ggml/src/ggml-cuda/common.cuh
index a3256d59d..29d1853cf 100644
--- a/ggml/src/ggml-cuda/common.cuh
+++ b/ggml/src/ggml-cuda/common.cuh
@@ -27,8 +27,16 @@
 #include <cstdio>
 #include <string>
 #include <unordered_map>
+#include <unordered_set>
 #include <vector>
 
+#if defined(GGML_USE_HIP)
+#include "gfx906/gfx906-config.h"
+#if GFX906_KVQ_MOE_CACHE_ENABLED
+#include "gfx906/quantize/q8-cache.cuh"
+#endif
+#endif
+
 #if defined(GGML_USE_HIP)
 #include "vendors/hip.h"
 #elif defined(GGML_USE_MUSA)
@@ -402,6 +410,35 @@ struct ggml_cuda_unroll<1> {
     }
 };
 
+// ================================================================================================
+// AMD GFX906 DPP-based Warp Reductions
+// ================================================================================================
+// All AMD GFX906-specific DPP optimizations moved to gfx906/gfx906-common.cuh
+// ================================================================================================
+
+#ifdef GGML_USE_HIP
+    #include "gfx906/gfx906-common.cuh"
+#endif // GGML_USE_HIP
+
+// ============================================================================
+// Unified shuffle XOR operation - dispatches to DPP on AMD, shuffle on NVIDIA
+// ============================================================================
+template<int width = WARP_SIZE, typename T>
+static __device__ __forceinline__ T ggml_cuda_shfl_xor_sync(T x, int offset) {
+#if defined(GGML_USE_HIP)
+    switch (~offset) {
+        case ~1:  return hip_dpp_xor1(x);
+        case ~2:  return hip_dpp_xor2(x);
+        case ~4:  return hip_dpp_xor4(x);
+        case ~8:  return hip_dpp_xor8(x);
+        case ~16: return hip_dpp_xor16(x);
+        default:  return __shfl_xor(x, offset, width);
+    }
+#else
+    return __shfl_xor_sync(0xffffffff, x, offset, width);
+#endif
+}
+
 template<int width = WARP_SIZE>
 static __device__ __forceinline__ int warp_reduce_sum(int x) {
 #if !defined(GGML_USE_HIP) && __CUDA_ARCH__ >= GGML_CUDA_CC_AMPERE
@@ -409,7 +446,7 @@ static __device__ __forceinline__ int warp_reduce_sum(int x) {
 #else
 #pragma unroll
     for (int offset = width/2; offset > 0; offset >>= 1) {
-        x += __shfl_xor_sync(0xffffffff, x, offset, width);
+        x += ggml_cuda_shfl_xor_sync<width>(x, offset);
     }
     return x;
 #endif // !defined(GGML_USE_HIP) && __CUDA_ARCH__ >= GGML_CUDA_CC_AMPERE
@@ -417,19 +454,23 @@ static __device__ __forceinline__ int warp_reduce_sum(int x) {
 
 template<int width = WARP_SIZE>
 static __device__ __forceinline__ float warp_reduce_sum(float x) {
+#if defined(GGML_USE_HIP)
+    return warp_reduce_amd_f32<width, AddOp>(x);  // Use fused DPP instructions
+#else
 #pragma unroll
     for (int offset = width/2; offset > 0; offset >>= 1) {
-        x += __shfl_xor_sync(0xffffffff, x, offset, width);
+        x += ggml_cuda_shfl_xor_sync<width>(x, offset);
     }
     return x;
+#endif
 }
 
 template<int width = WARP_SIZE>
 static __device__ __forceinline__ float2 warp_reduce_sum(float2 a) {
 #pragma unroll
     for (int offset = width/2; offset > 0; offset >>= 1) {
-        a.x += __shfl_xor_sync(0xffffffff, a.x, offset, width);
-        a.y += __shfl_xor_sync(0xffffffff, a.y, offset, width);
+        a.x += ggml_cuda_shfl_xor_sync<width>(a.x, offset);
+        a.y += ggml_cuda_shfl_xor_sync<width>(a.y, offset);
     }
     return a;
 }
@@ -439,10 +480,9 @@ static __device__ __forceinline__ half2 warp_reduce_sum(half2 a) {
 #ifdef FP16_AVAILABLE
 #pragma unroll
     for (int offset = width/2; offset > 0; offset >>= 1) {
-        a = __hadd2(a, __shfl_xor_sync(0xffffffff, a, offset, width));
+        a = __hadd2(a, ggml_cuda_shfl_xor_sync<width>(a, offset));
     }
     return a;
-
 #else
     NO_DEVICE_CODE;
     return a;
@@ -456,7 +496,7 @@ static __device__ __forceinline__ int warp_reduce_all(int x) {
     } else {
 #pragma unroll
         for (int offset = width/2; offset > 0; offset >>= 1) {
-            x = __shfl_xor_sync(0xffffffff, x, offset, width) && x;
+            x = ggml_cuda_shfl_xor_sync<width>(x, offset) && x;
         }
         return x;
     }
@@ -469,7 +509,7 @@ static __device__ __forceinline__ int warp_reduce_any(int x) {
     } else {
 #pragma unroll
         for (int offset = width/2; offset > 0; offset >>= 1) {
-            x = __shfl_xor_sync(0xffffffff, x, offset, width) || x;
+            x = ggml_cuda_shfl_xor_sync<width>(x, offset) || x;
         }
         return x;
     }
@@ -477,11 +517,15 @@ static __device__ __forceinline__ int warp_reduce_any(int x) {
 
 template<int width = WARP_SIZE>
 static __device__ __forceinline__ float warp_reduce_max(float x) {
+#if defined(GGML_USE_HIP)
+    return warp_reduce_amd_f32<width, MaxOp>(x);  // Use fused DPP instructions
+#else
 #pragma unroll
     for (int offset = width/2; offset > 0; offset >>= 1) {
-        x = fmaxf(x, __shfl_xor_sync(0xffffffff, x, offset, width));
+        x = fmaxf(x, ggml_cuda_shfl_xor_sync<width>(x, offset));
     }
     return x;
+#endif
 }
 
 template<typename T, int width = WARP_SIZE>
@@ -645,7 +689,7 @@ static __device__ __forceinline__ half2 warp_reduce_max(half2 x) {
 #if !defined(GGML_USE_HIP) && __CUDA_ARCH__ >= GGML_CUDA_CC_PASCAL || defined(GGML_USE_HIP)
 #pragma unroll
    for (int offset = width/2; offset > 0; offset >>= 1) {
-       x = ggml_cuda_hmax2(x, __shfl_xor_sync(0xffffffff, x, offset, width));
+       x = ggml_cuda_hmax2(x, ggml_cuda_shfl_xor_sync<width>(x, offset));
    }
    return x;
 #else
@@ -785,6 +829,10 @@ static __device__ __forceinline__ float ggml_cuda_e8m0_to_fp32(uint8_t x) {
 #if CUDART_VERSION >= 12080
     const nv_bfloat16 e = __nv_cvt_e8m0_to_bf16raw(x);
     return (float) e;
+#elif defined(GGML_USE_HIP) && defined(__gfx906__)
+    // GFX906: Branchless with direct bit cast
+    const uint32_t bits = x ? ((uint32_t)x << 23) : 0x00400000u;
+    return __uint_as_float(bits);
 #else
     uint32_t bits;
     if (x == 0) {
@@ -1424,6 +1472,15 @@ struct ggml_backend_cuda_context {
     ggml_cuda_pool & pool() {
         return pool(device);
     }
+
+#if defined(GGML_USE_HIP) && GFX906_KVQ_MOE_CACHE_ENABLED
+    q8_hashmap_cache q8_cache;
+    std::unordered_map<const ggml_tensor*, prequantized_q8_info> fusion_prequant_map;
+    std::unordered_set<const ggml_tensor*> fusion_handled_mul_nodes;
+    std::vector<std::unique_ptr<ggml_cuda_pool_alloc<char>>> fusion_q8_buffers;
+
+    void clear_q8_cache() { q8_cache.clear(); }
+#endif
 };
 
 struct ggml_cuda_mm_fusion_args_host {
diff --git a/ggml/src/ggml-cuda/fattn-common.cuh b/ggml/src/ggml-cuda/fattn-common.cuh
index b6a7460da..4912f05e4 100644
--- a/ggml/src/ggml-cuda/fattn-common.cuh
+++ b/ggml/src/ggml-cuda/fattn-common.cuh
@@ -11,11 +11,9 @@
 #define SOFTMAX_FTZ_THRESHOLD -20.0f                   // Softmax exp. of values smaller than this are flushed to zero to avoid NaNs.
 
 // log(2) = 0.6931, by adding this to the KQ maximum used for the softmax the numerical range representable
-//     by the VKQ accumulators is effectively being shifted up by a factor of 2.
+//     by the VKQ accumulators is effectively being shifted up by a factor of 8.
 // This reduces issues with numerical overflow but also causes larger values to be flushed to zero.
 // However, as the output from FlashAttention will usually be used as an input for a matrix multiplication this should be negligible.
-// Still, the value range should be shifted as much as necessary but as little as possible.
-// The macro on the following line shifts it by a factor of 2**3=8, as was needed to fix https://github.com/ggml-org/llama.cpp/issues/18606 .
 #define FATTN_KQ_MAX_OFFSET (3.0f*0.6931f)
 
 typedef void (* fattn_kernel_t)(
@@ -276,8 +274,8 @@ static __device__ __forceinline__ void quantize_q8_1_to_shared(
     }
 #pragma unroll
     for (int mask = QI8_1/2; mask > 0; mask >>= 1) {
-        amax = fmaxf(amax, __shfl_xor_sync(0xFFFFFFFF, amax, mask, 32));
-        sum +=             __shfl_xor_sync(0xFFFFFFFF, sum,  mask, 32);
+        amax = fmaxf(amax, ggml_cuda_shfl_xor_sync<32>(amax, mask));
+        sum +=             ggml_cuda_shfl_xor_sync<32>(sum,  mask);
     }
 
     const float d = amax / 127;
@@ -965,6 +963,20 @@ void launch_fattn(
             }
         }
 
+        // AMD GFX906 optimization: Different Split-K for PP vs TG
+        const bool is_amd = !GGML_CUDA_CC_IS_NVIDIA(cc);
+        const bool is_prompt_processing = Q->ne[1] > 1;  // Q->ne[1] = num query tokens
+
+        if (is_amd) {
+            if (is_prompt_processing) {
+                // PP: Disable Split-K to avoid combine overhead
+                parallel_blocks = 1;
+            } else {
+                // TG: Use auto-tuned value for better SM utilization
+                // (parallel_blocks already set by auto-tuner)
+            }
+        }
+
         blocks_num.x = ntiles_x;
         blocks_num.y = parallel_blocks;
         blocks_num.z = ntiles_z_gqa*K->ne[2]*Q->ne[3];
diff --git a/ggml/src/ggml-cuda/fattn.cu b/ggml/src/ggml-cuda/fattn.cu
index 721edd999..d70bf55ef 100644
--- a/ggml/src/ggml-cuda/fattn.cu
+++ b/ggml/src/ggml-cuda/fattn.cu
@@ -6,6 +6,11 @@
 #include "fattn-wmma-f16.cuh"
 #include "fattn.cuh"
 
+// GFX906 Q8 Flash Attention kernel
+#ifdef GGML_USE_HIP
+    #include "gfx906/attention/fattn-q8.cuh"
+#endif
+
 template <int DKQ, int DV, int ncols2>
 static void ggml_cuda_flash_attn_ext_mma_f16_switch_ncols1(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {
     const int cc = ggml_cuda_info().devices[ggml_cuda_get_device()].cc;
@@ -275,6 +280,9 @@ enum best_fattn_kernel {
     BEST_FATTN_KERNEL_VEC      = 100,
     BEST_FATTN_KERNEL_WMMA_F16 = 300,
     BEST_FATTN_KERNEL_MMA_F16  = 400,
+#ifdef GGML_USE_HIP
+    BEST_FATTN_KERNEL_TILE_Q8  = 250,
+#endif
 };
 
 static best_fattn_kernel ggml_cuda_get_best_fattn_kernel(const int device, const ggml_tensor * dst) {
@@ -448,23 +456,49 @@ static best_fattn_kernel ggml_cuda_get_best_fattn_kernel(const int device, const
                     return BEST_FATTN_KERNEL_VEC;
                 }
             }
-        } else {
+        }
+#ifndef GGML_USE_HIP
+        else {
             if (Q->ne[1] <= 2) {
                 return BEST_FATTN_KERNEL_VEC;
             }
         }
+#endif
+    }
+
+#ifdef GGML_USE_HIP
+    if (K->type == GGML_TYPE_Q8_0 || V->type == GGML_TYPE_Q8_0) {
+        const bool q8_head_size_supported = (K->ne[0] % 32 == 0) &&
+                                            (K->ne[0] != 40) &&
+                                            (K->ne[0] != 80) &&
+                                            (K->ne[0] != 112) &&
+                                            (K->ne[0] != 576);
+
+        if (q8_head_size_supported) {
+            return BEST_FATTN_KERNEL_TILE_Q8;
+        }
     }
+#endif
+
     return BEST_FATTN_KERNEL_TILE;
 }
 
 void ggml_cuda_flash_attn_ext(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {
     ggml_cuda_set_device(ctx.device);
-    switch (ggml_cuda_get_best_fattn_kernel(ggml_cuda_get_device(), dst)) {
+
+    best_fattn_kernel kernel = ggml_cuda_get_best_fattn_kernel(ggml_cuda_get_device(), dst);
+
+    switch (kernel) {
         case BEST_FATTN_KERNEL_NONE:
             GGML_ABORT("fatal error");
         case BEST_FATTN_KERNEL_TILE:
             ggml_cuda_flash_attn_ext_tile(ctx, dst);
             break;
+#ifdef GGML_USE_HIP
+        case BEST_FATTN_KERNEL_TILE_Q8:
+            ggml_cuda_flash_attn_ext_tile_q8(ctx, dst);
+            break;
+#endif // GGML_USE_HIP
         case BEST_FATTN_KERNEL_VEC:
             ggml_cuda_flash_attn_ext_vec(ctx, dst);
             break;
diff --git a/ggml/src/ggml-cuda/gfx906/attention/fattn-q8.cu b/ggml/src/ggml-cuda/gfx906/attention/fattn-q8.cu
new file mode 100644
index 000000000..51e53f029
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/attention/fattn-q8.cu
@@ -0,0 +1,34 @@
+// Q8 Flash Attention dispatch: head sizes 64, 96, 128, 256, 576
+
+#include "../../common.cuh"
+#include "fattn-q8.cuh"
+
+void ggml_cuda_flash_attn_ext_tile_q8(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {
+    const ggml_tensor * K = dst->src[1];
+    const ggml_tensor * V = dst->src[2];
+    switch (K->ne[0]) {
+        case  64: {
+            GGML_ASSERT(V->ne[0] == K->ne[0]);
+            ggml_cuda_flash_attn_ext_tile_q8_case< 64,  64>(ctx, dst);
+        } break;
+        case  96: {
+            GGML_ASSERT(V->ne[0] == K->ne[0]);
+            ggml_cuda_flash_attn_ext_tile_q8_case< 96,  96>(ctx, dst);
+        } break;
+        case 128: {
+            GGML_ASSERT(V->ne[0] == K->ne[0]);
+            ggml_cuda_flash_attn_ext_tile_q8_case<128, 128>(ctx, dst);
+        } break;
+        case 256: {
+            GGML_ASSERT(V->ne[0] == K->ne[0]);
+            ggml_cuda_flash_attn_ext_tile_q8_case<256, 256>(ctx, dst);
+        } break;
+        case 576: {
+            GGML_ASSERT(V->ne[0] == 512);
+            ggml_cuda_flash_attn_ext_tile_q8_case<576, 512>(ctx, dst);
+        } break;
+        default: {
+            GGML_ABORT("Unsupported head size for Q8 tile kernel");
+        } break;
+    }
+}
diff --git a/ggml/src/ggml-cuda/gfx906/attention/fattn-q8.cuh b/ggml/src/ggml-cuda/gfx906/attention/fattn-q8.cuh
new file mode 100644
index 000000000..a6a7817a7
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/attention/fattn-q8.cuh
@@ -0,0 +1,1003 @@
+// Flash Attention with Q8_0 quantized KV cache for GFX906
+// Uses v_dot4_i32_i8 for INT8 dot products
+
+#include "../../common.cuh"
+#include "../../fattn-common.cuh"
+#include "../../cpy-utils.cuh"
+
+#include "../gfx906-config.h"
+#include "../gfx906-common.cuh"
+
+#define GGML_CUDA_FATTN_TILE_CONFIG_CASE(DKQ_, DV_, ncols_, nthreads, occupancy, nbatch_fa, nbatch_K) \
+    if (DKQ == (DKQ_) && DV == (DV_) && ncols == (ncols_)) {                                          \
+        static_assert((nthreads)          <= 512, "bad nthreads");                                    \
+        static_assert((occupancy)         <=   8, "bad occupancy");                                   \
+        static_assert((nbatch_fa)         <= 256, "bad nbatch_fa");                                   \
+        static_assert((nbatch_K)          <= 256, "bad nbatch_K");                                    \
+        return ((nthreads) << 0) | ((occupancy) << 10) | ((nbatch_fa) << 14) | ((nbatch_K) << 23);    \
+    }
+
+static constexpr __host__ __device__ uint32_t ggml_cuda_fattn_tile_q8_get_config_amd(const int DKQ, const int DV, const int ncols) {
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 40,  40,  2,  64, 2,  32,  40)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 40,  40,  4, 128, 2,  32,  40)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 40,  40,  8, 256, 2,  32,  40)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 40,  40, 16, 256, 2,  32,  40)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 40,  40, 32, 256, 2,  32,  40)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 40,  40, 64, 256, 2,  32,  40)
+
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64,  2,  64, 3,  32,  64)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64,  4, 128, 3,  64,  64)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64,  8, 128, 2,  32,  64)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 16, 256, 2, 128,  64)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 32, 256, 2,  64,  64)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 64, 256, 2,  64,  64)
+
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  2,  64, 2,  32,  40)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  4, 128, 2,  32,  40)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  8, 256, 2,  32,  40)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80, 16, 256, 2,  32,  40)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80, 32, 256, 2,  32,  40)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80, 64, 256, 2,  32,  40)
+
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 96,  96,  2,  64, 2,  32,  48)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 96,  96,  4, 128, 2,  32,  48)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 96,  96,  8, 256, 2,  32,  48)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 96,  96, 16, 256, 2,  32,  48)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 96,  96, 32, 256, 2,  32,  48)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 96,  96, 64, 256, 2,  32,  48)
+
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(112, 112,  2,  64, 2,  32,  56)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(112, 112,  4, 128, 2,  32,  56)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(112, 112,  8, 256, 2,  32,  56)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(112, 112, 16, 256, 2,  32,  56)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(112, 112, 32, 256, 2,  32,  56)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(112, 112, 64, 256, 2,  32,  56)
+
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(128, 128,  2, 256, 2, 128,  64)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(128, 128,  4, 128, 2,  64, 128)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(128, 128,  8, 256, 2,  64, 128)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(128, 128, 16, 256, 2,  64, 128)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(128, 128, 32, 256, 2,  64,  64)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(128, 128, 64, 256, 2,  64,  64)
+
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(256, 256,  2, 256, 2, 128,  64)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(256, 256,  4, 256, 2,  64, 128)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(256, 256,  8, 256, 2,  64, 128)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(256, 256, 16, 256, 2,  32, 128)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(256, 256, 32, 256, 2,  32, 128)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(576, 512, 16, 256, 2,  64,  64)
+    GGML_CUDA_FATTN_TILE_CONFIG_CASE(576, 512, 32, 512, 1, 128,  64)
+
+    return 0;
+}
+
+static __host__ uint32_t ggml_cuda_fattn_tile_q8_get_config(const int DKQ, const int DV, const int ncols, const int cc) {
+    return ggml_cuda_fattn_tile_q8_get_config_amd(DKQ, DV, ncols);
+}
+
+static constexpr __device__ uint32_t ggml_cuda_fattn_tile_q8_get_config(const int DKQ, const int DV, const int ncols) {
+    return ggml_cuda_fattn_tile_q8_get_config_amd(DKQ, DV, ncols);
+}
+
+static __host__ int ggml_cuda_fattn_tile_q8_get_nthreads(const int DKQ, const int DV, const int ncols, const int cc) {
+    return (ggml_cuda_fattn_tile_q8_get_config(DKQ, DV, ncols, cc) >> 0) & ((1 << 10) - 1);
+}
+
+static constexpr __device__ int ggml_cuda_fattn_tile_q8_get_nthreads(const int DKQ, const int DV, const int ncols) {
+    return (ggml_cuda_fattn_tile_q8_get_config(DKQ, DV, ncols) >> 0) & ((1 << 10) - 1);
+}
+
+static __host__ int ggml_cuda_fattn_tile_q8_get_occupancy(const int DKQ, const int DV, const int ncols, const int cc) {
+    return (ggml_cuda_fattn_tile_q8_get_config(DKQ, DV, ncols, cc) >> 10) & ((1 << 4) - 1);
+}
+
+static constexpr __device__ int ggml_cuda_fattn_tile_q8_get_occupancy(const int DKQ, const int DV, const int ncols) {
+    return (ggml_cuda_fattn_tile_q8_get_config(DKQ, DV, ncols) >> 10) & ((1 << 4) - 1);
+}
+
+static __host__ int ggml_cuda_fattn_tile_q8_get_nbatch_fa(const int DKQ, const int DV, const int ncols, const int cc) {
+    return (ggml_cuda_fattn_tile_q8_get_config(DKQ, DV, ncols, cc) >> 14) & ((1 << 9) - 1);
+}
+
+static constexpr __device__ int ggml_cuda_fattn_tile_q8_get_nbatch_fa(const int DKQ, const int DV, const int ncols) {
+    return (ggml_cuda_fattn_tile_q8_get_config(DKQ, DV, ncols) >> 14) & ((1 << 9) - 1);
+}
+
+static __host__ int ggml_cuda_fattn_tile_q8_get_nbatch_K(const int DKQ, const int DV, const int ncols, const int cc) {
+    return (ggml_cuda_fattn_tile_q8_get_config(DKQ, DV, ncols, cc) >> 23) & ((1 << 9) - 1);
+}
+
+static constexpr __device__ int ggml_cuda_fattn_tile_q8_get_nbatch_K(const int DKQ, const int DV, const int ncols) {
+    return (ggml_cuda_fattn_tile_q8_get_config(DKQ, DV, ncols) >> 23) & ((1 << 9) - 1);
+}
+
+template<int warp_size, int nwarps, int I, int J, int J_padding, bool oob_check>
+static __device__ __forceinline__ void flash_attn_tile_q8_q8_load_tile(
+        const half2 * const __restrict__ KV, half2 * const __restrict__ tile_KV, const int stride_KV, const int i_sup) {
+    constexpr int cpy_nb = ggml_cuda_get_max_cpy_bytes();
+    constexpr int cpy_ne = cpy_nb / 4;
+
+    auto load = [&] __device__ (const int n) {
+        const int stride_j = warp_size >> n;
+
+        if (stride_j == 0) {
+            return;
+        }
+
+        const int j0_start = stride_j == warp_size ? 0 : ((J/2)/cpy_ne) - ((J/2)/cpy_ne) % (2*stride_j);
+        const int j0_stop  =                             ((J/2)/cpy_ne) - ((J/2)/cpy_ne) % (1*stride_j);
+        const int stride_i = warp_size / stride_j;
+
+        if (j0_start == j0_stop) {
+            return;
+        }
+
+#pragma unroll
+        for (int i0 = 0; i0 < I; i0 += nwarps*stride_i) {
+            const int i = i0 + threadIdx.y*stride_i + (stride_j == warp_size ? 0 : threadIdx.x / stride_j);
+
+            if (i0 + nwarps*stride_i <= I || i < I) {
+#pragma unroll
+                for (int j0 = j0_start; j0 < j0_stop; j0 += stride_j) {
+                    const int j = j0*cpy_ne + (stride_j == warp_size ? threadIdx.x : threadIdx.x % stride_j)*cpy_ne;
+
+                    const half2 zero[cpy_ne] = {{0.0f, 0.0f}};
+                    ggml_cuda_memcpy_1<cpy_nb>(
+                        tile_KV + i*(J/2 + J_padding) + j,
+                        !oob_check || i < i_sup ? KV + i*stride_KV + j : zero);
+                }
+            }
+        }
+    };
+
+    static_assert(J % 8 == 0, "bad J");
+    static_assert((J/2) % cpy_ne == 0, "bad J");
+    ggml_cuda_unroll<7>{}(load);
+}
+
+template<int warp_size, int nwarps, int I, int J, int K_row_stride, bool oob_check>
+static __device__ __forceinline__ void flash_attn_tile_q8_q8_load_tile_q8(
+        const block_q8_0 * const __restrict__ K_q8,
+        int8_t * const __restrict__ K_values,
+        half * const __restrict__ K_scales,
+        const int stride_K_q8,
+        const int i_sup) {
+
+    if constexpr (J > 0) {
+        constexpr int blocks_per_row = J / 32;
+        const int tid = threadIdx.y * blockDim.x + threadIdx.x;
+        const int total_blocks = I * blocks_per_row;
+
+        for (int block_idx = tid; block_idx < total_blocks; block_idx += blockDim.x * blockDim.y) {
+            const int row = block_idx / blocks_per_row;
+            const int col_block = block_idx % blocks_per_row;
+
+            if (oob_check && row >= i_sup) {
+                break;
+            }
+
+            const int global_block_idx = row * stride_K_q8 + col_block;
+            const block_q8_0 src_block = K_q8[global_block_idx];
+
+            K_scales[col_block * I + row] = src_block.d;
+
+            int8_t * dst = K_values + row * K_row_stride + col_block * 32;
+            const int4* src_int4 = (const int4*)src_block.qs;
+            int4* dst_int4 = (int4*)dst;
+
+            dst_int4[0] = src_int4[0];
+            dst_int4[1] = src_int4[1];
+        }
+
+        __syncthreads();
+    }
+}
+
+template<int nthreads, int ncols, int ncols2, int DKQ>
+static __device__ __forceinline__ void flash_attn_tile_q8_quantize_Q_to_shared(
+        const float * __restrict__ Q_f,
+        int8_t * __restrict__ Q_values,
+        half * __restrict__ Q_scales,
+        const int col_Q_0,
+        const int ne01,
+        const int head0,
+        const int ne02,
+        const int32_t nb01,
+        const int32_t nb02,
+        const float scale) {
+
+    constexpr int blocks_per_col = DKQ / QK8_0;
+    const int tid = threadIdx.y * blockDim.x + threadIdx.x;
+    const int total_blocks = ncols * blocks_per_col;
+
+    for (int block_idx = tid; block_idx < total_blocks; block_idx += blockDim.x * blockDim.y) {
+        const int col = block_idx / blocks_per_col;
+        const int col_block = block_idx % blocks_per_col;
+        const int block_start = col_block * QK8_0;
+
+        const int jc = col;
+        const int j = jc / ncols2;
+        const int c = jc % ncols2;
+
+        // Bounds check for non-power-of-2 GQA ratios
+        // ncols1 = ncols / ncols2
+        if ((ncols > ncols2 && col_Q_0 + j >= ne01) || (ncols2 > 1 && head0 + c >= ne02)) {
+            continue;
+        }
+
+        float Q_vals[QK8_0];
+        block_q8_0 Q_block;
+
+        const int base_offset = c*(nb02/sizeof(float)) + j*(nb01/sizeof(float)) + block_start;
+
+        // Use float4 vectorized loads for better memory bandwidth (8x float4 = 32 floats)
+        // DKQ is always multiple of 32 (static_assert at line 622), so no bounds check needed
+        const float4* Q_f4 = reinterpret_cast<const float4*>(Q_f + base_offset);
+        float4* Q_vals4 = reinterpret_cast<float4*>(Q_vals);
+
+        #pragma unroll
+        for (int i = 0; i < QK8_0/4; i++) {
+            float4 tmp = Q_f4[i];
+            tmp.x *= scale;
+            tmp.y *= scale;
+            tmp.z *= scale;
+            tmp.w *= scale;
+            Q_vals4[i] = tmp;
+        }
+
+        quantize_f32_q8_0_block(Q_vals, &Q_block);
+
+        Q_scales[col_block * ncols + jc] = Q_block.d;
+
+        int8_t * dst = Q_values + jc * DKQ + col_block * 32;
+        const int4* src_int4 = (const int4*)Q_block.qs;
+        int4* dst_int4 = (int4*)dst;
+
+        dst_int4[0] = src_int4[0];
+        dst_int4[1] = src_int4[1];
+    }
+
+    __syncthreads();
+}
+
+template <int warp_size, int nwarps, int ncols1, int ncols2, int DKQ, int nbatch_fa, int nbatch_K,
+    bool oob_check>
+static __device__ __forceinline__ void flash_attn_tile_q8_q8_iter_KQ(
+        int8_t * const Q_values,
+        half * const Q_scales,
+        const block_q8_0 * const __restrict__ K_q8,
+        int8_t * const K_values,
+        half * const K_scales,
+        const int stride_K_q8,
+        const int k_VKQ_0,
+        const int k_VKQ_sup,
+        const int k_KQ_0,
+        float * KQ_acc) {
+    constexpr int ncols = ncols1*ncols2;
+    constexpr int cpw   = ncols > nwarps ? ncols/nwarps : 1;
+    constexpr int np    = nwarps > ncols ? nwarps/ncols : 1;
+
+    constexpr int K_row_stride = nbatch_K + 16;
+
+    flash_attn_tile_q8_q8_load_tile_q8<warp_size, nwarps, nbatch_fa, nbatch_K, K_row_stride, oob_check>
+        (K_q8 + int64_t(k_VKQ_0)*stride_K_q8 + (k_KQ_0/32), K_values, K_scales, stride_K_q8, k_VKQ_sup);
+    __syncthreads();
+
+    static_assert(nbatch_K % 4 == 0, "nbatch_K must be multiple of 4 for sdot4");
+
+    constexpr int blocks_per_K_row = nbatch_K / 32;
+
+    #pragma unroll 4
+    for (int jc0 = 0; jc0 < cpw; ++jc0) {
+        const int jc = jc0 + (threadIdx.y / np)*cpw;
+
+        #pragma unroll 4
+        for (int i_KQ_0 = 0; i_KQ_0 < nbatch_fa; i_KQ_0 += np*warp_size) {
+            const int i_KQ = i_KQ_0 + (threadIdx.y % np)*warp_size + threadIdx.x;
+            const int idx = i_KQ_0/(np*warp_size)*cpw + jc0;
+
+            const int8_t* K_row_base = K_values + i_KQ * K_row_stride;
+
+            #pragma unroll 4
+            for (int block_id = 0; block_id < blocks_per_K_row; block_id++) {
+                const int4* K_ptr4 = (const int4*)(K_row_base + block_id * 32);
+                const int4* Q_ptr4 = (const int4*)&Q_values[jc * DKQ + k_KQ_0 + block_id * 32];
+
+                int acc_int = 0;
+                {
+                    const int4 K_lo = K_ptr4[0];
+                    const int4 Q_lo = Q_ptr4[0];
+                    acc_int = ggml_cuda_dp4a(K_lo.x, Q_lo.x, acc_int);
+                    acc_int = ggml_cuda_dp4a(K_lo.y, Q_lo.y, acc_int);
+                    acc_int = ggml_cuda_dp4a(K_lo.z, Q_lo.z, acc_int);
+                    acc_int = ggml_cuda_dp4a(K_lo.w, Q_lo.w, acc_int);
+                }
+                {
+                    const int4 K_hi = K_ptr4[1];
+                    const int4 Q_hi = Q_ptr4[1];
+                    acc_int = ggml_cuda_dp4a(K_hi.x, Q_hi.x, acc_int);
+                    acc_int = ggml_cuda_dp4a(K_hi.y, Q_hi.y, acc_int);
+                    acc_int = ggml_cuda_dp4a(K_hi.z, Q_hi.z, acc_int);
+                    acc_int = ggml_cuda_dp4a(K_hi.w, Q_hi.w, acc_int);
+                }
+
+                const half combined_scale_h = __hmul(
+                    K_scales[block_id * nbatch_fa + i_KQ],
+                    Q_scales[((k_KQ_0/32) + block_id) * ncols + jc]);
+                KQ_acc[idx] += __half2float(combined_scale_h) * (float)acc_int;
+            }
+        }
+    }
+
+    if (k_KQ_0 + nbatch_K < DKQ) {
+        __syncthreads();
+    }
+}
+
+template <int warp_size, int nwarps, int ncols1, int ncols2, int DKQ, int DV, int nbatch_fa, int nbatch_K,
+    bool use_logit_softcap, bool oob_check, typename T_KQ, typename T_acc>
+static __device__ __forceinline__ void flash_attn_tile_q8_q8_iter(
+        int8_t * const Q_values,
+        half * const Q_scales,
+        const block_q8_0 * const __restrict__ K_q8,
+        const half2 * const __restrict__ V_h2,
+        const half  * const __restrict__ mask,
+        const float logit_softcap,
+        const float slope,
+        T_KQ      * const KQ,
+        int8_t * const K_values,
+        half * const K_scales,
+        half2 * const V_tmp,
+        const int stride_K_q8,
+        const int stride_V2,
+        const int stride_mask,
+        float * const KQ_max,
+        float * const KQ_sum,
+        T_acc * const VKQ,
+        const int k_VKQ_0,
+        const int k_VKQ_max) {
+    constexpr int cpy_ne = ggml_cuda_get_max_cpy_bytes() / 4;
+
+    constexpr int ncols = ncols1*ncols2;
+    constexpr int cpw   = ncols > nwarps ? ncols/nwarps : 1;
+    constexpr int np    = nwarps > ncols ? nwarps/ncols : 1;
+
+    constexpr int DVp = (DV + 2*warp_size - 1) & ~(2*warp_size - 1);
+
+    constexpr int KQ_cs = cpw < 2*cpy_ne ? cpw : 2*cpy_ne;
+    static_assert(cpw % KQ_cs == 0, "bad KQ_cs");
+    const int k_VKQ_sup = k_VKQ_max - k_VKQ_0;
+
+    float KQ_max_new[cpw];
+#pragma unroll
+    for (int jc0 = 0; jc0 < cpw; ++jc0) {
+        KQ_max_new[jc0] = KQ_max[jc0];
+    }
+
+    constexpr int num_i_KQ_iters = nbatch_fa/(np*warp_size);
+
+    // Overlay KQ_acc onto V_tmp shared memory to reduce VGPR pressure.
+    // KQ_acc and V_tmp are never used simultaneously (KQ_acc during dot products, V_tmp during V loading).
+    constexpr int kv_tmp_elems = nbatch_fa * (nbatch_K/2 + cpy_ne) + DVp - DV;
+    constexpr size_t kv_tmp_bytes = kv_tmp_elems * sizeof(half2);
+    constexpr size_t kq_acc_bytes = nwarps * warp_size * num_i_KQ_iters * cpw * sizeof(float);
+    constexpr bool use_kq_acc_overlay = (kq_acc_bytes <= kv_tmp_bytes);
+
+    float  KQ_acc_local[use_kq_acc_overlay ? 1 : num_i_KQ_iters * cpw];
+    float* KQ_acc;
+
+    if constexpr (use_kq_acc_overlay) {
+        const int tid = threadIdx.y * warp_size + threadIdx.x;
+        KQ_acc = reinterpret_cast<float*>(V_tmp) + tid * (num_i_KQ_iters * cpw);
+    } else {
+        KQ_acc = KQ_acc_local;
+    }
+    #pragma unroll
+    for (int i = 0; i < num_i_KQ_iters * cpw; ++i) {
+        KQ_acc[i] = 0.0f;
+    }
+
+    constexpr int nbatch_K_last = DKQ % nbatch_K;
+    constexpr int num_K_tiles = (DKQ - nbatch_K_last) / nbatch_K;
+
+    #pragma unroll
+    for (int tile = 0; tile < num_K_tiles; tile++) {
+        const int k_KQ_0 = tile * nbatch_K;
+        flash_attn_tile_q8_q8_iter_KQ<warp_size, nwarps, ncols1, ncols2, DKQ, nbatch_fa, nbatch_K, oob_check>(
+            Q_values, Q_scales, K_q8, K_values, K_scales, stride_K_q8, k_VKQ_0, k_VKQ_sup, k_KQ_0, KQ_acc);
+    }
+
+    if constexpr (nbatch_K_last > 0) {
+        constexpr int k_KQ_0 = DKQ - nbatch_K_last;
+        flash_attn_tile_q8_q8_iter_KQ<warp_size, nwarps, ncols1, ncols2, DKQ, nbatch_fa, nbatch_K_last, oob_check>(
+            Q_values, Q_scales, K_q8, K_values, K_scales, stride_K_q8, k_VKQ_0, k_VKQ_sup, k_KQ_0, KQ_acc);
+    }
+
+    if constexpr (num_i_KQ_iters == 1) {
+        const int i_KQ = (threadIdx.y % np)*warp_size + threadIdx.x;
+
+#pragma unroll
+        for (int jc0 = 0; jc0 < cpw; ++jc0) {
+            const int j = (jc0 + (threadIdx.y / np)*cpw)/ncols2;
+
+            if (use_logit_softcap) {
+                KQ_acc[jc0] = logit_softcap * fast_tanh_f32(KQ_acc[jc0]);
+            }
+
+            if (!oob_check || i_KQ < k_VKQ_sup) {
+                KQ_acc[jc0] += (ncols2 > 1 || mask) ?
+                    slope*__half2float(mask[j*stride_mask + k_VKQ_0 + i_KQ]) : 0.0f;
+
+                KQ_max_new[jc0] = fmaxf(KQ_max_new[jc0], KQ_acc[jc0]);
+            }
+
+            KQ_max_new[jc0] = warp_reduce_max<warp_size>(KQ_max_new[jc0]);
+        }
+    } else {
+#pragma unroll
+        for (int jc0 = 0; jc0 < cpw; ++jc0) {
+            const int j = (jc0 + (threadIdx.y / np)*cpw)/ncols2;
+
+#pragma unroll
+            for (int i_KQ_0 = 0; i_KQ_0 < nbatch_fa; i_KQ_0 += np*warp_size) {
+                const int i_KQ = i_KQ_0 + (threadIdx.y % np)*warp_size + threadIdx.x;
+
+                if (use_logit_softcap) {
+                    KQ_acc[(i_KQ_0/(np*warp_size))*cpw + jc0] = logit_softcap * fast_tanh_f32(KQ_acc[(i_KQ_0/(np*warp_size))*cpw + jc0]);
+                }
+
+                if (!oob_check || i_KQ < k_VKQ_sup) {
+                    KQ_acc[(i_KQ_0/(np*warp_size))*cpw + jc0] += (ncols2 > 1 || mask) ?
+                        slope*__half2float(mask[j*stride_mask + k_VKQ_0 + i_KQ]) : 0.0f;
+
+                    KQ_max_new[jc0] = fmaxf(KQ_max_new[jc0], KQ_acc[(i_KQ_0/(np*warp_size))*cpw + jc0]);
+                }
+            }
+
+            KQ_max_new[jc0] = warp_reduce_max<warp_size>(KQ_max_new[jc0]);
+        }
+    }
+
+    if constexpr (np == 1) {
+        __syncthreads();
+    } else {
+        static_assert(cpw == 1, "bad cpw");
+        __shared__ float KQ_max_new_shared[nwarps];
+        if (threadIdx.x == 0) {
+            KQ_max_new_shared[threadIdx.y] = KQ_max_new[0];
+        }
+        __syncthreads();
+        KQ_max_new[0] = KQ_max_new_shared[(threadIdx.y & ~(np-1)) + threadIdx.x % np];
+        KQ_max_new[0] = warp_reduce_max<np>(KQ_max_new[0]);
+    }
+
+    if constexpr (num_i_KQ_iters == 1) {
+        const int i_KQ = (threadIdx.y % np)*warp_size + threadIdx.x;
+
+#pragma unroll
+        for (int jc0 = 0; jc0 < cpw; jc0 += KQ_cs) {
+            half tmp[1][KQ_cs];
+
+#pragma unroll
+            for (int jc1 = 0; jc1 < KQ_cs; ++jc1) {
+                const int jc = jc0 + jc1;
+
+                const float KQ_max_scale = fast_exp_f32(KQ_max[jc] - KQ_max_new[jc]);
+                KQ_max[jc] = KQ_max_new[jc];
+
+                const float val = !oob_check || i_KQ < k_VKQ_sup ?
+                    fast_exp_f32(KQ_acc[jc] - KQ_max[jc]) : 0.0f;
+                const float KQ_sum_add = val;
+                tmp[0][jc1] = val;
+
+                KQ_sum[jc] = KQ_sum[jc]*KQ_max_scale + KQ_sum_add;
+
+                const half2 KQ_max_scale_h2 = make_half2(KQ_max_scale, KQ_max_scale);
+#pragma unroll
+                for (int i0 = 0; i0 < DVp/2; i0 += warp_size) {
+                    VKQ[jc*((DVp/2)/warp_size) + i0/warp_size] *= KQ_max_scale_h2;
+                }
+            }
+
+            ggml_cuda_memcpy_1<sizeof(tmp[0])>(
+                KQ + (jc0/KQ_cs + (threadIdx.y / np)*(cpw/KQ_cs))*(nbatch_fa*KQ_cs) + i_KQ*KQ_cs,
+                tmp[0]);
+        }
+    } else {
+#pragma unroll
+        for (int jc0 = 0; jc0 < cpw; jc0 += KQ_cs) {
+            half tmp[num_i_KQ_iters][KQ_cs];
+
+#pragma unroll
+            for (int jc1 = 0; jc1 < KQ_cs; ++jc1) {
+                const int jc = jc0 + jc1;
+
+                const float KQ_max_scale = fast_exp_f32(KQ_max[jc] - KQ_max_new[jc]);
+                KQ_max[jc] = KQ_max_new[jc];
+
+                float KQ_sum_add = 0.0f;
+#pragma unroll
+                for (int i0 = 0; i0 < nbatch_fa; i0 += np*warp_size) {
+                    const float val = !oob_check || i0 + (threadIdx.y % np)*warp_size + threadIdx.x < k_VKQ_sup ?
+                        fast_exp_f32(KQ_acc[(i0/(np*warp_size))*cpw + jc] - KQ_max[jc]) : 0.0f;
+                    KQ_sum_add += val;
+                    tmp[i0/(np*warp_size)][jc1] = val;
+                }
+                KQ_sum[jc] = KQ_sum[jc]*KQ_max_scale + KQ_sum_add;
+
+                const half2 KQ_max_scale_h2 = make_half2(KQ_max_scale, KQ_max_scale);
+#pragma unroll
+                for (int i0 = 0; i0 < DVp/2; i0 += warp_size) {
+                    VKQ[jc*((DVp/2)/warp_size) + i0/warp_size] *= KQ_max_scale_h2;
+                }
+            }
+
+#pragma unroll
+            for (int i0 = 0; i0 < nbatch_fa; i0 += np*warp_size) {
+                const int i = i0 + (threadIdx.y % np)*warp_size + threadIdx.x;
+
+                ggml_cuda_memcpy_1<sizeof(tmp[0])>(
+                    KQ + (jc0/KQ_cs + (threadIdx.y / np)*(cpw/KQ_cs))*(nbatch_fa*KQ_cs) + i*KQ_cs,
+                    tmp[i0/(np*warp_size)]);
+            }
+        }
+    }
+
+    static_assert(DV <= DKQ, "bad DV");
+    static_assert(DV % nbatch_K == 0 || (nbatch_K % 3 == 0 && DV % (nbatch_K*2/3) == 0), "bad nbatch_K");
+    constexpr int nbatch_V = (DV % nbatch_K == 0 ? nbatch_K : nbatch_K*2/3) * nbatch_fa / DV;
+    static_assert(nbatch_fa % nbatch_V == 0, "bad nbatch_V");
+    static_assert(nbatch_V % np == 0, "bad nbatch_V");
+
+    if constexpr (use_kq_acc_overlay) {
+        // KQ_acc was overlaid onto V_tmp; ensure all threads finished reading before V_tmp reuse.
+        __syncthreads();
+    }
+
+#pragma unroll
+    for (int k0 = 0; k0 < nbatch_fa; k0 += nbatch_V) {
+        flash_attn_tile_q8_q8_load_tile<warp_size, nwarps, nbatch_V, DV, 0, oob_check>
+            (V_h2 + int64_t(k_VKQ_0 + k0)*stride_V2, V_tmp, stride_V2, k_VKQ_sup - k0);
+        __syncthreads();
+
+#pragma unroll
+        for (int k1 = 0; k1 < nbatch_V; k1 += np) {
+            half2 V_k[(DVp/2)/warp_size];
+            half2 KQ_k[cpw];
+
+            constexpr int cpy_ne_D = cpy_ne/2 < (DVp/2)/warp_size ? cpy_ne/2 : (DVp/2)/warp_size;
+
+#pragma unroll
+            for (int i0 = 0; i0 < DVp/2; i0 += warp_size*cpy_ne_D) {
+                ggml_cuda_memcpy_1<cpy_ne_D*4>(&V_k[i0/warp_size], &V_tmp[(k1 + threadIdx.y % np)*(DV/2) + i0 + threadIdx.x*cpy_ne_D]);
+            }
+
+#pragma unroll
+            for (int jc_VKQ_0 = 0; jc_VKQ_0 < cpw; jc_VKQ_0 += KQ_cs) {
+                const int jc_KQ = jc_VKQ_0/KQ_cs + (threadIdx.y / np)*(cpw/KQ_cs);
+
+                half tmp[KQ_cs];
+                ggml_cuda_memcpy_1<KQ_cs*sizeof(half)>(
+                    &tmp, KQ + jc_KQ*(nbatch_fa*KQ_cs) + (k0 + k1 + threadIdx.y % np)*KQ_cs);
+#pragma unroll
+                for (int jc_VKQ_1 = 0; jc_VKQ_1 < KQ_cs; ++jc_VKQ_1) {
+                    KQ_k[jc_VKQ_0+jc_VKQ_1] = __half2half2(tmp[jc_VKQ_1]);
+                }
+            }
+
+#pragma unroll
+            for (int i0 = 0; i0 < DVp/2; i0 += warp_size) {
+                const half2 v_val = V_k[i0/warp_size];
+#pragma unroll
+                for (int jc_VKQ_0 = 0; jc_VKQ_0 < cpw; ++jc_VKQ_0) {
+                    VKQ[jc_VKQ_0*((DVp/2)/warp_size) + i0/warp_size] += v_val * KQ_k[jc_VKQ_0];
+                }
+            }
+        }
+
+        __syncthreads();
+    }
+}
+
+template<int DKQ, int DV, int ncols1, int ncols2, bool use_logit_softcap>
+__launch_bounds__(ggml_cuda_fattn_tile_q8_get_nthreads(DKQ, DV, ncols1*ncols2), ggml_cuda_fattn_tile_q8_get_occupancy(DKQ, DV, ncols1*ncols2))
+static __global__ void flash_attn_tile_q8(
+        const char * __restrict__ Q,
+        const char * __restrict__ K,
+        const char * __restrict__ V,
+        const char * __restrict__ mask,
+        const char * __restrict__ sinks,
+        const int  * __restrict__ KV_max,
+        float      * __restrict__ dst,
+        float2     * __restrict__ dst_meta,
+        const float scale,
+        const float max_bias,
+        const float m0,
+        const float m1,
+        const uint32_t n_head_log2,
+        const float logit_softcap,
+        const int32_t ne00, const uint3   ne01, const int32_t ne02, const int32_t ne03,
+                            const int32_t nb01, const int32_t nb02, const int32_t nb03,
+        const int32_t ne10, const int32_t ne11, const int32_t ne12, const int32_t ne13,
+                            const int32_t nb11, const int32_t nb12, const int64_t nb13,
+                            const int32_t nb21, const int32_t nb22, const int64_t nb23,
+                            const int32_t ne31, const int32_t ne32, const int32_t ne33,
+                            const int32_t nb31, const int32_t nb32, const int64_t nb33) {
+#ifdef FLASH_ATTN_AVAILABLE
+
+    if (use_logit_softcap && !(DV == 128 || DV == 256)) {
+        GGML_UNUSED_VARS(Q, K, V, mask, sinks, KV_max, dst, dst_meta, scale,
+            max_bias, m0, m1, n_head_log2, logit_softcap,
+            ne00, ne01, ne02, ne03,
+                  nb01, nb02, nb03,
+            ne10, ne11, ne12, ne13,
+                  nb11, nb12, nb13,
+                  nb21, nb22, nb23,
+                  ne31, ne32, ne33,
+                  nb31, nb32, nb33);
+        NO_DEVICE_CODE;
+        return;
+    }
+
+    static_assert(ggml_cuda_fattn_tile_q8_get_config(DKQ, DV, ncols1*ncols2) != 0, "kernel config not defined");
+    static_assert(DKQ % 32 == 0, "DKQ must be multiple of 32 for Q8_0 quantization");
+
+    constexpr int ncols     = ncols1*ncols2;
+    constexpr int warp_size = 32;
+    constexpr int nwarps    = ggml_cuda_fattn_tile_q8_get_nthreads (DKQ, DV, ncols1*ncols2) / warp_size;
+    constexpr int nbatch_fa = ggml_cuda_fattn_tile_q8_get_nbatch_fa(DKQ, DV, ncols1*ncols2);
+    constexpr int nbatch_K  = ggml_cuda_fattn_tile_q8_get_nbatch_K (DKQ, DV, ncols1*ncols2);
+
+    const int col_Q_0 = blockIdx.x * ncols1;
+
+    // Use ceiling division to handle non-power-of-2 GQA ratios
+    const int ntiles_z = (ne02 + ncols2 - 1) / ncols2;
+    const int sequence = blockIdx.z / ntiles_z;
+    const int zt = blockIdx.z - sequence * ntiles_z;
+    const int head0 = zt * ncols2;
+    const int gqa_ratio = ne02 / ne12;
+    const float * Q_f  = (const float *) (Q + nb03*sequence + nb02* head0              + nb01*col_Q_0);
+    const block_q8_0 * K_q8 = (const block_q8_0 *) (K + nb13*sequence + nb12*(head0 / gqa_ratio));
+    const half2 * V_h2 = (const half2 *) (V + nb23*sequence + nb22*(head0 / gqa_ratio));
+
+    const half * maskh = mask ? (const half *) (mask + nb33*(sequence % ne33) + nb31*col_Q_0) : nullptr;
+
+    const int stride_K_q8 = nb11 / sizeof(block_q8_0);
+    const int stride_V2   = nb21 / sizeof(half2);
+    const int stride_mask = nb31 / sizeof(half);
+
+    float slope_tmp = 0.0f;
+    if (threadIdx.x == 0) {
+        slope_tmp = ncols2 == 1 ? get_alibi_slope(max_bias, head0, n_head_log2, m0, m1) : 1.0f;
+    }
+    const float slope = sgpr_broadcast_f32(slope_tmp);
+
+    constexpr int cpy_ne = ggml_cuda_get_max_cpy_bytes() / 4;
+
+    constexpr int cpw = ncols > nwarps ? ncols/nwarps : 1;
+    constexpr int np  = nwarps > ncols ? nwarps/ncols : 1;
+    static_assert(cpw == 1 || np == 1, "bad cpw / np");
+    static_assert(nbatch_fa % (np*warp_size) == 0, "nbatch_fa % (np*warp_size) != 0");
+
+    constexpr int DVp  = (DV  + 2*warp_size - 1) & ~(2*warp_size - 1);
+
+    __shared__ int8_t Q_values[ncols * DKQ];
+    __shared__ half   Q_scales[ncols * (DKQ/32)];
+
+    constexpr int K_row_padding = 16;
+    __shared__ int8_t K_values[nbatch_fa * (nbatch_K + K_row_padding)];
+    __shared__ half   K_scales[nbatch_fa * (nbatch_K/32)];
+
+    __shared__ half2 KV_tmp[nbatch_fa * (nbatch_K/2 + cpy_ne) + DVp-DV];
+
+    __shared__ half  KQ[ncols * nbatch_fa];
+    half2 VKQ[cpw * ((DVp/2)/warp_size)] = {{0.0f, 0.0f}};
+
+    float KQ_max[cpw];
+#pragma unroll
+    for (int j0 = 0; j0 < ncols; j0 += nwarps) {
+        KQ_max[j0/nwarps] = -FLT_MAX/2.0f;
+    }
+    float KQ_sum[cpw] = {0.0f};
+
+    flash_attn_tile_q8_quantize_Q_to_shared<nwarps*warp_size, ncols, ncols2, DKQ>(
+        Q_f, Q_values, Q_scales, col_Q_0, int(ne01.z), head0, ne02, nb01, nb02, scale);
+
+    const int k_VKQ_max = KV_max ? KV_max[sequence*gridDim.x + blockIdx.x] : ne11;
+    if (ncols2 == 1) {
+        int k_VKQ_0 = blockIdx.y*nbatch_fa;
+        while (k_VKQ_0 < k_VKQ_max - nbatch_fa) {
+            constexpr bool oob_check = false;
+            flash_attn_tile_q8_q8_iter<warp_size, nwarps, ncols1, ncols2, DKQ, DV, nbatch_fa, nbatch_K, use_logit_softcap, oob_check>
+                (Q_values, Q_scales, K_q8, V_h2, maskh, logit_softcap, slope, KQ, K_values, K_scales, KV_tmp,
+                stride_K_q8, stride_V2, stride_mask, KQ_max, KQ_sum, VKQ, k_VKQ_0, k_VKQ_max);
+            k_VKQ_0 += gridDim.y*nbatch_fa;
+        }
+        if (k_VKQ_0 < k_VKQ_max) {
+            constexpr bool oob_check = true;
+            flash_attn_tile_q8_q8_iter<warp_size, nwarps, ncols1, ncols2, DKQ, DV, nbatch_fa, nbatch_K, use_logit_softcap, oob_check>
+                (Q_values, Q_scales, K_q8, V_h2, maskh, logit_softcap, slope, KQ, K_values, K_scales, KV_tmp,
+                stride_K_q8, stride_V2, stride_mask, KQ_max, KQ_sum, VKQ, k_VKQ_0, k_VKQ_max);
+        }
+    } else {
+        for (int k_VKQ_0 = blockIdx.y*nbatch_fa; k_VKQ_0 < k_VKQ_max; k_VKQ_0 += gridDim.y*nbatch_fa) {
+            constexpr bool oob_check = false;
+            flash_attn_tile_q8_q8_iter<warp_size, nwarps, ncols1, ncols2, DKQ, DV, nbatch_fa, nbatch_K, use_logit_softcap, oob_check>
+                (Q_values, Q_scales, K_q8, V_h2, maskh, logit_softcap, slope, KQ, K_values, K_scales, KV_tmp,
+                stride_K_q8, stride_V2, stride_mask, KQ_max, KQ_sum, VKQ, k_VKQ_0, k_VKQ_max);
+        }
+    }
+
+#pragma unroll
+    for (int jc0 = 0; jc0 < cpw; ++jc0) {
+        KQ_sum[jc0] = warp_reduce_sum<warp_size>(KQ_sum[jc0]);
+    }
+
+    if constexpr (np > 1) {
+        static_assert(cpw == 1, "bad cpw");
+        static_assert(nbatch_fa*nbatch_K >= nwarps*DVp, "KV_tmp too small");
+
+        half2 * VKQ_combine    = (half2 *) KV_tmp;
+        float * KQ_sum_combine = (float *) Q_values;
+
+        if (threadIdx.y % np != 0) {
+            constexpr int cpy_ne_D = cpy_ne < (DVp/2)/warp_size ? cpy_ne : (DVp/2)/warp_size;
+#pragma unroll
+            for (int i0 = 0; i0 < DVp/2; i0 += warp_size*cpy_ne_D) {
+                ggml_cuda_memcpy_1<cpy_ne_D*4>(&VKQ_combine[threadIdx.y*(DVp/2) + i0 + threadIdx.x*cpy_ne_D], &VKQ[i0/warp_size]);
+            }
+
+            if (threadIdx.x == 0) {
+                KQ_sum_combine[threadIdx.y] = KQ_sum[0];
+            }
+
+            return;
+        }
+
+        __syncthreads();
+
+#pragma unroll
+        for (int ip = 1; ip < np; ++ip) {
+            constexpr int cpy_ne_D = cpy_ne < (DVp/2)/warp_size ? cpy_ne : (DVp/2)/warp_size;
+#pragma unroll
+            for (int i0 = 0; i0 < DVp/2; i0 += warp_size*cpy_ne_D) {
+                half2 tmp[cpy_ne_D];
+                ggml_cuda_memcpy_1<cpy_ne_D*4>(tmp, &VKQ_combine[(threadIdx.y + ip)*(DVp/2) + i0 + threadIdx.x*cpy_ne_D]);
+#pragma unroll
+                for (int i1 = 0; i1 < cpy_ne_D; ++i1) {
+                    VKQ[i0/warp_size + i1] += tmp[i1];
+                }
+            }
+
+            KQ_sum[0] += KQ_sum_combine[threadIdx.y + ip];
+        }
+    }
+
+    if (sinks && blockIdx.y == 0) {
+#pragma unroll
+        for (int jc0 = 0; jc0 < cpw; ++jc0) {
+            const int jc = jc0 + (threadIdx.y/np)*cpw;
+            const float sink = ((const float *) sinks)[head0 + jc % ncols2];
+
+            float KQ_max_new_j = fmaxf(KQ_max[jc0], sink);
+            const float KQ_max_scale = fast_exp_f32(KQ_max[jc0] - KQ_max_new_j);
+            KQ_max[jc0] = KQ_max_new_j;
+
+            const float val = fast_exp_f32(sink - KQ_max[jc0]);
+            KQ_sum[jc0] = KQ_sum[jc0]*KQ_max_scale + val;
+
+            const half2 KQ_max_scale_h2 = make_half2(KQ_max_scale, KQ_max_scale);
+#pragma unroll
+            for (int i0 = 0; i0 < DVp/2; i0 += warp_size) {
+                VKQ[jc0*((DVp/2)/warp_size) + i0/warp_size] *= KQ_max_scale_h2;
+            }
+        }
+    }
+
+#pragma unroll
+    for (int jc0 = 0; jc0 < cpw; ++jc0) {
+        const int jc = jc0 + (threadIdx.y/np)*cpw;
+
+        const int j = jc / ncols2;
+        const int c = jc % ncols2;
+
+        // Bounds check for non-power-of-2 GQA ratios
+        if ((ncols1 > 1 && col_Q_0 + j >= int(ne01.z)) || (ncols2 > 1 && head0 + c >= ne02)) {
+            continue;
+        }
+
+        const float scale = gridDim.y == 1 ? 1.0f/KQ_sum[jc0] : 1.0f;
+
+        const int j_dst_unrolled = ((sequence*int(ne01.z) + col_Q_0 + j)*ne02 + head0 + c)*gridDim.y + blockIdx.y;
+
+        constexpr int cpy_ne_D = cpy_ne/2 < (DVp/2)/warp_size ? cpy_ne/2 : (DVp/2)/warp_size;
+#pragma unroll
+        for (int i0 = 0; i0 < DVp/2; i0 += warp_size*cpy_ne_D) {
+            float2 tmp[cpy_ne_D];
+#pragma unroll
+            for (int i1 = 0; i1 < cpy_ne_D; ++i1) {
+                tmp[i1] = __half22float2(VKQ[jc0*((DVp/2)/warp_size) + i0/warp_size + i1]);
+                tmp[i1].x *= scale;
+                tmp[i1].y *= scale;
+            }
+            if (i0 + warp_size*cpy_ne_D <= DV/2 || i0 + threadIdx.x*cpy_ne_D < DV/2) {
+                ggml_cuda_memcpy_1<sizeof(tmp)>(&dst[j_dst_unrolled*DV + 2*i0 + threadIdx.x*(2*cpy_ne_D)], tmp);
+            }
+        }
+
+        if (gridDim.y != 1 && threadIdx.x == 0) {
+            dst_meta[j_dst_unrolled] = make_float2(KQ_max[jc0], KQ_sum[jc0]);
+        }
+    }
+#else
+    GGML_UNUSED_VARS(Q, K, V, mask, sinks, KV_max, dst, dst_meta, scale,
+        max_bias, m0, m1, n_head_log2, logit_softcap,
+        ne00, ne01, ne02, ne03,
+              nb01, nb02, nb03,
+        ne10, ne11, ne12, ne13,
+              nb11, nb12, nb13,
+              nb21, nb22, nb23,
+              ne31, ne32, ne33,
+              nb31, nb32, nb33);
+    NO_DEVICE_CODE;
+#endif
+}
+
+template <int DKQ, int DV, int ncols2, bool use_logit_softcap>
+static void launch_fattn_tile_q8_switch_ncols1(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {
+    const ggml_tensor * Q = dst->src[0];
+
+    const int id        = ggml_cuda_get_device();
+    const int cc        = ggml_cuda_info().devices[id].cc;
+    const int warp_size = 32;
+
+    constexpr size_t nbytes_shared = 0;
+
+    if constexpr (DV <= 128) {
+        if (Q->ne[1] > 32/ncols2) {
+            constexpr int cols_per_block = 64;
+            const int nwarps    = ggml_cuda_fattn_tile_q8_get_nthreads (DKQ, DV, cols_per_block, cc) / warp_size;
+            const int nbatch_fa = ggml_cuda_fattn_tile_q8_get_nbatch_fa(DKQ, DV, cols_per_block, cc);
+            fattn_kernel_t fattn_kernel = flash_attn_tile_q8<DKQ, DV, cols_per_block/ncols2, ncols2, use_logit_softcap>;
+            launch_fattn<DV, cols_per_block/ncols2, ncols2>
+                (ctx, dst, fattn_kernel, nwarps, nbytes_shared, nbatch_fa, false, true, false, warp_size);
+            return;
+        }
+    }
+
+    {
+        if (Q->ne[1] > 16/ncols2) {
+            constexpr int cols_per_block = 32;
+            const int nwarps    = ggml_cuda_fattn_tile_q8_get_nthreads (DKQ, DV, cols_per_block, cc) / warp_size;
+            const int nbatch_fa = ggml_cuda_fattn_tile_q8_get_nbatch_fa(DKQ, DV, cols_per_block, cc);
+            fattn_kernel_t fattn_kernel = flash_attn_tile_q8<DKQ, DV, cols_per_block/ncols2, ncols2, use_logit_softcap>;
+            launch_fattn<DV, cols_per_block/ncols2, ncols2>
+                (ctx, dst, fattn_kernel, nwarps, nbytes_shared, nbatch_fa, false, true, false, warp_size);
+            return;
+        }
+    }
+
+    if (Q->ne[1] > 8/ncols2) {
+        constexpr int cols_per_block = 16;
+        const int nwarps    = ggml_cuda_fattn_tile_q8_get_nthreads (DKQ, DV, cols_per_block, cc) / warp_size;
+        const int nbatch_fa = ggml_cuda_fattn_tile_q8_get_nbatch_fa(DKQ, DV, cols_per_block, cc);
+        fattn_kernel_t fattn_kernel = flash_attn_tile_q8<DKQ, DV, cols_per_block/ncols2, ncols2, use_logit_softcap>;
+        launch_fattn<DV, cols_per_block/ncols2, ncols2>
+            (ctx, dst, fattn_kernel, nwarps, nbytes_shared, nbatch_fa, false, true, false, warp_size);
+        return;
+    }
+
+    if constexpr (ncols2 <= 8) {
+        if (Q->ne[1] > 4/ncols2) {
+            constexpr int cols_per_block = 8;
+            const int nwarps    = ggml_cuda_fattn_tile_q8_get_nthreads (DKQ, DV, cols_per_block, cc) / warp_size;
+            const int nbatch_fa = ggml_cuda_fattn_tile_q8_get_nbatch_fa(DKQ, DV, cols_per_block, cc);
+            fattn_kernel_t fattn_kernel = flash_attn_tile_q8<DKQ, DV, cols_per_block/ncols2, ncols2, use_logit_softcap>;
+            launch_fattn<DV, cols_per_block/ncols2, ncols2>
+                (ctx, dst, fattn_kernel, nwarps, nbytes_shared, nbatch_fa, false, true, false, warp_size);
+            return;
+        }
+    }
+
+    if constexpr (ncols2 <= 4) {
+        if (Q->ne[1] > 2/ncols2) {
+            constexpr int cols_per_block = 4;
+            const int nwarps    = ggml_cuda_fattn_tile_q8_get_nthreads (DKQ, DV, cols_per_block, cc) / warp_size;
+            const int nbatch_fa = ggml_cuda_fattn_tile_q8_get_nbatch_fa(DKQ, DV, cols_per_block, cc);
+            fattn_kernel_t fattn_kernel = flash_attn_tile_q8<DKQ, DV, cols_per_block/ncols2, ncols2, use_logit_softcap>;
+            launch_fattn<DV, cols_per_block/ncols2, ncols2>
+                (ctx, dst, fattn_kernel, nwarps, nbytes_shared, nbatch_fa, false, true, false, warp_size);
+            return;
+        }
+    }
+
+    if constexpr (ncols2 <= 2) {
+        constexpr int cols_per_block = 2;
+        const int nwarps    = ggml_cuda_fattn_tile_q8_get_nthreads (DKQ, DV, cols_per_block, cc) / warp_size;
+        const int nbatch_fa = ggml_cuda_fattn_tile_q8_get_nbatch_fa(DKQ, DV, cols_per_block, cc);
+        fattn_kernel_t fattn_kernel = flash_attn_tile_q8<DKQ, DV, cols_per_block/ncols2, ncols2, use_logit_softcap>;
+        launch_fattn<DV, cols_per_block/ncols2, ncols2>
+            (ctx, dst, fattn_kernel, nwarps, nbytes_shared, nbatch_fa, false, true, false, warp_size);
+        return;
+    }
+
+    GGML_ABORT("fatal error");
+}
+
+template <int DKQ, int DV, bool use_logit_softcap>
+static void launch_fattn_tile_q8_switch_ncols2(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {
+    const ggml_tensor * KQV  = dst;
+    const ggml_tensor * Q    = dst->src[0];
+    const ggml_tensor * K    = dst->src[1];
+    const ggml_tensor * mask = dst->src[3];
+
+    float max_bias = 0.0f;
+    memcpy(&max_bias, (const float *) KQV->op_params + 1, sizeof(float));
+
+    GGML_ASSERT(Q->ne[2] % K->ne[2] == 0);
+    const int gqa_ratio = Q->ne[2] / K->ne[2];
+
+    const bool nvidia = GGML_CUDA_CC_IS_NVIDIA(ggml_cuda_info().devices[ggml_cuda_get_device()].cc);
+    const int gqa_limit = nvidia && gqa_ratio <= 4 ? 16 : INT_MAX;
+    const bool use_gqa_opt = mask && max_bias == 0.0f && Q->ne[1] <= gqa_limit && K->ne[1] % FATTN_KQ_STRIDE == 0;
+
+    if constexpr (DV == 512) {
+        // Changed from gqa_ratio % 16 == 0 to gqa_ratio > 8 to handle non-power-of-2 GQA ratios
+        if (use_gqa_opt && gqa_ratio > 8) {
+            launch_fattn_tile_q8_switch_ncols1<DKQ, DV, 16, use_logit_softcap>(ctx, dst);
+            return;
+        }
+    }
+
+    if constexpr (DV <= 256) {
+        // Changed from gqa_ratio % N == 0 to gqa_ratio > N to handle non-power-of-2 GQA ratios
+        if (use_gqa_opt && gqa_ratio > 4) {
+            launch_fattn_tile_q8_switch_ncols1<DKQ, DV, 8, use_logit_softcap>(ctx, dst);
+            return;
+        }
+
+        if (use_gqa_opt && gqa_ratio > 2) {
+            launch_fattn_tile_q8_switch_ncols1<DKQ, DV, 4, use_logit_softcap>(ctx, dst);
+            return;
+        }
+
+        if (use_gqa_opt && gqa_ratio > 1) {
+            launch_fattn_tile_q8_switch_ncols1<DKQ, DV, 2, use_logit_softcap>(ctx, dst);
+            return;
+        }
+
+        launch_fattn_tile_q8_switch_ncols1<DKQ, DV, 1, use_logit_softcap>(ctx, dst);
+        return;
+    }
+    GGML_ABORT("fatal error");
+}
+
+template <int DKQ, int DV>
+void ggml_cuda_flash_attn_ext_tile_q8_case(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {
+    const ggml_tensor * KQV = dst;
+
+    float logit_softcap;
+    memcpy(&logit_softcap, (const float *) KQV->op_params + 2, sizeof(float));
+
+    if (logit_softcap == 0.0f) {
+        constexpr bool use_logit_softcap = false;
+        launch_fattn_tile_q8_switch_ncols2<DKQ, DV, use_logit_softcap>(ctx, dst);
+    } else {
+        constexpr bool use_logit_softcap = true;
+        launch_fattn_tile_q8_switch_ncols2<DKQ, DV, use_logit_softcap>(ctx, dst);
+    }
+}
+
+void ggml_cuda_flash_attn_ext_tile_q8(ggml_backend_cuda_context & ctx, ggml_tensor * dst);
+
+#define DECL_FATTN_TILE_CASE(DKQ, DV)                             \
+    template void ggml_cuda_flash_attn_ext_tile_q8_case              \
+    <DKQ, DV>(ggml_backend_cuda_context & ctx, ggml_tensor * dst) \
+
+extern DECL_FATTN_TILE_CASE( 40,  40);
+extern DECL_FATTN_TILE_CASE( 64,  64);
+extern DECL_FATTN_TILE_CASE( 80,  80);
+extern DECL_FATTN_TILE_CASE( 96,  96);
+extern DECL_FATTN_TILE_CASE(112, 112);
+extern DECL_FATTN_TILE_CASE(128, 128);
+extern DECL_FATTN_TILE_CASE(256, 256);
diff --git a/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq112-dv112.cu b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq112-dv112.cu
new file mode 100644
index 000000000..d3c0fea42
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq112-dv112.cu
@@ -0,0 +1,7 @@
+// Q8 kernel template instantiation
+
+#include "../fattn-q8.cuh"
+
+// Phase 5: Temporarily disabled - DKQ=112 is not multiple of 32 (Q8_0 block size)
+// TODO: Re-enable after adding support for non-32-multiple head sizes
+// DECL_FATTN_TILE_CASE(112, 112);
diff --git a/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq128-dv128.cu b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq128-dv128.cu
new file mode 100644
index 000000000..cb1e56474
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq128-dv128.cu
@@ -0,0 +1,5 @@
+// Q8 kernel template instantiation
+
+#include "../fattn-q8.cuh"
+
+DECL_FATTN_TILE_CASE(128, 128);
diff --git a/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq256-dv256.cu b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq256-dv256.cu
new file mode 100644
index 000000000..be963cf73
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq256-dv256.cu
@@ -0,0 +1,5 @@
+// Q8 kernel template instantiation
+
+#include "../fattn-q8.cuh"
+
+DECL_FATTN_TILE_CASE(256, 256);
diff --git a/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq40-dv40.cu b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq40-dv40.cu
new file mode 100644
index 000000000..7ed8420ea
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq40-dv40.cu
@@ -0,0 +1,7 @@
+// Q8 kernel template instantiation
+
+#include "../fattn-q8.cuh"
+
+// Phase 5: Temporarily disabled - DKQ=40 is not multiple of 32 (Q8_0 block size)
+// TODO: Re-enable after adding support for non-32-multiple head sizes
+// DECL_FATTN_TILE_CASE(40, 40);
diff --git a/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq576-dv512.cu b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq576-dv512.cu
new file mode 100644
index 000000000..db6f634d1
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq576-dv512.cu
@@ -0,0 +1,5 @@
+// Q8 kernel template instantiation
+
+#include "../fattn-q8.cuh"
+
+DECL_FATTN_TILE_CASE(576, 512);
diff --git a/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq64-dv64.cu b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq64-dv64.cu
new file mode 100644
index 000000000..33cf545ea
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq64-dv64.cu
@@ -0,0 +1,5 @@
+// Q8 kernel template instantiation
+
+#include "../fattn-q8.cuh"
+
+DECL_FATTN_TILE_CASE(64, 64);
diff --git a/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq80-dv80.cu b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq80-dv80.cu
new file mode 100644
index 000000000..ae3135371
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq80-dv80.cu
@@ -0,0 +1,7 @@
+// Q8 kernel template instantiation
+
+#include "../fattn-q8.cuh"
+
+// Phase 5: Temporarily disabled - DKQ=80 is not multiple of 32 (Q8_0 block size)
+// TODO: Re-enable after adding support for non-32-multiple head sizes
+// DECL_FATTN_TILE_CASE(80, 80);
diff --git a/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq96-dv96.cu b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq96-dv96.cu
new file mode 100644
index 000000000..fab8152be
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/attention/instances/fattn-tile-q8-instance-dkq96-dv96.cu
@@ -0,0 +1,5 @@
+// Q8 kernel template instantiation
+
+#include "../fattn-q8.cuh"
+
+DECL_FATTN_TILE_CASE(96, 96);
diff --git a/ggml/src/ggml-cuda/gfx906/attention/rope.cuh b/ggml/src/ggml-cuda/gfx906/attention/rope.cuh
new file mode 100644
index 000000000..dfc7679fe
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/attention/rope.cuh
@@ -0,0 +1,163 @@
+#pragma once
+
+// GFX906 RoPE kernel using __sincosf() for combined sin/cos computation
+
+#include "../gfx906-config.h"
+
+#if defined(GGML_USE_HIP) && defined(GFX906_ROPE_ENABLED)
+
+#define GFX906_ROPE_BLOCK_SIZE 256
+
+struct gfx906_rope_corr_dims {
+    float v[2];
+};
+
+struct gfx906_mrope_sections {
+    int v[4];
+};
+
+template<bool forward>
+static __device__ __forceinline__ void gfx906_rope_yarn(
+        const float theta_extrap,
+        const float freq_scale,
+        const gfx906_rope_corr_dims corr_dims,
+        const int i0,
+        const float ext_factor,
+        float mscale,
+        float & cos_theta,
+        float & sin_theta) {
+
+    float theta_interp = freq_scale * theta_extrap;
+    float theta = theta_interp;
+
+    if (ext_factor != 0.0f) {
+        const float y = (i0 / 2 - corr_dims.v[0]) / fmaxf(0.001f, corr_dims.v[1] - corr_dims.v[0]);
+        const float ramp_mix = (1.0f - fminf(1.0f, fmaxf(0.0f, y))) * ext_factor;
+        theta = theta_interp * (1.0f - ramp_mix) + theta_extrap * ramp_mix;
+        mscale *= 1.0f + 0.1f * __logf(1.0f / freq_scale);
+    }
+
+    __sincosf(theta, &sin_theta, &cos_theta);
+    cos_theta *= mscale;
+    sin_theta *= mscale;
+
+    if (!forward) {
+        sin_theta = -sin_theta;
+    }
+}
+
+template<bool forward, bool has_ff, typename T>
+static __global__ void gfx906_rope_multi(
+        const T * __restrict__ x,
+        T * __restrict__ dst,
+        const int ne0,
+        const int ne1,
+        const int ne2,
+        const int s1,
+        const int s2,
+        const int n_dims,
+        const int32_t * __restrict__ pos,
+        const float freq_scale,
+        const float ext_factor,
+        const float attn_factor,
+        const gfx906_rope_corr_dims corr_dims,
+        const float theta_scale,
+        const float * __restrict__ freq_factors,
+        const gfx906_mrope_sections sections,
+        const bool is_imrope) {
+
+    const int i0 = 2 * (blockDim.y * blockIdx.y + threadIdx.y);
+
+    if (i0 >= ne0) return;
+
+    const int row_dst = blockDim.x * blockIdx.x + threadIdx.x;
+    const int row_x = row_dst % ne1;
+    const int channel_x = row_dst / ne1;
+
+    const int idst = row_dst * ne0 + i0 / 2;
+    const int ix = channel_x * s2 + row_x * s1 + i0 / 2;
+
+    if (i0 >= n_dims) {
+        dst[idst + i0/2 + 0] = x[ix + i0/2 + 0];
+        dst[idst + i0/2 + 1] = x[ix + i0/2 + 1];
+        return;
+    }
+
+    const float theta_power = __powf(theta_scale, (float)(i0 / 2));
+
+    const int sect_dims = sections.v[0] + sections.v[1] + sections.v[2] + sections.v[3];
+    const int sec_w = sections.v[1] + sections.v[0];
+    const int sector = (i0 / 2) % sect_dims;
+
+    int pos_idx;
+    if (is_imrope) {
+        const int mod3 = sector % 3;
+        if (mod3 == 1 && sector < 3 * sections.v[1]) {
+            pos_idx = channel_x + ne2;
+        } else if (mod3 == 2 && sector < 3 * sections.v[2]) {
+            pos_idx = channel_x + ne2 * 2;
+        } else if (mod3 == 0 && sector < 3 * sections.v[0]) {
+            pos_idx = channel_x;
+        } else {
+            pos_idx = channel_x + ne2 * 3;
+        }
+    } else {
+        if (sector < sections.v[0]) {
+            pos_idx = channel_x;
+        } else if (sector < sec_w) {
+            pos_idx = channel_x + ne2;
+        } else if (sector < sec_w + sections.v[2]) {
+            pos_idx = channel_x + ne2 * 2;
+        } else {
+            pos_idx = channel_x + ne2 * 3;
+        }
+    }
+
+    const float theta_base = pos[pos_idx] * theta_power;
+    const float freq_factor = has_ff ? freq_factors[i0 / 2] : 1.0f;
+
+    float cos_theta, sin_theta;
+    gfx906_rope_yarn<forward>(theta_base / freq_factor, freq_scale, corr_dims,
+                               i0, ext_factor, attn_factor, cos_theta, sin_theta);
+
+    const float x0 = x[ix + 0];
+    const float x1 = x[ix + n_dims / 2];
+
+    dst[idst + 0] = x0 * cos_theta - x1 * sin_theta;
+    dst[idst + n_dims / 2] = x0 * sin_theta + x1 * cos_theta;
+}
+
+template<bool forward, typename T>
+static void gfx906_rope_multi_cuda(
+        const T * x, T * dst,
+        const int ne0, const int ne1, const int ne2,
+        const int s1, const int s2, const int n_dims, const int nr,
+        const int32_t * pos,
+        const float freq_scale, const float freq_base,
+        const float ext_factor, const float attn_factor,
+        const gfx906_rope_corr_dims corr_dims,
+        const float * freq_factors,
+        const gfx906_mrope_sections sections,
+        const bool is_imrope,
+        cudaStream_t stream) {
+
+    GGML_ASSERT(ne0 % 2 == 0);
+
+    const dim3 block_dims(1, GFX906_ROPE_BLOCK_SIZE, 1);
+    const int n_blocks_x = (ne0 + 2 * GFX906_ROPE_BLOCK_SIZE - 1) / (2 * GFX906_ROPE_BLOCK_SIZE);
+    const dim3 block_nums(nr, n_blocks_x, 1);
+
+    const float theta_scale = powf(freq_base, -2.0f / n_dims);
+
+    if (freq_factors == nullptr) {
+        gfx906_rope_multi<forward, false, T><<<block_nums, block_dims, 0, stream>>>(
+            x, dst, ne0, ne1, ne2, s1, s2, n_dims, pos, freq_scale, ext_factor,
+            attn_factor, corr_dims, theta_scale, freq_factors, sections, is_imrope);
+    } else {
+        gfx906_rope_multi<forward, true, T><<<block_nums, block_dims, 0, stream>>>(
+            x, dst, ne0, ne1, ne2, s1, s2, n_dims, pos, freq_scale, ext_factor,
+            attn_factor, corr_dims, theta_scale, freq_factors, sections, is_imrope);
+    }
+}
+
+#endif // GGML_USE_HIP && GFX906_ROPE_ENABLED
diff --git a/ggml/src/ggml-cuda/gfx906/fused/gather-q8.cu b/ggml/src/ggml-cuda/gfx906/fused/gather-q8.cu
new file mode 100644
index 000000000..a98a67773
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/fused/gather-q8.cu
@@ -0,0 +1,97 @@
+// MoE Q8_1 gather kernel - gathers selected rows from cached quantized data
+
+#include "gather-q8.cuh"
+
+constexpr int64_t BYTES_PER_BLOCK = 144;  // sizeof(block_q8_1_mmq)
+constexpr int64_t INT4S_PER_BLOCK = 9;    // 144 / 16
+
+template<int BLOCK_SIZE>
+__global__ void gather_q8_1_kernel(
+    const char* __restrict__ src,
+    const int* __restrict__ ids,
+    char* __restrict__ dst,
+    int64_t n_blocks,
+    int64_t n_src_rows_per_slice,
+    int64_t n_dst_rows
+) {
+    const int64_t out_row = blockIdx.x;
+    if (out_row >= n_dst_rows) return;
+
+    const int src_flat_row = ids[out_row];
+    const int64_t src_slice = src_flat_row / n_src_rows_per_slice;
+    const int64_t src_row_in_slice = src_flat_row % n_src_rows_per_slice;
+    const int64_t src_slice_base = src_slice * n_src_rows_per_slice * n_blocks;
+    const int64_t total_int4s = n_blocks * INT4S_PER_BLOCK;
+
+    for (int64_t i = threadIdx.x; i < total_int4s; i += BLOCK_SIZE) {
+        const int64_t block_idx = i / INT4S_PER_BLOCK;
+        const int64_t int4_offset = i - block_idx * INT4S_PER_BLOCK;
+
+        const int64_t src_block_idx = src_slice_base + block_idx * n_src_rows_per_slice + src_row_in_slice;
+        const int64_t dst_block_idx = block_idx * n_dst_rows + out_row;
+
+        const int4* src_ptr = reinterpret_cast<const int4*>(src + src_block_idx * BYTES_PER_BLOCK) + int4_offset;
+        int4* dst_ptr = reinterpret_cast<int4*>(dst + dst_block_idx * BYTES_PER_BLOCK) + int4_offset;
+        *dst_ptr = *src_ptr;
+    }
+}
+
+template<int BLOCK_SIZE>
+__global__ void gather_q8_1_kernel_generic(
+    const char* __restrict__ src,
+    const int* __restrict__ ids,
+    char* __restrict__ dst,
+    int64_t block_size,
+    int64_t n_blocks,
+    int64_t n_src_rows_per_slice,
+    int64_t n_dst_rows
+) {
+    const int64_t out_row = blockIdx.x;
+    if (out_row >= n_dst_rows) return;
+
+    const int src_flat_row = ids[out_row];
+    const int64_t src_slice = src_flat_row / n_src_rows_per_slice;
+    const int64_t src_row_in_slice = src_flat_row % n_src_rows_per_slice;
+    const int64_t src_slice_base = src_slice * n_src_rows_per_slice * n_blocks;
+    const int64_t int4s_per_block = block_size / 16;
+    const int64_t total_int4s = n_blocks * int4s_per_block;
+
+    for (int64_t i = threadIdx.x; i < total_int4s; i += BLOCK_SIZE) {
+        const int64_t block_idx = i / int4s_per_block;
+        const int64_t int4_offset = i % int4s_per_block;
+
+        const int64_t src_block_idx = src_slice_base + block_idx * n_src_rows_per_slice + src_row_in_slice;
+        const int64_t dst_block_idx = block_idx * n_dst_rows + out_row;
+
+        const int4* src_ptr = reinterpret_cast<const int4*>(src + src_block_idx * block_size) + int4_offset;
+        int4* dst_ptr = reinterpret_cast<int4*>(dst + dst_block_idx * block_size) + int4_offset;
+        *dst_ptr = *src_ptr;
+    }
+}
+
+void gather_q8_1_rows_cuda(
+    const void* src,
+    const int* ids,
+    void* dst,
+    int64_t block_size,
+    int64_t n_blocks,
+    int64_t n_src_rows_per_slice,
+    int64_t n_dst_rows,
+    cudaStream_t stream
+) {
+    if (n_dst_rows == 0) return;
+    GGML_ASSERT(block_size % 16 == 0);
+
+    constexpr int THREADS = 256;
+    const int n_cuda_blocks = static_cast<int>(n_dst_rows);
+
+    if (block_size == 144) {
+        gather_q8_1_kernel<THREADS><<<n_cuda_blocks, THREADS, 0, stream>>>(
+            static_cast<const char*>(src), ids, static_cast<char*>(dst),
+            n_blocks, n_src_rows_per_slice, n_dst_rows);
+    } else {
+        gather_q8_1_kernel_generic<THREADS><<<n_cuda_blocks, THREADS, 0, stream>>>(
+            static_cast<const char*>(src), ids, static_cast<char*>(dst),
+            block_size, n_blocks, n_src_rows_per_slice, n_dst_rows);
+    }
+}
diff --git a/ggml/src/ggml-cuda/gfx906/fused/gather-q8.cuh b/ggml/src/ggml-cuda/gfx906/fused/gather-q8.cuh
new file mode 100644
index 000000000..2bba5dec0
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/fused/gather-q8.cuh
@@ -0,0 +1,16 @@
+#pragma once
+
+// Gather selected rows from Q8_1 quantized tensor for MoE caching
+
+#include "../../common.cuh"
+
+void gather_q8_1_rows_cuda(
+    const void* src,
+    const int* ids,
+    void* dst,
+    int64_t block_size,
+    int64_t n_blocks,
+    int64_t n_src_rows_per_slice,
+    int64_t n_dst_rows,
+    cudaStream_t stream
+);
diff --git a/ggml/src/ggml-cuda/gfx906/fused/graph-fusion.cuh b/ggml/src/ggml-cuda/gfx906/fused/graph-fusion.cuh
new file mode 100644
index 000000000..02ce1b318
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/fused/graph-fusion.cuh
@@ -0,0 +1,233 @@
+#pragma once
+
+// Graph fusion for Q8 KV/MoE caching - detects RMS_NORM -> MUL -> MUL_MAT patterns
+// Fusion decisions are cached per-graph to avoid repeated O(n²) scans
+
+#include "../quantize/q8-cache.cuh"
+#include "norm-fused-q8.cuh"
+#include "mmq-prequantized.cuh"
+#include "../../common.cuh"
+#include <unordered_map>
+#include <unordered_set>
+
+#if defined(GGML_USE_HIP) && GFX906_KVQ_MOE_CACHE_ENABLED
+
+// Cached fusion decision for a node
+struct fusion_decision {
+    mmq_q8_1_ds_layout ds_layout;
+    const ggml_tensor* mul_node;        // The MUL node following RMS_NORM
+    const ggml_tensor* mul_weight_src;  // The weight tensor for MUL
+};
+
+// Cache of fusion decisions per graph (keyed by graph pointer + n_nodes for invalidation)
+struct fusion_cache {
+    const ggml_cgraph* graph_ptr;
+    int n_nodes;
+    std::unordered_map<int, fusion_decision> decisions;  // node_idx -> decision
+};
+
+// Thread-local fusion cache
+static thread_local fusion_cache g_fusion_cache = {nullptr, 0, {}};
+
+// Analyze graph and cache all fusion decisions (called once per new graph)
+static void analyze_graph_for_fusion(ggml_cgraph* cgraph, int cc) {
+    g_fusion_cache.graph_ptr = cgraph;
+    g_fusion_cache.n_nodes = cgraph->n_nodes;
+    g_fusion_cache.decisions.clear();
+
+    // Build consumer map: tensor -> list of (consumer_node, consumer_idx)
+    std::unordered_map<const ggml_tensor*, std::vector<std::pair<ggml_tensor*, int>>> consumer_map;
+    for (int j = 0; j < cgraph->n_nodes; j++) {
+        ggml_tensor* node = cgraph->nodes[j];
+        for (int s = 0; s < GGML_MAX_SRC && node->src[s]; s++) {
+            consumer_map[node->src[s]].push_back({node, s});
+        }
+    }
+
+    // Scan for fusible patterns
+    for (int i = 0; i < cgraph->n_nodes - 1; i++) {
+        ggml_tensor* node = cgraph->nodes[i];
+
+        if (node->op != GGML_OP_RMS_NORM) {
+            continue;
+        }
+
+        ggml_tensor* rms_norm = node;
+        ggml_tensor* mul = cgraph->nodes[i + 1];
+
+        // Check pattern: RMS_NORM -> MUL
+        if (mul->op != GGML_OP_MUL || (mul->src[0] != rms_norm && mul->src[1] != rms_norm)) {
+            continue;
+        }
+
+        // Get all consumers of the MUL output
+        auto it = consumer_map.find(mul);
+        if (it == consumer_map.end()) {
+            continue;
+        }
+        const auto& mul_consumers = it->second;
+
+        // Check all consumers are MMQ-eligible MUL_MAT operations
+        std::vector<ggml_tensor*> mmq_consumers;
+        bool has_non_mmq_consumer = false;
+
+        for (const auto& [consumer, src_idx] : mul_consumers) {
+            if (consumer->op == GGML_OP_MUL_MAT && src_idx == 1) {
+                const ggml_tensor* weights = consumer->src[0];
+                if (ggml_cuda_should_use_mmq(weights->type, cc, consumer->ne[1], 1)) {
+                    mmq_consumers.push_back(consumer);
+                } else {
+                    has_non_mmq_consumer = true;
+                    break;
+                }
+            } else {
+                has_non_mmq_consumer = true;
+                break;
+            }
+        }
+
+        // Need at least 2 MMQ consumers and no non-MMQ consumers
+        if (mmq_consumers.size() < 2 || has_non_mmq_consumer) {
+            continue;
+        }
+
+        // Verify types are F32
+        if (rms_norm->src[0]->type != GGML_TYPE_F32 ||
+            rms_norm->type != GGML_TYPE_F32 ||
+            mul->type != GGML_TYPE_F32) {
+            continue;
+        }
+
+        // Get Q8_1 layout and verify consistency among consumers
+        mmq_q8_1_ds_layout ds_layout = mmq_get_q8_1_ds_layout(mmq_consumers[0]->src[0]->type);
+        bool layout_consistent = true;
+        for (size_t c = 1; c < mmq_consumers.size(); c++) {
+            if (mmq_get_q8_1_ds_layout(mmq_consumers[c]->src[0]->type) != ds_layout) {
+                layout_consistent = false;
+                break;
+            }
+        }
+        if (!layout_consistent) {
+            continue;
+        }
+
+        // Cache the fusion decision
+        fusion_decision decision;
+        decision.ds_layout = ds_layout;
+        decision.mul_node = mul;
+        decision.mul_weight_src = (mul->src[0] == rms_norm) ? mul->src[1] : mul->src[0];
+
+        g_fusion_cache.decisions[i] = decision;
+    }
+}
+
+// Check if cache is valid for this graph
+static inline bool is_cache_valid(const ggml_cgraph* cgraph) {
+    return g_fusion_cache.graph_ptr == cgraph &&
+           g_fusion_cache.n_nodes == cgraph->n_nodes;
+}
+
+// Check cached decision and execute fusion if applicable
+static inline bool try_rms_mul_mmq_fusion(
+    ggml_backend_cuda_context* cuda_ctx,
+    ggml_cgraph* cgraph,
+    int node_idx,
+    bool use_cuda_graph,
+    bool cuda_graph_update_required
+) {
+    // Rebuild cache if graph changed
+    if (!is_cache_valid(cgraph)) {
+        const int cc = ggml_cuda_info().devices[ggml_cuda_get_device()].cc;
+        analyze_graph_for_fusion(cgraph, cc);
+    }
+
+    // Fast lookup in cache
+    auto it = g_fusion_cache.decisions.find(node_idx);
+    if (it == g_fusion_cache.decisions.end()) {
+        return false;
+    }
+
+    const fusion_decision& decision = it->second;
+
+    // Skip during CUDA graph capture (pool allocations not allowed)
+    if (use_cuda_graph && cuda_graph_update_required) {
+        return false;
+    }
+
+    // Execute fusion
+    ggml_tensor* rms_norm = cgraph->nodes[node_idx];
+    const ggml_tensor* input = rms_norm->src[0];
+    const int64_t ncols = input->ne[0];
+    const int64_t nrows = ggml_nrows(input);
+    const int cc = ggml_cuda_info().devices[ggml_cuda_get_device()].cc;
+    const size_t q8_buffer_size = ggml_cuda_get_q8_1_buffer_size(ncols, nrows, cc);
+
+    auto pool_alloc = std::make_unique<ggml_cuda_pool_alloc<char>>();
+    pool_alloc->alloc(cuda_ctx->pool(), q8_buffer_size);
+    char* buffer_ptr = pool_alloc->get();
+    cuda_ctx->fusion_q8_buffers.push_back(std::move(pool_alloc));
+
+    // Store dimensions for MMQ consumers
+    prequantized_q8_info info;
+    info.buffer_ptr = buffer_ptr;
+    info.ne10 = input->ne[0];
+    info.ne11 = input->ne[1];
+    info.ne12 = input->ne[2];
+    info.ne13 = input->ne[3];
+
+    // Execute fused kernel
+    const float* mul_weights = (const float*)decision.mul_weight_src->data;
+    ggml_cuda_op_rms_norm_fused_q8_1(*cuda_ctx, rms_norm, buffer_ptr, decision.ds_layout,
+                                      mul_weights, decision.mul_weight_src);
+
+    // Store in map for MUL_MAT consumers
+    cuda_ctx->fusion_prequant_map[decision.mul_node] = info;
+    cuda_ctx->fusion_handled_mul_nodes.insert(decision.mul_node);
+
+    return true;
+}
+
+// Check if a MUL node was handled by fusion (should be skipped)
+static inline bool is_mul_handled_by_fusion(ggml_backend_cuda_context* cuda_ctx, ggml_tensor* node) {
+    if (node->op != GGML_OP_MUL) {
+        return false;
+    }
+    return cuda_ctx->fusion_handled_mul_nodes.count(node) > 0;
+}
+
+// Use prequantized data for MUL_MAT if available
+static inline bool try_prequantized_mul_mat(ggml_backend_cuda_context* cuda_ctx, ggml_tensor* node) {
+    if (node->op != GGML_OP_MUL_MAT || node->src[1] == nullptr) {
+        return false;
+    }
+
+    const int cc = ggml_cuda_info().devices[ggml_cuda_get_device()].cc;
+    if (!ggml_cuda_should_use_mmq(node->src[0]->type, cc, node->ne[1], 1)) {
+        return false;
+    }
+
+    auto it = cuda_ctx->fusion_prequant_map.find(node->src[1]);
+    if (it == cuda_ctx->fusion_prequant_map.end()) {
+        return false;
+    }
+
+    const prequantized_q8_info& info = it->second;
+
+    if (info.ne10 != node->src[1]->ne[0] || info.ne11 != node->src[1]->ne[1] ||
+        info.ne12 != node->src[1]->ne[2] || info.ne13 != node->src[1]->ne[3]) {
+        return false;
+    }
+
+    ggml_cuda_mul_mat_q_prequantized(*cuda_ctx, node->src[0], info.buffer_ptr,
+                                      node, info.ne10, info.ne11, info.ne12, info.ne13);
+    return true;
+}
+
+// Clear fusion state at start of graph compute
+static inline void clear_fusion_state(ggml_backend_cuda_context* cuda_ctx) {
+    cuda_ctx->fusion_prequant_map.clear();
+    cuda_ctx->fusion_handled_mul_nodes.clear();
+    cuda_ctx->fusion_q8_buffers.clear();
+}
+
+#endif // GGML_USE_HIP && GFX906_KVQ_MOE_CACHE_ENABLED
diff --git a/ggml/src/ggml-cuda/gfx906/fused/mmq-prequantized.cuh b/ggml/src/ggml-cuda/gfx906/fused/mmq-prequantized.cuh
new file mode 100644
index 000000000..2e3ca7dd5
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/fused/mmq-prequantized.cuh
@@ -0,0 +1,93 @@
+#pragma once
+
+// Pre-quantized MMQ path - accepts already-quantized Q8_1 data
+
+#include "../../mmq.cuh"
+
+static void mmq_switch_type_prequantized(ggml_backend_cuda_context & ctx, const mmq_args & args, cudaStream_t stream) {
+    switch (args.type_x) {
+        case GGML_TYPE_Q4_0:   mul_mat_q_case<GGML_TYPE_Q4_0>(ctx, args, stream);   break;
+        case GGML_TYPE_Q4_1:   mul_mat_q_case<GGML_TYPE_Q4_1>(ctx, args, stream);   break;
+        case GGML_TYPE_Q5_0:   mul_mat_q_case<GGML_TYPE_Q5_0>(ctx, args, stream);   break;
+        case GGML_TYPE_Q5_1:   mul_mat_q_case<GGML_TYPE_Q5_1>(ctx, args, stream);   break;
+        case GGML_TYPE_Q8_0:   mul_mat_q_case<GGML_TYPE_Q8_0>(ctx, args, stream);   break;
+        case GGML_TYPE_MXFP4:  mul_mat_q_case<GGML_TYPE_MXFP4>(ctx, args, stream);  break;
+        case GGML_TYPE_Q2_K:   mul_mat_q_case<GGML_TYPE_Q2_K>(ctx, args, stream);   break;
+        case GGML_TYPE_Q3_K:   mul_mat_q_case<GGML_TYPE_Q3_K>(ctx, args, stream);   break;
+        case GGML_TYPE_Q4_K:   mul_mat_q_case<GGML_TYPE_Q4_K>(ctx, args, stream);   break;
+        case GGML_TYPE_Q5_K:   mul_mat_q_case<GGML_TYPE_Q5_K>(ctx, args, stream);   break;
+        case GGML_TYPE_Q6_K:   mul_mat_q_case<GGML_TYPE_Q6_K>(ctx, args, stream);   break;
+        case GGML_TYPE_IQ2_XXS: mul_mat_q_case<GGML_TYPE_IQ2_XXS>(ctx, args, stream); break;
+        case GGML_TYPE_IQ2_XS: mul_mat_q_case<GGML_TYPE_IQ2_XS>(ctx, args, stream); break;
+        case GGML_TYPE_IQ2_S:  mul_mat_q_case<GGML_TYPE_IQ2_S>(ctx, args, stream);  break;
+        case GGML_TYPE_IQ3_XXS: mul_mat_q_case<GGML_TYPE_IQ3_XXS>(ctx, args, stream); break;
+        case GGML_TYPE_IQ3_S:  mul_mat_q_case<GGML_TYPE_IQ3_S>(ctx, args, stream);  break;
+        case GGML_TYPE_IQ1_S:  mul_mat_q_case<GGML_TYPE_IQ1_S>(ctx, args, stream);  break;
+        case GGML_TYPE_IQ4_NL: mul_mat_q_case<GGML_TYPE_IQ4_NL>(ctx, args, stream); break;
+        case GGML_TYPE_IQ4_XS: mul_mat_q_case<GGML_TYPE_IQ4_XS>(ctx, args, stream); break;
+        default: GGML_ABORT("unsupported type");
+    }
+}
+
+// MUL_MAT with pre-quantized Q8_1 src1 data
+static void ggml_cuda_mul_mat_q_prequantized(
+    ggml_backend_cuda_context & ctx,
+    const ggml_tensor * src0,
+    const void * src1_q8_1,
+    ggml_tensor * dst,
+    int64_t ne10, int64_t ne11, int64_t ne12, int64_t ne13
+) {
+    GGML_ASSERT(dst->type == GGML_TYPE_F32);
+
+    cudaStream_t stream = ctx.stream();
+    const int cc = ggml_cuda_info().devices[ggml_cuda_get_device()].cc;
+
+    const size_t ts_src0 = ggml_type_size(src0->type);
+    const size_t ts_dst  = ggml_type_size(dst->type);
+
+    GGML_ASSERT(src0->nb[0] == ts_src0);
+    GGML_ASSERT(dst->nb[0]  == ts_dst);
+
+    const char * src0_d = (const char *) src0->data;
+    float * dst_d = (float *) dst->data;
+
+    if (ggml_backend_buffer_get_usage(src0->buffer) == GGML_BACKEND_BUFFER_USAGE_COMPUTE) {
+        const size_t size_data  = ggml_nbytes(src0);
+        const size_t size_alloc = ggml_backend_buffer_get_alloc_size(src0->buffer, src0);
+        if (size_alloc > size_data) {
+            GGML_ASSERT(ggml_is_contiguously_allocated(src0));
+            GGML_ASSERT(!src0->view_src);
+            CUDA_CHECK(cudaMemsetAsync((char *) src0->data + size_data, 0, size_alloc - size_data, stream));
+        }
+    }
+
+    const int64_t ne00 = src0->ne[0];
+    const int64_t ne01 = src0->ne[1];
+    const int64_t ne02 = src0->ne[2];
+    const int64_t ne03 = src0->ne[3];
+    const int64_t ne1  = dst->ne[1];
+
+    const int64_t ne10_padded = GGML_PAD(ne10, MATRIX_ROW_PADDING);
+
+    const int64_t s01 = src0->nb[1] / ts_src0;
+    const int64_t s1  = dst->nb[1]  / ts_dst;
+    const int64_t s02 = src0->nb[2] / ts_src0;
+    const int64_t s2  = dst->nb[2]  / ts_dst;
+    const int64_t s03 = src0->nb[3] / ts_src0;
+    const int64_t s3  = dst->nb[3]  / ts_dst;
+
+    const bool use_stream_k = (GGML_CUDA_CC_IS_NVIDIA(cc) && ggml_cuda_highest_compiled_arch(cc) >= GGML_CUDA_CC_VOLTA)
+                            || GGML_CUDA_CC_IS_CDNA(cc);
+
+    const int64_t s12 = ne11 * ne10_padded * sizeof(block_q8_1) / (QK8_1 * sizeof(int));
+    const int64_t s13 = ne12 * s12;
+
+    const mmq_args args = {
+        src0_d, src0->type, (const int *) src1_q8_1, nullptr, nullptr, dst_d,
+        ne00, ne01, ne1, s01, ne11, s1,
+        ne02, ne12, s02, s12, s2,
+        ne03, ne13, s03, s13, s3,
+        use_stream_k, ne1};
+
+    mmq_switch_type_prequantized(ctx, args, stream);
+}
diff --git a/ggml/src/ggml-cuda/gfx906/fused/norm-fused-q8.cu b/ggml/src/ggml-cuda/gfx906/fused/norm-fused-q8.cu
new file mode 100644
index 000000000..aee9cdaa0
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/fused/norm-fused-q8.cu
@@ -0,0 +1,262 @@
+// Fused RMS Norm + Q8_1 Quantization Kernel for GFX906
+
+#include "norm-fused-q8.cuh"
+
+#ifdef GGML_USE_HIP
+#include "../gfx906-common.cuh"
+#include "../quantize/epilogue.cuh"
+#endif
+
+template <int block_size, mmq_q8_1_ds_layout ds_layout, bool do_multiply = false>
+__launch_bounds__(256, 8)
+static __global__ void rms_norm_f32_to_q8_1(
+    const float* __restrict__ x,
+    void* __restrict__ vy,
+    const int ncols,
+    const int64_t stride_row,
+    const int64_t stride_channel,
+    const int64_t stride_sample,
+    const float eps,
+    const float* __restrict__ mul = nullptr,
+    const int64_t mul_stride_row = 0,
+    const int64_t mul_stride_channel = 0,
+    const int64_t mul_stride_sample = 0,
+    const uint3 mul_ncols_packed = make_uint3(0, 0, 0),
+    const uint3 mul_nrows_packed = make_uint3(0, 0, 0),
+    const uint3 mul_nchannels_packed = make_uint3(0, 0, 0),
+    const uint3 mul_nsamples_packed = make_uint3(0, 0, 0)
+) {
+    const int nrows = gridDim.x;
+    const int nchannels = gridDim.y;
+    const int row = blockIdx.x;
+    const int channel = blockIdx.y;
+    const int sample = blockIdx.z;
+    const int tid = threadIdx.x;
+
+    const float* __restrict__ x_row = x + sample * stride_sample + channel * stride_channel + row * stride_row;
+
+    const float* __restrict__ mul_row = nullptr;
+    if constexpr (do_multiply) {
+        const uint32_t mul_row_idx = fastmodulo(row, mul_nrows_packed);
+        const uint32_t mul_channel_idx = fastmodulo(channel, mul_nchannels_packed);
+        const uint32_t mul_sample_idx = fastmodulo(sample, mul_nsamples_packed);
+        mul_row = mul + mul_sample_idx * mul_stride_sample + mul_channel_idx * mul_stride_channel + mul_row_idx * mul_stride_row;
+    }
+
+    const int64_t ncols_padded = GGML_PAD(ncols, MATRIX_ROW_PADDING);
+    const int64_t blocks_per_row = ncols_padded / (4 * QK8_1);
+    const int64_t channel_idx = (int64_t)sample * nchannels + channel;
+    const int64_t ib0 = channel_idx * (nrows * blocks_per_row);
+    block_q8_1_mmq* __restrict__ y_base = (block_q8_1_mmq*)vy + ib0;
+
+    extern __shared__ float4 s_data4[];
+    float* s_data = (float*)s_data4;
+    __shared__ float s_reduce[32];
+
+    const float4* __restrict__ x4 = (const float4*)x_row;
+    const int n_float4 = ncols / 4;
+
+    float sum_sq = 0.0f;
+    for (int idx = tid; idx < n_float4; idx += block_size) {
+        const float4 v = x4[idx];
+        s_data4[idx] = v;
+        sum_sq = __fmaf_rn(v.x, v.x, sum_sq);
+        sum_sq = __fmaf_rn(v.y, v.y, sum_sq);
+        sum_sq = __fmaf_rn(v.z, v.z, sum_sq);
+        sum_sq = __fmaf_rn(v.w, v.w, sum_sq);
+    }
+
+    const int remainder = ncols % 4;
+    if (tid < remainder) {
+        const int idx = n_float4 * 4 + tid;
+        const float v = x_row[idx];
+        s_data[idx] = v;
+        sum_sq = __fmaf_rn(v, v, sum_sq);
+    }
+    __syncthreads();
+
+    sum_sq = warp_reduce_sum(sum_sq);
+
+    const int warp_id = tid / WARP_SIZE;
+    const int lane_id = tid % WARP_SIZE;
+    constexpr int nwarps = block_size / WARP_SIZE;
+
+    if (lane_id == 0) s_reduce[warp_id] = sum_sq;
+    __syncthreads();
+
+    if (warp_id == 0) {
+        sum_sq = (lane_id < nwarps) ? s_reduce[lane_id] : 0.0f;
+        sum_sq = warp_reduce_sum(sum_sq);
+    }
+
+    float rms_scale;
+    if (tid == 0) {
+        rms_scale = rsqrtf(sum_sq / ncols + eps);
+        s_reduce[0] = rms_scale;
+    }
+    __syncthreads();
+    rms_scale = s_reduce[0];
+
+    constexpr int vals_per_iter = block_size * 4;
+    const float4* __restrict__ mul4 = do_multiply ? (const float4*)mul_row : nullptr;
+
+    for (int base_col = 0; base_col < ncols_padded; base_col += vals_per_iter) {
+        const int col = base_col + tid * 4;
+        if (col >= ncols_padded) continue;
+
+        float4 v;
+        if (col + 3 < ncols) {
+            v = s_data4[col / 4];
+        } else {
+            v.x = (col + 0 < ncols) ? s_data[col + 0] : 0.0f;
+            v.y = (col + 1 < ncols) ? s_data[col + 1] : 0.0f;
+            v.z = (col + 2 < ncols) ? s_data[col + 2] : 0.0f;
+            v.w = (col + 3 < ncols) ? s_data[col + 3] : 0.0f;
+        }
+
+        v.x *= rms_scale; v.y *= rms_scale; v.z *= rms_scale; v.w *= rms_scale;
+
+        if constexpr (do_multiply) {
+            if (col + 3 < ncols) {
+                const float4 m = mul4[col / 4];
+                v.x *= m.x; v.y *= m.y; v.z *= m.z; v.w *= m.w;
+            } else {
+                if (col + 0 < ncols) v.x *= mul_row[fastmodulo(col + 0, mul_ncols_packed)];
+                if (col + 1 < ncols) v.y *= mul_row[fastmodulo(col + 1, mul_ncols_packed)];
+                if (col + 2 < ncols) v.z *= mul_row[fastmodulo(col + 2, mul_ncols_packed)];
+                if (col + 3 < ncols) v.w *= mul_row[fastmodulo(col + 3, mul_ncols_packed)];
+            }
+        }
+
+        float amax = fmaxf(fmaxf(fabsf(v.x), fabsf(v.y)), fmaxf(fabsf(v.z), fabsf(v.w)));
+        float sum = v.x + v.y + v.z + v.w;
+
+        // 8-thread group reduction using explicit shuffle
+        #pragma unroll
+        for (int offset = 4; offset > 0; offset >>= 1) {
+            amax = fmaxf(amax, __shfl_xor_sync(0xFFFFFFFF, amax, offset, 8));
+            if constexpr (ds_layout != MMQ_Q8_1_DS_LAYOUT_D4) {
+                sum += __shfl_xor_sync(0xFFFFFFFF, sum, offset, 8);
+            }
+        }
+
+        constexpr float inv_127 = 1.0f / 127.0f;
+        const float d = amax * inv_127;
+#if defined(GGML_USE_HIP) && defined(__gfx906__)
+        const float d_inv = fast_rcp_f32(d);
+#else
+        const float d_inv = (amax > 0.0f) ? (127.0f / amax) : 0.0f;
+#endif
+
+        const char4 q = make_char4(
+            static_cast<int8_t>(__float2int_rn(v.x * d_inv)),
+            static_cast<int8_t>(__float2int_rn(v.y * d_inv)),
+            static_cast<int8_t>(__float2int_rn(v.z * d_inv)),
+            static_cast<int8_t>(__float2int_rn(v.w * d_inv))
+        );
+
+        const int block_idx = col / 128;
+        const int pos_in_block = col % 128;
+        const int group_in_block = pos_in_block / 32;
+        const int lane_in_group = (pos_in_block % 32) / 4;
+
+        if (block_idx < blocks_per_row) {
+            block_q8_1_mmq* __restrict__ block_out = &y_base[block_idx * nrows + row];
+            reinterpret_cast<char4*>(block_out->qs)[group_in_block * 8 + lane_in_group] = q;
+
+            if (lane_in_group == 0) {
+                if constexpr (ds_layout == MMQ_Q8_1_DS_LAYOUT_DS4) {
+                    block_out->ds4[group_in_block] = make_half2(__float2half(d), __float2half(sum));
+                } else if constexpr (ds_layout == MMQ_Q8_1_DS_LAYOUT_D4) {
+                    block_out->d4[group_in_block] = d;
+                }
+            }
+        }
+    }
+}
+
+void ggml_cuda_op_rms_norm_fused_q8_1(
+    ggml_backend_cuda_context& ctx,
+    const ggml_tensor* rms_norm,
+    void* q8_output,
+    mmq_q8_1_ds_layout ds_layout,
+    const float* mul_weights,
+    const ggml_tensor* mul_src
+) {
+    const ggml_tensor* src = rms_norm->src[0];
+    const float* src_d = (const float*)src->data;
+    cudaStream_t stream = ctx.stream();
+
+    GGML_ASSERT(src->type == GGML_TYPE_F32);
+
+    const int64_t ne00 = src->ne[0];
+    const int64_t ne01 = src->ne[1];
+    const int64_t ne02 = src->ne[2];
+    const int64_t ne03 = src->ne[3];
+
+    float eps = 0.0f;
+    memcpy(&eps, rms_norm->op_params, sizeof(float));
+    GGML_ASSERT(eps >= 0.0f);
+
+    const size_t ts0 = ggml_type_size(src->type);
+    GGML_ASSERT(src->nb[0] == ts0);
+    const int64_t s01 = src->nb[1] / ts0;
+    const int64_t s02 = src->nb[2] / ts0;
+    const int64_t s03 = src->nb[3] / ts0;
+
+    const dim3 blocks_num(ne01, ne02, ne03);
+    const size_t smem_size = ne00 * sizeof(float);
+    const dim3 block_dims(256, 1, 1);
+
+    if (mul_weights != nullptr && mul_src != nullptr) {
+        const size_t ts_mul = ggml_type_size(mul_src->type);
+        GGML_ASSERT(mul_src->nb[0] == ts_mul);
+        const int64_t mul_s01 = mul_src->nb[1] / ts_mul;
+        const int64_t mul_s02 = mul_src->nb[2] / ts_mul;
+        const int64_t mul_s03 = mul_src->nb[3] / ts_mul;
+
+        const uint3 mul_ncols_packed = init_fastdiv_values(mul_src->ne[0]);
+        const uint3 mul_nrows_packed = init_fastdiv_values(mul_src->ne[1]);
+        const uint3 mul_nchannels_packed = init_fastdiv_values(mul_src->ne[2]);
+        const uint3 mul_nsamples_packed = init_fastdiv_values(mul_src->ne[3]);
+
+        switch (ds_layout) {
+            case MMQ_Q8_1_DS_LAYOUT_D4:
+                rms_norm_f32_to_q8_1<256, MMQ_Q8_1_DS_LAYOUT_D4, true><<<blocks_num, block_dims, smem_size, stream>>>(
+                    src_d, q8_output, ne00, s01, s02, s03, eps, mul_weights, mul_s01, mul_s02, mul_s03,
+                    mul_ncols_packed, mul_nrows_packed, mul_nchannels_packed, mul_nsamples_packed);
+                break;
+            case MMQ_Q8_1_DS_LAYOUT_DS4:
+                rms_norm_f32_to_q8_1<256, MMQ_Q8_1_DS_LAYOUT_DS4, true><<<blocks_num, block_dims, smem_size, stream>>>(
+                    src_d, q8_output, ne00, s01, s02, s03, eps, mul_weights, mul_s01, mul_s02, mul_s03,
+                    mul_ncols_packed, mul_nrows_packed, mul_nchannels_packed, mul_nsamples_packed);
+                break;
+            case MMQ_Q8_1_DS_LAYOUT_D2S6:
+                rms_norm_f32_to_q8_1<256, MMQ_Q8_1_DS_LAYOUT_D2S6, true><<<blocks_num, block_dims, smem_size, stream>>>(
+                    src_d, q8_output, ne00, s01, s02, s03, eps, mul_weights, mul_s01, mul_s02, mul_s03,
+                    mul_ncols_packed, mul_nrows_packed, mul_nchannels_packed, mul_nsamples_packed);
+                break;
+        }
+    } else {
+        switch (ds_layout) {
+            case MMQ_Q8_1_DS_LAYOUT_D4:
+                rms_norm_f32_to_q8_1<256, MMQ_Q8_1_DS_LAYOUT_D4, false><<<blocks_num, block_dims, smem_size, stream>>>(
+                    src_d, q8_output, ne00, s01, s02, s03, eps);
+                break;
+            case MMQ_Q8_1_DS_LAYOUT_DS4:
+                rms_norm_f32_to_q8_1<256, MMQ_Q8_1_DS_LAYOUT_DS4, false><<<blocks_num, block_dims, smem_size, stream>>>(
+                    src_d, q8_output, ne00, s01, s02, s03, eps);
+                break;
+            case MMQ_Q8_1_DS_LAYOUT_D2S6:
+                rms_norm_f32_to_q8_1<256, MMQ_Q8_1_DS_LAYOUT_D2S6, false><<<blocks_num, block_dims, smem_size, stream>>>(
+                    src_d, q8_output, ne00, s01, s02, s03, eps);
+                break;
+        }
+    }
+}
+
+size_t ggml_cuda_get_q8_1_buffer_size(int64_t ncols, int64_t nrows, int cc) {
+    const int64_t ncols_padded = GGML_PAD(ncols, MATRIX_ROW_PADDING);
+    const int64_t blocks_per_row = ncols_padded / (4 * QK8_1);
+    return nrows * blocks_per_row * sizeof(block_q8_1_mmq) + get_mmq_x_max_host(cc) * sizeof(block_q8_1_mmq);
+}
diff --git a/ggml/src/ggml-cuda/gfx906/fused/norm-fused-q8.cuh b/ggml/src/ggml-cuda/gfx906/fused/norm-fused-q8.cuh
new file mode 100644
index 000000000..5456a8911
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/fused/norm-fused-q8.cuh
@@ -0,0 +1,19 @@
+#pragma once
+
+// Fused RMS Norm + Q8_1 Quantization for GFX906 KV/MoE caching
+
+#include "../../common.cuh"
+#include "../../mmq.cuh"
+
+// Fused RMS_NORM + MUL -> Q8 output
+void ggml_cuda_op_rms_norm_fused_q8_1(
+    ggml_backend_cuda_context& ctx,
+    const ggml_tensor* rms_norm,
+    void* q8_output,
+    mmq_q8_1_ds_layout ds_layout,
+    const float* mul_weights = nullptr,
+    const ggml_tensor* mul_src = nullptr
+);
+
+// Get required buffer size for Q8_1 MMQ output
+size_t ggml_cuda_get_q8_1_buffer_size(int64_t ncols, int64_t nrows, int cc);
diff --git a/ggml/src/ggml-cuda/gfx906/gfx906-common.cuh b/ggml/src/ggml-cuda/gfx906/gfx906-common.cuh
new file mode 100644
index 000000000..d471d01ce
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/gfx906-common.cuh
@@ -0,0 +1,308 @@
+#pragma once
+
+#include "gfx906-config.h"
+
+#ifdef GGML_USE_HIP
+
+static __device__ __forceinline__ float sgpr_broadcast_f32(float value) {
+    int i = __float_as_int(value);
+    i = __builtin_amdgcn_readfirstlane(i);
+    return __int_as_float(i);
+}
+
+static __device__ __forceinline__ int sgpr_broadcast_i32(int value) {
+    return __builtin_amdgcn_readfirstlane(value);
+}
+
+static __device__ __forceinline__ half sgpr_broadcast_f16(half value) {
+    int i = *reinterpret_cast<const short*>(&value);
+    i = __builtin_amdgcn_readfirstlane(i);
+    short s = static_cast<short>(i);
+    return *reinterpret_cast<half*>(&s);
+}
+
+static __device__ __forceinline__ float fast_exp_f32(float x) {
+    constexpr float LOG2_E = 1.4426950408889634f;
+    float result;
+    asm volatile(
+        "v_exp_f32 %0, %1"
+        : "=v"(result)
+        : "v"(x * LOG2_E)
+    );
+    return result;
+}
+
+static __device__ __forceinline__ float fast_exp2_f32(float x) {
+    float result;
+    asm volatile(
+        "v_exp_f32 %0, %1"
+        : "=v"(result)
+        : "v"(x)
+    );
+    return result;
+}
+
+static __device__ __forceinline__ float fast_log2_f32(float x) {
+    float result;
+    asm volatile(
+        "v_log_f32 %0, %1"
+        : "=v"(result)
+        : "v"(x)
+    );
+    return result;
+}
+
+static __device__ __forceinline__ float fast_tanh_f32(float x) {
+    if (x > 10.0f) return 1.0f;
+    if (x < -10.0f) return -1.0f;
+
+    const float exp2x = fast_exp_f32(2.0f * x);
+    return 1.0f - 2.0f / (exp2x + 1.0f);
+}
+
+static __device__ __forceinline__ float fast_rcp_f32(float x) {
+    float result;
+    asm volatile(
+        "v_rcp_f32 %0, %1"
+        : "=v"(result)
+        : "v"(x)
+    );
+    return result;
+}
+
+#define DEFINE_FUSED_DPP_F32(name, barrier, dpp_ctrl, vop_instr)           \
+    static __device__ __forceinline__ float name(float x) {                \
+        float result;                                                       \
+        asm volatile(                                                       \
+            barrier                                                         \
+            vop_instr " %0, %1, %1 " dpp_ctrl " row_mask:0xf bank_mask:0xf" \
+            : "=v"(result) : "v"(x) : "memory"                             \
+        );                                                                  \
+        return result;                                                      \
+    }
+
+DEFINE_FUSED_DPP_F32(hip_add_xor1_f32, "s_nop 4\n", "quad_perm:[1,0,3,2]", "v_add_f32_dpp")
+DEFINE_FUSED_DPP_F32(hip_max_xor1_f32, "s_nop 4\n", "quad_perm:[1,0,3,2]", "v_max_f32_dpp")
+
+DEFINE_FUSED_DPP_F32(hip_add_xor2_f32, "s_nop 1\n", "quad_perm:[2,3,0,1]", "v_add_f32_dpp")
+DEFINE_FUSED_DPP_F32(hip_max_xor2_f32, "s_nop 1\n", "quad_perm:[2,3,0,1]", "v_max_f32_dpp")
+
+DEFINE_FUSED_DPP_F32(hip_add_xor8_f32, "s_nop 1\n", "row_ror:8", "v_add_f32_dpp")
+DEFINE_FUSED_DPP_F32(hip_max_xor8_f32, "s_nop 1\n", "row_ror:8", "v_max_f32_dpp")
+
+#undef DEFINE_FUSED_DPP_F32
+
+static __device__ __forceinline__ float hip_shuffle_xor4_f32(float x) {
+    int v_src = __float_as_int(x);
+    int v_dst;
+    asm volatile(
+        "v_mov_b32 %0, %1\n"
+        "s_nop 1\n"
+        "v_mov_b32_dpp %0, %1 row_shl:4 row_mask:0xf bank_mask:0x5\n"
+        "v_mov_b32_dpp %0, %1 row_shr:4 row_mask:0xf bank_mask:0xa\n"
+        : "=v"(v_dst) : "v"(v_src) : "memory"
+    );
+    return __int_as_float(v_dst);
+}
+
+static __device__ __forceinline__ float hip_shuffle_xor16_f32(float x) {
+    int int_val = __float_as_int(x);
+    int result;
+    asm volatile(
+        "ds_swizzle_b32 %0, %1 offset:swizzle(SWAP,16)\n"
+        "s_waitcnt lgkmcnt(0)\n"
+        : "=v"(result) : "v"(int_val) : "memory"
+    );
+    return __int_as_float(result);
+}
+
+struct AddOp {
+    static __device__ __forceinline__ float apply(float a, float b) { return a + b; }
+    static __device__ __forceinline__ float xor1(float x) { return hip_add_xor1_f32(x); }
+    static __device__ __forceinline__ float xor2(float x) { return hip_add_xor2_f32(x); }
+    static __device__ __forceinline__ float xor8(float x) { return hip_add_xor8_f32(x); }
+};
+
+struct MaxOp {
+    static __device__ __forceinline__ float apply(float a, float b) { return fmaxf(a, b); }
+    static __device__ __forceinline__ float xor1(float x) { return hip_max_xor1_f32(x); }
+    static __device__ __forceinline__ float xor2(float x) { return hip_max_xor2_f32(x); }
+    static __device__ __forceinline__ float xor8(float x) { return hip_max_xor8_f32(x); }
+};
+
+template<int width = WARP_SIZE, typename Op>
+static __device__ __forceinline__ float warp_reduce_amd_f32(float x) {
+    if (width >= 2)  x = Op::xor1(x);
+    if (width >= 4)  x = Op::xor2(x);
+    if (width >= 8)  x = Op::apply(x, hip_shuffle_xor4_f32(x));
+    if (width >= 16) x = Op::xor8(x);
+    if (width >= 32) x = Op::apply(x, hip_shuffle_xor16_f32(x));
+    if (width == 64) x = Op::apply(x, __shfl_xor(x, 32, 64));
+    return x;
+}
+
+template<typename T>
+static __device__ __forceinline__ T hip_dpp_xor1(T value) {
+    static_assert(sizeof(T) == 4, "DPP operations require 32-bit types");
+    int int_val = *reinterpret_cast<int*>(&value);
+    int result;
+    asm volatile(
+        "s_nop 4\n"
+        "v_mov_b32_dpp %0, %1 quad_perm:[1,0,3,2] row_mask:0xf bank_mask:0xf"
+        : "=v"(result) : "v"(int_val) : "memory"
+    );
+    return *reinterpret_cast<T*>(&result);
+}
+
+template<typename T>
+static __device__ __forceinline__ T hip_dpp_xor2(T value) {
+    static_assert(sizeof(T) == 4, "DPP operations require 32-bit types");
+    int int_val = *reinterpret_cast<int*>(&value);
+    int result;
+    asm volatile(
+        "s_nop 1\n"
+        "v_mov_b32_dpp %0, %1 quad_perm:[2,3,0,1] row_mask:0xf bank_mask:0xf"
+        : "=v"(result) : "v"(int_val) : "memory"
+    );
+    return *reinterpret_cast<T*>(&result);
+}
+
+template<typename T>
+static __device__ __forceinline__ T hip_dpp_xor4(T value) {
+    static_assert(sizeof(T) == 4, "DPP operations require 32-bit types");
+    int v_src = *reinterpret_cast<int*>(&value);
+    int v_dst;
+    asm volatile(
+        "v_mov_b32 %0, %1\n"
+        "s_nop 1\n"
+        "v_mov_b32_dpp %0, %1 row_shl:4 row_mask:0xf bank_mask:0x5\n"
+        "v_mov_b32_dpp %0, %1 row_shr:4 row_mask:0xf bank_mask:0xa\n"
+        : "=v"(v_dst) : "v"(v_src) : "memory"
+    );
+    return *reinterpret_cast<T*>(&v_dst);
+}
+
+template<typename T>
+static __device__ __forceinline__ T hip_dpp_xor8(T value) {
+    static_assert(sizeof(T) == 4, "DPP operations require 32-bit types");
+    int int_val = *reinterpret_cast<int*>(&value);
+    int result;
+    asm volatile(
+        "s_nop 1\n"
+        "v_mov_b32_dpp %0, %1 row_ror:8 row_mask:0xf bank_mask:0xf"
+        : "=v"(result) : "v"(int_val) : "memory"
+    );
+    return *reinterpret_cast<T*>(&result);
+}
+
+template<typename T>
+static __device__ __forceinline__ T hip_dpp_xor16(T value) {
+    static_assert(sizeof(T) == 4, "DPP operations require 32-bit types");
+    int int_val = *reinterpret_cast<int*>(&value);
+    int result;
+    asm volatile(
+        "ds_swizzle_b32 %0, %1 offset:swizzle(SWAP,16)\n"
+        "s_waitcnt lgkmcnt(0)\n"
+        : "=v"(result) : "v"(int_val) : "memory"
+    );
+    return *reinterpret_cast<T*>(&result);
+}
+
+template<int width = WARP_SIZE, typename T>
+static __device__ __forceinline__ T gfx906_shfl_xor_sync(T x, int offset) {
+    switch (~offset) {
+        case ~1:  return hip_dpp_xor1(x);
+        case ~2:  return hip_dpp_xor2(x);
+        case ~4:  return hip_dpp_xor4(x);
+        case ~8:  return hip_dpp_xor8(x);
+        case ~16: return hip_dpp_xor16(x);
+        default:  return __shfl_xor(x, offset, width);
+    }
+}
+
+template<int width = WARP_SIZE>
+static __device__ __forceinline__ float gfx906_warp_reduce_sum_f32(float x) {
+    return warp_reduce_amd_f32<width, AddOp>(x);
+}
+
+template<int width = WARP_SIZE>
+static __device__ __forceinline__ float gfx906_warp_reduce_max_f32(float x) {
+    return warp_reduce_amd_f32<width, MaxOp>(x);
+}
+
+template<int width = WARP_SIZE, typename T>
+static __device__ __forceinline__ T gfx906_warp_reduce_sum_generic(T x) {
+    #pragma unroll
+    for (int offset = width/2; offset > 0; offset >>= 1) {
+        x += gfx906_shfl_xor_sync<width>(x, offset);
+    }
+    return x;
+}
+
+// ================================================================================================
+// Specialized N-thread reductions for quantization and other group operations
+// These provide optimized DPP-based reductions for common group sizes (4, 8, 16, 32)
+// ================================================================================================
+
+// Generic N-thread reduction using DPP operations (N must be power of 2)
+template<int N, typename Op>
+static __device__ __forceinline__ float warp_reduce_n_amd_f32(float x) {
+    static_assert((N & (N-1)) == 0 && N <= 64, "N must be power of 2 <= 64");
+    // Build reduction tree from largest to smallest offset
+    if constexpr (N > 32) x = Op::apply(x, __shfl_xor(x, 32, 64));
+    if constexpr (N > 16) x = Op::apply(x, hip_shuffle_xor16_f32(x));
+    if constexpr (N > 8)  x = Op::apply(x, hip_shuffle_xor4_f32(x));
+    if constexpr (N > 4)  x = Op::xor2(x);
+    if constexpr (N > 2)  x = Op::xor1(x);
+    if constexpr (N > 1)  {
+        // For N=2, we still need xor1
+        if constexpr (N == 2) x = Op::xor1(x);
+    }
+    return x;
+}
+
+// Specialized 8-thread reduction for Q8_1 quantization (most common case)
+// This is used when 8 threads cooperate to quantize a 32-element block
+template<typename Op>
+static __device__ __forceinline__ float warp_reduce_8_amd_f32(float x) {
+    // 8-thread reduction tree: xor4 -> xor2 -> xor1
+    x = Op::apply(x, hip_shuffle_xor4_f32(x));  // Reduce 8->4
+    x = Op::xor2(x);                             // Reduce 4->2  
+    x = Op::xor1(x);                             // Reduce 2->1
+    return x;
+}
+
+// Public interface: 8-thread sum reduction
+static __device__ __forceinline__ float warp_reduce_sum_8_f32(float x) {
+    return warp_reduce_8_amd_f32<AddOp>(x);
+}
+
+// Public interface: 8-thread max reduction
+static __device__ __forceinline__ float warp_reduce_max_8_f32(float x) {
+    return warp_reduce_8_amd_f32<MaxOp>(x);
+}
+
+// Generic N-thread sum reduction (compile-time N)
+template<int N>
+static __device__ __forceinline__ float warp_reduce_sum_n_f32(float x) {
+    return warp_reduce_n_amd_f32<N, AddOp>(x);
+}
+
+// Generic N-thread max reduction (compile-time N)
+template<int N>
+static __device__ __forceinline__ float warp_reduce_max_n_f32(float x) {
+    return warp_reduce_n_amd_f32<N, MaxOp>(x);
+}
+
+// Convenience aliases that match common.cuh naming
+template<int width = WARP_SIZE>
+static __device__ __forceinline__ float gfx906_warp_reduce_sum(float x) {
+    return gfx906_warp_reduce_sum_f32<width>(x);
+}
+
+template<int width = WARP_SIZE>
+static __device__ __forceinline__ float gfx906_warp_reduce_max(float x) {
+    return gfx906_warp_reduce_max_f32<width>(x);
+}
+
+#endif
diff --git a/ggml/src/ggml-cuda/gfx906/gfx906-config.h b/ggml/src/ggml-cuda/gfx906/gfx906-config.h
new file mode 100644
index 000000000..164ac9269
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/gfx906-config.h
@@ -0,0 +1,20 @@
+#pragma once
+
+// GFX906 (Vega 20 / MI50) kernel configuration
+
+#ifdef GGML_USE_HIP
+#define GFX906_MMQ_ITER_K 256
+#define GFX906_MMQ_NWARPS 2
+#define GFX906_FATTN_Q8_ENABLED 1
+#define GFX906_Q8_SUPPORTS_HEAD_DIM(d) \
+    ((d) % 32 == 0 && (d) != 40 && (d) != 80 && (d) != 112)
+
+#define GFX906_USE_DPP_REDUCTIONS 1
+#define GFX906_FATTN_TILE_SIZE_DEFAULT 128
+#define GFX906_Q8_SCALE_HOISTING 1
+#define GFX906_KVQ_MOE_CACHE_ENABLED 0
+//cache disabled cause it has problems with different models. 
+//still needs to be debugged
+#define GFX906_ROPE_ENABLED 1
+
+#endif // GGML_USE_HIP
diff --git a/ggml/src/ggml-cuda/gfx906/matmul/mmf.cuh b/ggml/src/ggml-cuda/gfx906/matmul/mmf.cuh
new file mode 100644
index 000000000..57b2b141a
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/matmul/mmf.cuh
@@ -0,0 +1,176 @@
+#pragma once
+
+// GFX906 custom FP16 GEMM kernel for medium batch sizes (9-2048)
+
+#include "../gfx906-common.cuh"
+#include "../gfx906-config.h"
+#include "sgemm.cuh"
+
+#ifdef GGML_USE_HIP
+
+#define GFX906_MMF_TILE_M 32
+#define GFX906_MMF_TILE_N 64
+#define GFX906_MMF_TILE_K 64
+
+template <int tile_m, int tile_n, int tile_k>
+__launch_bounds__(256, 4)
+static __global__ void gfx906_mul_mat_f16_packed(
+        const half * __restrict__ src0,
+        const half * __restrict__ src1,
+        float * __restrict__ dst,
+        const int M, const int N, const int K,
+        const int stride_src0_row,
+        const int stride_src1_col,
+        const int stride_dst_col) {
+
+    const int block_row = blockIdx.x * tile_m;
+    const int block_col = blockIdx.y * tile_n;
+
+    if (block_row >= M) return;
+
+    constexpr int lds_a_stride = tile_k + 4;
+    constexpr int lds_b_stride = tile_n + 4;
+
+    extern __shared__ char shared_mem[];
+    half * tile_A = (half *)shared_mem;
+    half * tile_B = tile_A + tile_m * lds_a_stride;
+
+    const int thread_row = threadIdx.x >> 3;
+    const int thread_col_group = threadIdx.x & 7;
+    const int thread_col_base = thread_col_group << 3;
+
+    float acc0 = 0.0f, acc1 = 0.0f, acc2 = 0.0f, acc3 = 0.0f;
+    float acc4 = 0.0f, acc5 = 0.0f, acc6 = 0.0f, acc7 = 0.0f;
+
+    const int out_row = block_row + thread_row;
+    const int out_col_base = block_col + thread_col_base;
+
+    for (int k_tile = 0; k_tile < K; k_tile += tile_k) {
+        {
+            const int elems_A_h2 = (tile_m * tile_k) / 2;
+
+            for (int idx = threadIdx.x; idx < elems_A_h2; idx += blockDim.x) {
+                const int flat_idx = idx * 2;
+                const int m = flat_idx / tile_k;
+                const int k = flat_idx % tile_k;
+                const int global_m = block_row + m;
+                const int global_k = k_tile + k;
+
+                half2 val;
+                if (global_m < M && global_k + 1 < K) {
+                    val = *((const half2 *)&src0[global_k + global_m * stride_src0_row]);
+                } else if (global_m < M && global_k < K) {
+                    val = __halves2half2(src0[global_k + global_m * stride_src0_row], __float2half(0.0f));
+                } else {
+                    val = __float2half2_rn(0.0f);
+                }
+
+                tile_A[m * lds_a_stride + k] = __low2half(val);
+                tile_A[m * lds_a_stride + k + 1] = __high2half(val);
+            }
+        }
+
+        {
+            const int elems_B = tile_k * tile_n;
+            for (int idx = threadIdx.x; idx < elems_B; idx += blockDim.x) {
+                const int k = idx / tile_n;
+                const int n = idx % tile_n;
+                const int global_k = k_tile + k;
+                const int global_n = block_col + n;
+
+                half val = (global_k < K && global_n < N) ?
+                    src1[global_k + global_n * stride_src1_col] : __float2half(0.0f);
+                tile_B[k * lds_b_stride + n] = val;
+            }
+        }
+
+        __syncthreads();
+
+        if (thread_row < tile_m) {
+            const half * a_row = &tile_A[thread_row * lds_a_stride];
+
+            #pragma unroll 4
+            for (int kk = 0; kk < tile_k; kk++) {
+                const float a_val = __half2float(a_row[kk]);
+                const half * b_row = &tile_B[kk * lds_b_stride + thread_col_base];
+
+                acc0 = __fmaf_rn(a_val, __half2float(b_row[0]), acc0);
+                acc1 = __fmaf_rn(a_val, __half2float(b_row[1]), acc1);
+                acc2 = __fmaf_rn(a_val, __half2float(b_row[2]), acc2);
+                acc3 = __fmaf_rn(a_val, __half2float(b_row[3]), acc3);
+                acc4 = __fmaf_rn(a_val, __half2float(b_row[4]), acc4);
+                acc5 = __fmaf_rn(a_val, __half2float(b_row[5]), acc5);
+                acc6 = __fmaf_rn(a_val, __half2float(b_row[6]), acc6);
+                acc7 = __fmaf_rn(a_val, __half2float(b_row[7]), acc7);
+            }
+        }
+
+        __syncthreads();
+    }
+
+    if (out_row < M) {
+        if (out_col_base + 0 < N) dst[out_row + (out_col_base + 0) * stride_dst_col] = acc0;
+        if (out_col_base + 1 < N) dst[out_row + (out_col_base + 1) * stride_dst_col] = acc1;
+        if (out_col_base + 2 < N) dst[out_row + (out_col_base + 2) * stride_dst_col] = acc2;
+        if (out_col_base + 3 < N) dst[out_row + (out_col_base + 3) * stride_dst_col] = acc3;
+        if (out_col_base + 4 < N) dst[out_row + (out_col_base + 4) * stride_dst_col] = acc4;
+        if (out_col_base + 5 < N) dst[out_row + (out_col_base + 5) * stride_dst_col] = acc5;
+        if (out_col_base + 6 < N) dst[out_row + (out_col_base + 6) * stride_dst_col] = acc6;
+        if (out_col_base + 7 < N) dst[out_row + (out_col_base + 7) * stride_dst_col] = acc7;
+    }
+}
+
+static bool gfx906_mmf_dispatch(
+        const half * src0,
+        const half * src1,
+        float * dst,
+        const int M, const int N, const int K,
+        const int stride_src0,
+        const int stride_src1,
+        const int stride_dst,
+        cudaStream_t stream) {
+
+    if (N < 9 || N > 2048) {
+        return false;
+    }
+
+    if (K < 32 || M < 16) {
+        return false;
+    }
+
+    constexpr int tile_m = GFX906_MMF_TILE_M;
+    constexpr int tile_n = GFX906_MMF_TILE_N;
+    constexpr int tile_k = GFX906_MMF_TILE_K;
+
+    const int grid_m = (M + tile_m - 1) / tile_m;
+    const int grid_n = (N + tile_n - 1) / tile_n;
+
+    dim3 grid(grid_m, grid_n);
+    dim3 block(256);
+
+    const size_t shmem_size = (tile_m * (tile_k + 4) + tile_k * (tile_n + 4)) * sizeof(half);
+
+    gfx906_mul_mat_f16_packed<tile_m, tile_n, tile_k><<<grid, block, shmem_size, stream>>>(
+        src0, src1, dst,
+        M, N, K,
+        stride_src0, stride_src1, stride_dst
+    );
+
+    return true;
+}
+
+static bool gfx906_sgemm_dispatch(
+        const float * src0,
+        const float * src1,
+        float * dst,
+        const int M, const int N, const int K,
+        const int stride_src0,
+        const int stride_src1,
+        const int stride_dst,
+        cudaStream_t stream) {
+
+    return gfx906_sgemm_custom_dispatch(src0, src1, dst, M, N, K,
+                                        stride_src0, stride_src1, stride_dst, stream);
+}
+
+#endif // GGML_USE_HIP
diff --git a/ggml/src/ggml-cuda/gfx906/matmul/mmq-prefetch.cuh b/ggml/src/ggml-cuda/gfx906/matmul/mmq-prefetch.cuh
new file mode 100644
index 000000000..582f81ee9
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/matmul/mmq-prefetch.cuh
@@ -0,0 +1,231 @@
+#pragma once
+
+// Y-tile prefetch for MMQ: issues global_load_dword for next iteration
+// Hides memory latency by overlapping loads with compute
+
+#include "../gfx906-config.h"
+
+#if defined(GGML_USE_HIP) && defined(__gfx906__)
+
+template<int mmq_x, int mmq_tile_y_k, int nwarps, int warp_size>
+static __device__ __forceinline__ int gfx906_prefetch_y_tile_v4(
+    const int * __restrict__ y,
+    const int ncols_y,
+    const int kb0,
+    const int kb0_stop,
+    const int qk,
+    const int blocks_per_iter) {
+
+    const int kb0_next = kb0 + blocks_per_iter;
+
+    if (kb0_next >= kb0_stop) {
+        return 0;
+    }
+
+    constexpr int block_q8_1_mmq_bytes = 144;
+    constexpr int QK8_1_val = 32;
+    const int stride_factor = qk * block_q8_1_mmq_bytes / (4 * QK8_1_val * sizeof(int));
+
+    const int * by_next = y + ncols_y * (kb0_next * stride_factor);
+
+    // Use 16 threads from warp 0 to prefetch 16 cache lines (1KB total)
+    // This uses ~16 spare VGPRs to warm L2 cache for next iteration
+    const int lane_id = threadIdx.x;
+    if (threadIdx.y != 0 || lane_id >= 16) {
+        return 0;
+    }
+
+    // Each thread prefetches a different cache line (64 bytes apart = 16 ints)
+    const int prefetch_offset = lane_id * 16;
+    const int * prefetch_addr = by_next + prefetch_offset;
+
+    int prefetch_data;
+    asm volatile(
+        "global_load_dword %0, %1, off\n"
+        : "=v"(prefetch_data)
+        : "v"(prefetch_addr)
+        : "memory"
+    );
+    return prefetch_data;
+}
+
+// Prefetch second Y tile (the +sz offset one)
+// Uses remaining threads from warp 0 (lanes 16-31) to prefetch second half
+template<int mmq_x, int mmq_tile_y_k, int nwarps, int warp_size>
+static __device__ __forceinline__ int gfx906_prefetch_y_tile_second(
+    const int * __restrict__ y,
+    const int ncols_y,
+    const int kb0,
+    const int kb0_stop,
+    const int qk,
+    const int blocks_per_iter,
+    const int sz) {
+
+    const int kb0_next = kb0 + blocks_per_iter;
+
+    if (kb0_next >= kb0_stop) {
+        return 0;
+    }
+
+    constexpr int block_q8_1_mmq_bytes = 144;
+    constexpr int QK8_1_val = 32;
+    const int stride_factor = qk * block_q8_1_mmq_bytes / (4 * QK8_1_val * sizeof(int));
+
+    // Prefetch the second Y tile (at +sz offset)
+    const int * by_next_second = y + ncols_y * (kb0_next * stride_factor + sz);
+
+    // Use lanes 16-31 from warp 0 for second tile prefetch
+    const int lane_id = threadIdx.x;
+    if (threadIdx.y != 0 || lane_id < 16 || lane_id >= 32) {
+        return 0;
+    }
+
+    const int prefetch_offset = (lane_id - 16) * 16;
+    const int * prefetch_addr = by_next_second + prefetch_offset;
+
+    int prefetch_data;
+    asm volatile(
+        "global_load_dword %0, %1, off\n"
+        : "=v"(prefetch_data)
+        : "v"(prefetch_addr)
+        : "memory"
+    );
+    return prefetch_data;
+}
+
+static __device__ __forceinline__ void gfx906_prefetch_consume(int prefetch_data) {
+    asm volatile(
+        "v_mov_b32 %0, %0\n"
+        : "+v"(prefetch_data)
+    );
+}
+
+template<int mmq_x, int mmq_tile_y_k, int nwarps, int warp_size>
+static __device__ __forceinline__ void gfx906_prefetch_y_tile_v2(
+    const int * __restrict__ y,
+    const int ncols_y,
+    const int kb0,
+    const int kb0_stop,
+    const int qk,
+    const int blocks_per_iter) {
+
+    const int kb0_next = kb0 + blocks_per_iter;
+
+    if (kb0_next >= kb0_stop) {
+        return;
+    }
+
+    const int tid = threadIdx.y * warp_size + threadIdx.x;
+
+    constexpr int total_elements = mmq_x * mmq_tile_y_k;
+    if (tid >= total_elements) {
+        return;
+    }
+
+    asm volatile("s_waitcnt vmcnt(0)" ::: "memory");
+
+    constexpr int block_q8_1_mmq_bytes = 144;
+    constexpr int QK8_1_val = 32;
+    const int stride_factor = qk * block_q8_1_mmq_bytes / (4 * QK8_1_val * sizeof(int));
+
+    const int * by_next = y + ncols_y * (kb0_next * stride_factor);
+    const int * prefetch_addr = by_next + tid;
+
+    int dummy;
+    asm volatile(
+        "global_load_dword %0, %1, off\n"
+        : "=v"(dummy)
+        : "v"(prefetch_addr)
+        : "memory"
+    );
+}
+
+template<int mmq_x, int mmq_tile_y_k, int nwarps, int warp_size>
+static __device__ __forceinline__ void gfx906_prefetch_y_tile_v1(
+    const int * __restrict__ y,
+    const int ncols_y,
+    const int kb0,
+    const int kb0_stop,
+    const int qk,
+    const int blocks_per_iter) {
+
+    const int kb0_next = kb0 + blocks_per_iter;
+
+    if (kb0_next >= kb0_stop) {
+        return;
+    }
+
+    const int tid = threadIdx.y * warp_size + threadIdx.x;
+    constexpr int total_elements = mmq_x * mmq_tile_y_k;
+
+    if (tid >= total_elements) {
+        return;
+    }
+
+    constexpr int block_q8_1_mmq_bytes = 144;
+    constexpr int QK8_1_val = 32;
+    const int stride_factor = qk * block_q8_1_mmq_bytes / (4 * QK8_1_val * sizeof(int));
+
+    const int * by_next = y + ncols_y * (kb0_next * stride_factor);
+    const int * prefetch_addr = by_next + tid;
+
+    int dummy;
+    asm volatile(
+        "global_load_dword %0, %1, off\n"
+        : "=v"(dummy)
+        : "v"(prefetch_addr)
+        : "memory"
+    );
+}
+
+template<int mmq_x, int mmq_tile_y_k, int nwarps, int warp_size>
+static __device__ __forceinline__ void gfx906_prefetch_y_tile_noop(
+    const int * __restrict__ y,
+    const int ncols_y,
+    const int kb0,
+    const int kb0_stop,
+    const int qk,
+    const int blocks_per_iter) {
+    (void)y; (void)ncols_y; (void)kb0; (void)kb0_stop; (void)qk; (void)blocks_per_iter;
+}
+
+// X-tile prefetch: warm L2 cache for next iteration's X data
+// Uses warp 1 threads to prefetch X tile data while warp 0 prefetches Y
+template<int mmq_y>
+static __device__ __forceinline__ int gfx906_prefetch_x_tile(
+    const char * __restrict__ x,
+    const int offset_x,
+    const int kb0,
+    const int kb0_stop,
+    const int blocks_per_iter,
+    const int stride_row_x) {
+
+    const int kb0_next = kb0 + blocks_per_iter;
+
+    if (kb0_next >= kb0_stop) {
+        return 0;
+    }
+
+    // Use 16 threads from warp 1 to prefetch X tile
+    const int lane_id = threadIdx.x;
+    if (threadIdx.y != 1 || lane_id >= 16) {
+        return 0;
+    }
+
+    // Prefetch X data for next iteration - different rows
+    // Each thread prefetches from a different row (stride by row)
+    const int row = lane_id;
+    const char * x_row = x + (offset_x + kb0_next) + row * stride_row_x;
+    const int * x_ptr = (const int *)x_row;
+
+    int prefetch_data;
+    asm volatile(
+        "global_load_dword %0, %1, off\n"
+        : "=v"(prefetch_data)
+        : "v"(x_ptr)
+        : "memory"
+    );
+    return prefetch_data;
+}
+
+#endif // defined(GGML_USE_HIP) && defined(__gfx906__)
diff --git a/ggml/src/ggml-cuda/gfx906/matmul/mmq.cuh b/ggml/src/ggml-cuda/gfx906/matmul/mmq.cuh
new file mode 100644
index 000000000..eef1577ef
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/matmul/mmq.cuh
@@ -0,0 +1,93 @@
+#pragma once
+
+// MMQ vectorized loads: 2x int4 (128-bit) instead of 8x scalar loads
+// Q8_0 software pipelining: separate load/store phases for better MLP
+
+#include "../gfx906-config.h"
+#include "../quantize/vecdotq.cuh"
+
+#if defined(GGML_USE_HIP)
+
+static __device__ __forceinline__ void gfx906_load_q4_0_quants_vectorized(
+    const int * __restrict__ y_qs,
+    const int base_addr,
+    const int qi,
+    int * __restrict__ u) {
+
+    const int4 vec0 = *((const int4 *) &y_qs[base_addr]);
+    const int4 vec1 = *((const int4 *) &y_qs[base_addr + qi]);
+
+    u[0] = vec0.x; u[2] = vec0.y; u[4] = vec0.z; u[6] = vec0.w;
+    u[1] = vec1.x; u[3] = vec1.y; u[5] = vec1.z; u[7] = vec1.w;
+}
+
+static __device__ __forceinline__ void gfx906_load_q4_1_quants_vectorized(
+    const int * __restrict__ y_qs,
+    const int base_addr,
+    const int qi,
+    int * __restrict__ u) {
+
+    const int4 vec0 = *((const int4 *) &y_qs[base_addr]);
+    const int4 vec1 = *((const int4 *) &y_qs[base_addr + qi]);
+
+    u[0] = vec0.x; u[2] = vec0.y; u[4] = vec0.z; u[6] = vec0.w;
+    u[1] = vec1.x; u[3] = vec1.y; u[5] = vec1.z; u[7] = vec1.w;
+}
+
+template<int VDR>
+static __device__ __forceinline__ void gfx906_load_quants_vectorized(
+    const int * __restrict__ y_qs,
+    const int base_addr,
+    const int qi,
+    int * __restrict__ u) {
+
+    static_assert(VDR == 4, "Only VDR=4 supported for vectorized loads");
+
+    const int4 vec0 = *((const int4 *) &y_qs[base_addr]);
+    const int4 vec1 = *((const int4 *) &y_qs[base_addr + qi]);
+
+    u[0] = vec0.x; u[2] = vec0.y; u[4] = vec0.z; u[6] = vec0.w;
+    u[1] = vec1.x; u[3] = vec1.y; u[5] = vec1.z; u[7] = vec1.w;
+}
+
+#if defined(__gfx906__)
+
+#define GFX906_LOAD_TILES_Q8_0_ASYNC(cache_size, nrows, nwarps, threads_per_row, need_check, \
+    x, kbx0, stride, i_max, txi, kbx, kqsx, qs0_cache, qs1_cache, i_slot_cache) \
+    do { \
+        _Pragma("unroll") \
+        for (int iter = 0; iter < cache_size; iter++) { \
+            const int i0 = iter * nrows * nwarps; \
+            const int i_slot = i0 + (nrows == 1 ? threadIdx.y : threadIdx.y*nrows + threadIdx.x/threads_per_row); \
+            const int i_read = need_check ? min(i_slot, i_max) : i_slot; \
+            const bool oob = need_check && (i_slot > i_max); \
+            const block_q8_0 * bxi = (const block_q8_0 *) x + kbx0 + i_read*stride + kbx; \
+            qs0_cache[iter] = oob ? 0 : gfx906_get_int_b2_fast(bxi[0].qs, kqsx); \
+            qs1_cache[iter] = oob ? 0 : gfx906_get_int_b2_fast(bxi[MMQ_TILE_NE_K/QI8_0].qs, kqsx); \
+            i_slot_cache[iter] = i_slot; \
+        } \
+    } while(0)
+
+#define GFX906_STORE_TILES_Q8_0_LDS_MMA(cache_size, x_qs, qs0_cache, qs1_cache, i_slot_cache, txi) \
+    do { \
+        _Pragma("unroll") \
+        for (int iter = 0; iter < cache_size; iter++) { \
+            const int i_slot = i_slot_cache[iter]; \
+            x_qs[i_slot*MMQ_MMA_TILE_X_K_Q8_0 + 0             + txi] = qs0_cache[iter]; \
+            x_qs[i_slot*MMQ_MMA_TILE_X_K_Q8_0 + MMQ_TILE_NE_K + txi] = qs1_cache[iter]; \
+        } \
+    } while(0)
+
+#define GFX906_STORE_TILES_Q8_0_LDS_LEGACY(cache_size, x_qs, qs0_cache, qs1_cache, i_slot_cache, txi) \
+    do { \
+        _Pragma("unroll") \
+        for (int iter = 0; iter < cache_size; iter++) { \
+            const int i_slot = i_slot_cache[iter]; \
+            x_qs[i_slot*(2*MMQ_TILE_NE_K + 1) + 0             + txi] = qs0_cache[iter]; \
+            x_qs[i_slot*(2*MMQ_TILE_NE_K + 1) + MMQ_TILE_NE_K + txi] = qs1_cache[iter]; \
+        } \
+    } while(0)
+
+#endif // defined(__gfx906__)
+
+#endif // GGML_USE_HIP
diff --git a/ggml/src/ggml-cuda/gfx906/matmul/mmvq-q4_0.cuh b/ggml/src/ggml-cuda/gfx906/matmul/mmvq-q4_0.cuh
new file mode 100644
index 000000000..0d2f5b158
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/matmul/mmvq-q4_0.cuh
@@ -0,0 +1,119 @@
+#pragma once
+
+// GFX906 Warp-Cooperative Q4_1 GEMV Kernel
+// Uses half-warp (32 threads) per row for better memory coalescing
+// Achieves better bandwidth improvement over sequential per-thread approach, which results in faster performance for small matrixes (ncols less than 1024)
+// This kernel is only included from mmvq.cu where all dependencies are available
+
+#if defined(GGML_USE_HIP)
+
+__launch_bounds__(64, 1)
+static __global__ void gfx906_mul_mat_vec_q4_0_warp_coop(
+        const void * __restrict__ vx, const void * __restrict__ vy,
+        const int32_t * __restrict__ ids,
+        float * __restrict__ dst,
+        const uint32_t ncols_x, const uint3 nchannels_y,
+        const uint32_t stride_row_x,
+        const uint32_t stride_col_dst, const uint3 channel_ratio,
+        const uint32_t stride_channel_x, const uint32_t stride_channel_y,
+        const uint32_t stride_channel_dst, const uint3 sample_ratio,
+        const uint32_t stride_sample_x, const uint32_t stride_sample_y,
+        const uint32_t stride_sample_dst, const uint32_t nrows_x) {
+
+    constexpr int qk_q4_0 = 32;
+
+    const int lane_id = threadIdx.x;
+    const int half_lane = lane_id % 32;
+    const int row_offset = lane_id / 32;
+
+    const int row = blockIdx.x * 2 + row_offset;
+
+    if (row >= (int)nrows_x) return;
+
+    const uint32_t channel_dst = blockIdx.y;
+    const uint32_t channel_x   = ids ? ids[channel_dst] : fastdiv(channel_dst, channel_ratio);
+    const uint32_t channel_y   = ids ? fastmodulo(channel_dst, nchannels_y) : channel_dst;
+    const uint32_t sample_dst  = blockIdx.z;
+    const uint32_t sample_x    = fastdiv(sample_dst, sample_ratio);
+    const uint32_t sample_y    = sample_dst;
+
+    const int blocks_per_row = ncols_x / qk_q4_0;
+    const int kbx_offset = sample_x * stride_sample_x + channel_x * stride_channel_x + row * stride_row_x;
+
+    const block_q4_0 * x = (const block_q4_0 *)vx + kbx_offset;
+    const block_q8_1 * y = (const block_q8_1 *)vy + sample_y * stride_sample_y + channel_y * stride_channel_y;
+
+    float sumf = 0.0f;
+
+    for (int ib = half_lane; ib < blocks_per_row; ib += 32) {
+        const block_q4_0 * bq4 = x + ib;
+        const block_q8_1 * bq8 = y + ib;
+
+        // Load 16 bytes of Q4_0 quantized values (32 nibbles)
+        int v0, v1, v2, v3;
+        memcpy(&v0, bq4->qs +  0, 4);
+        memcpy(&v1, bq4->qs +  4, 4);
+        memcpy(&v2, bq4->qs +  8, 4);
+        memcpy(&v3, bq4->qs + 12, 4);
+
+        // Load 32 bytes of Q8_1 quantized values
+        const int * q8 = (const int *)bq8->qs;
+        const int u0 = q8[0];
+        const int u1 = q8[1];
+        const int u2 = q8[2];
+        const int u3 = q8[3];
+        const int u4 = q8[4];
+        const int u5 = q8[5];
+        const int u6 = q8[6];
+        const int u7 = q8[7];
+
+        // Compute dot product (8 dp4a for full 32 values)
+        int sumi = 0;
+        sumi = ggml_cuda_dp4a((v0 >> 0) & 0x0F0F0F0F, u0, sumi);
+        sumi = ggml_cuda_dp4a((v0 >> 4) & 0x0F0F0F0F, u4, sumi);
+        sumi = ggml_cuda_dp4a((v1 >> 0) & 0x0F0F0F0F, u1, sumi);
+        sumi = ggml_cuda_dp4a((v1 >> 4) & 0x0F0F0F0F, u5, sumi);
+        sumi = ggml_cuda_dp4a((v2 >> 0) & 0x0F0F0F0F, u2, sumi);
+        sumi = ggml_cuda_dp4a((v2 >> 4) & 0x0F0F0F0F, u6, sumi);
+        sumi = ggml_cuda_dp4a((v3 >> 0) & 0x0F0F0F0F, u3, sumi);
+        sumi = ggml_cuda_dp4a((v3 >> 4) & 0x0F0F0F0F, u7, sumi);
+
+        // Q4_0 formula: d4 * (sumi * d8 - 8 * s8)
+        // where s8 is the sum of Q8 values (stored in bq8->ds.y)
+        const float d4 = bq4->d;
+        const float2 ds8 = __half22float2(bq8->ds);
+        sumf += d4 * (sumi * ds8.x - 8.0f * ds8.y);
+    }
+
+    // Half-warp reduction using fused DPP instructions
+    sumf = warp_reduce_sum<32>(sumf);
+
+    if (half_lane == 0) {
+        dst[sample_dst * stride_sample_dst + channel_dst * stride_channel_dst + row] = sumf;
+    }
+}
+
+static void gfx906_launch_mul_mat_vec_q4_0_warp_coop(
+        const void * vx, const void * vy, const int32_t * ids,
+        float * dst,
+        const uint32_t ncols_x, const uint3 nchannels_y,
+        const uint32_t stride_row_x,
+        const uint32_t stride_col_dst, const uint3 channel_ratio,
+        const uint32_t stride_channel_x, const uint32_t stride_channel_y,
+        const uint32_t stride_channel_dst, const uint3 sample_ratio,
+        const uint32_t stride_sample_x, const uint32_t stride_sample_y,
+        const uint32_t stride_sample_dst, const uint32_t nrows_x,
+        const uint32_t nchannels_dst, const uint32_t nsamples_dst,
+        cudaStream_t stream) {
+
+    const dim3 block_dims(64, 1, 1);
+    const dim3 block_nums((nrows_x + 1) / 2, nchannels_dst, nsamples_dst);
+
+    gfx906_mul_mat_vec_q4_0_warp_coop<<<block_nums, block_dims, 0, stream>>>(
+        vx, vy, ids, dst, ncols_x, nchannels_y, stride_row_x,
+        stride_col_dst, channel_ratio, stride_channel_x, stride_channel_y,
+        stride_channel_dst, sample_ratio, stride_sample_x, stride_sample_y,
+        stride_sample_dst, nrows_x);
+}
+
+#endif // GGML_USE_HIP
diff --git a/ggml/src/ggml-cuda/gfx906/matmul/mmvq-q4_1.cuh b/ggml/src/ggml-cuda/gfx906/matmul/mmvq-q4_1.cuh
new file mode 100644
index 000000000..72752554c
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/matmul/mmvq-q4_1.cuh
@@ -0,0 +1,122 @@
+#pragma once
+
+// GFX906 Warp-Cooperative Q4_1 GEMV Kernel
+// Uses half-warp (32 threads) per row for better memory coalescing
+// Achieves better bandwidth improvement over sequential per-thread approach, which results in faster performance for small matrixes (ncols less than 1024)
+// This kernel is only included from mmvq.cu where all dependencies are available
+
+#if defined(GGML_USE_HIP)
+
+__launch_bounds__(64, 1)
+static __global__ void gfx906_mul_mat_vec_q4_1_warp_coop(
+        const void * __restrict__ vx, const void * __restrict__ vy,
+        const int32_t * __restrict__ ids,
+        float * __restrict__ dst,
+        const uint32_t ncols_x, const uint3 nchannels_y,
+        const uint32_t stride_row_x,
+        const uint32_t stride_col_dst, const uint3 channel_ratio,
+        const uint32_t stride_channel_x, const uint32_t stride_channel_y,
+        const uint32_t stride_channel_dst, const uint3 sample_ratio,
+        const uint32_t stride_sample_x, const uint32_t stride_sample_y,
+        const uint32_t stride_sample_dst, const uint32_t nrows_x) {
+
+    constexpr int qk_q4_1 = 32;
+
+    const int lane_id = threadIdx.x;
+    const int half_lane = lane_id % 32;      
+    const int row_offset = lane_id / 32;     
+    const int row = blockIdx.x * 2 + row_offset;
+
+    if (row >= (int)nrows_x) return;
+
+    const uint32_t channel_dst = blockIdx.y;
+    const uint32_t channel_x   = ids ? ids[channel_dst] : fastdiv(channel_dst, channel_ratio);
+    const uint32_t channel_y   = ids ? fastmodulo(channel_dst, nchannels_y) : channel_dst;
+    const uint32_t sample_dst  = blockIdx.z;
+    const uint32_t sample_x    = fastdiv(sample_dst, sample_ratio);
+    const uint32_t sample_y    = sample_dst;
+
+    const int blocks_per_row = ncols_x / qk_q4_1;
+    const int kbx_offset = sample_x * stride_sample_x + channel_x * stride_channel_x + row * stride_row_x;
+
+    const block_q4_1 * x = (const block_q4_1 *)vx + kbx_offset;
+    const block_q8_1 * y = (const block_q8_1 *)vy + sample_y * stride_sample_y + channel_y * stride_channel_y;
+
+    float sumf = 0.0f;
+
+    for (int ib = half_lane; ib < blocks_per_row; ib += 32) {
+        const block_q4_1 * bq4 = x + ib;
+        const block_q8_1 * bq8 = y + ib;  
+
+        // Load ALL 16 bytes of Q4_1 quantized values (32 nibbles = 32 values)
+        int v0, v1, v2, v3;
+        memcpy(&v0, bq4->qs +  0, 4);  // bytes 0-3:   values 0-3 (low), 16-19 (high)
+        memcpy(&v1, bq4->qs +  4, 4);  // bytes 4-7:   values 4-7 (low), 20-23 (high)
+        memcpy(&v2, bq4->qs +  8, 4);  // bytes 8-11:  values 8-11 (low), 24-27 (high)
+        memcpy(&v3, bq4->qs + 12, 4);  // bytes 12-15: values 12-15 (low), 28-31 (high)
+
+        // Load ALL 32 bytes of Q8_1 quantized values (32 int8 values)
+        const int * q8 = (const int *)bq8->qs;
+        const int u0 = q8[0];  // Q8 values 0-3
+        const int u1 = q8[1];  // Q8 values 4-7
+        const int u2 = q8[2];  // Q8 values 8-11
+        const int u3 = q8[3];  // Q8 values 12-15
+        const int u4 = q8[4];  // Q8 values 16-19
+        const int u5 = q8[5];  // Q8 values 20-23
+        const int u6 = q8[6];  // Q8 values 24-27
+        const int u7 = q8[7];  // Q8 values 28-31
+
+        // Compute dot product with nibble extraction (8 dp4a for full 32 values)
+        int sumi = 0;
+        sumi = ggml_cuda_dp4a((v0 >> 0) & 0x0F0F0F0F, u0, sumi);  // Q4 0-3 * Q8 0-3
+        sumi = ggml_cuda_dp4a((v0 >> 4) & 0x0F0F0F0F, u4, sumi);  // Q4 16-19 * Q8 16-19
+        sumi = ggml_cuda_dp4a((v1 >> 0) & 0x0F0F0F0F, u1, sumi);  // Q4 4-7 * Q8 4-7
+        sumi = ggml_cuda_dp4a((v1 >> 4) & 0x0F0F0F0F, u5, sumi);  // Q4 20-23 * Q8 20-23
+        sumi = ggml_cuda_dp4a((v2 >> 0) & 0x0F0F0F0F, u2, sumi);  // Q4 8-11 * Q8 8-11
+        sumi = ggml_cuda_dp4a((v2 >> 4) & 0x0F0F0F0F, u6, sumi);  // Q4 24-27 * Q8 24-27
+        sumi = ggml_cuda_dp4a((v3 >> 0) & 0x0F0F0F0F, u3, sumi);  // Q4 12-15 * Q8 12-15
+        sumi = ggml_cuda_dp4a((v3 >> 4) & 0x0F0F0F0F, u7, sumi);  // Q4 28-31 * Q8 28-31
+
+        // Load and apply scale/bias: Q4_1 has (d, m), Q8_1 has (d, s)
+        // Full block formula: result = sumi * d4 * d8 + m4 * s8
+        const float2 dm4 = __half22float2(bq4->dm);
+        const float2 ds8 = __half22float2(bq8->ds);
+        sumf += sumi * dm4.x * ds8.x + dm4.y * ds8.y;
+    }
+
+    // Half-warp reduction using fused DPP instructions
+    sumf = warp_reduce_sum<32>(sumf);
+
+    // First thread of each half-warp writes result
+    if (half_lane == 0) {
+        dst[sample_dst * stride_sample_dst + channel_dst * stride_channel_dst + row] = sumf;
+    }
+}
+
+// Host-side dispatch function for GFX906 Q4_1 warp-cooperative kernel
+static void gfx906_launch_mul_mat_vec_q4_1_warp_coop(
+        const void * vx, const void * vy, const int32_t * ids,
+        float * dst,
+        const uint32_t ncols_x, const uint3 nchannels_y,
+        const uint32_t stride_row_x,
+        const uint32_t stride_col_dst, const uint3 channel_ratio,
+        const uint32_t stride_channel_x, const uint32_t stride_channel_y,
+        const uint32_t stride_channel_dst, const uint3 sample_ratio,
+        const uint32_t stride_sample_x, const uint32_t stride_sample_y,
+        const uint32_t stride_sample_dst, const uint32_t nrows_x,
+        const uint32_t nchannels_dst, const uint32_t nsamples_dst,
+        cudaStream_t stream) {
+
+    // 2 rows per block, 64 threads per block (1 warp processing 2 rows)
+    const dim3 block_dims(64, 1, 1);
+    const dim3 block_nums((nrows_x + 1) / 2, nchannels_dst, nsamples_dst);
+
+    gfx906_mul_mat_vec_q4_1_warp_coop<<<block_nums, block_dims, 0, stream>>>(
+        vx, vy, ids, dst, ncols_x, nchannels_y, stride_row_x,
+        stride_col_dst, channel_ratio, stride_channel_x, stride_channel_y,
+        stride_channel_dst, sample_ratio, stride_sample_x, stride_sample_y,
+        stride_sample_dst, nrows_x);
+}
+
+#endif // GGML_USE_HIP
+ 
\ No newline at end of file
diff --git a/ggml/src/ggml-cuda/gfx906/matmul/mmvq-q8_0.cuh b/ggml/src/ggml-cuda/gfx906/matmul/mmvq-q8_0.cuh
new file mode 100644
index 000000000..08bf4d92a
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/matmul/mmvq-q8_0.cuh
@@ -0,0 +1,120 @@
+#pragma once
+
+// GFX906 Warp-Cooperative Q8_0 GEMV Kernel
+// Simpler than Q4 variants - no nibble extraction needed
+
+#if defined(GGML_USE_HIP)
+
+__launch_bounds__(64, 1)
+static __global__ void gfx906_mul_mat_vec_q8_0_warp_coop(
+        const void * __restrict__ vx, const void * __restrict__ vy,
+        const int32_t * __restrict__ ids,
+        float * __restrict__ dst,
+        const uint32_t ncols_x, const uint3 nchannels_y,
+        const uint32_t stride_row_x,
+        const uint32_t stride_col_dst, const uint3 channel_ratio,
+        const uint32_t stride_channel_x, const uint32_t stride_channel_y,
+        const uint32_t stride_channel_dst, const uint3 sample_ratio,
+        const uint32_t stride_sample_x, const uint32_t stride_sample_y,
+        const uint32_t stride_sample_dst, const uint32_t nrows_x) {
+
+    constexpr int qk_q8_0 = 32;
+
+    const int lane_id = threadIdx.x;
+    const int half_lane = lane_id % 32;
+    const int row_offset = lane_id / 32;
+
+    const int row = blockIdx.x * 2 + row_offset;
+
+    if (row >= (int)nrows_x) return;
+
+    const uint32_t channel_dst = blockIdx.y;
+    const uint32_t channel_x   = ids ? ids[channel_dst] : fastdiv(channel_dst, channel_ratio);
+    const uint32_t channel_y   = ids ? fastmodulo(channel_dst, nchannels_y) : channel_dst;
+    const uint32_t sample_dst  = blockIdx.z;
+    const uint32_t sample_x    = fastdiv(sample_dst, sample_ratio);
+    const uint32_t sample_y    = sample_dst;
+
+    const int blocks_per_row = ncols_x / qk_q8_0;
+    const int kbx_offset = sample_x * stride_sample_x + channel_x * stride_channel_x + row * stride_row_x;
+
+    const block_q8_0 * x = (const block_q8_0 *)vx + kbx_offset;
+    const block_q8_1 * y = (const block_q8_1 *)vy + sample_y * stride_sample_y + channel_y * stride_channel_y;
+
+    float sumf = 0.0f;
+
+    for (int ib = half_lane; ib < blocks_per_row; ib += 32) {
+        const block_q8_0 * bq8_0 = x + ib;
+        const block_q8_1 * bq8_1 = y + ib;
+
+        // Load 32 bytes of Q8_0 quantized values (32 int8 values)
+        const int * v = (const int *)bq8_0->qs;
+        const int v0 = v[0];
+        const int v1 = v[1];
+        const int v2 = v[2];
+        const int v3 = v[3];
+        const int v4 = v[4];
+        const int v5 = v[5];
+        const int v6 = v[6];
+        const int v7 = v[7];
+
+        // Load 32 bytes of Q8_1 quantized values
+        const int * u = (const int *)bq8_1->qs;
+        const int u0 = u[0];
+        const int u1 = u[1];
+        const int u2 = u[2];
+        const int u3 = u[3];
+        const int u4 = u[4];
+        const int u5 = u[5];
+        const int u6 = u[6];
+        const int u7 = u[7];
+
+        // Compute dot product (8 dp4a for full 32 values)
+        int sumi = 0;
+        sumi = ggml_cuda_dp4a(v0, u0, sumi);
+        sumi = ggml_cuda_dp4a(v1, u1, sumi);
+        sumi = ggml_cuda_dp4a(v2, u2, sumi);
+        sumi = ggml_cuda_dp4a(v3, u3, sumi);
+        sumi = ggml_cuda_dp4a(v4, u4, sumi);
+        sumi = ggml_cuda_dp4a(v5, u5, sumi);
+        sumi = ggml_cuda_dp4a(v6, u6, sumi);
+        sumi = ggml_cuda_dp4a(v7, u7, sumi);
+
+        // Q8_0 formula: d0 * d1 * sumi (simple!)
+        const float d0 = bq8_0->d;
+        const float d1 = __low2float(bq8_1->ds);
+        sumf += d0 * d1 * (float)sumi;
+    }
+
+    // Half-warp reduction using fused DPP instructions
+    sumf = warp_reduce_sum<32>(sumf);
+
+    if (half_lane == 0) {
+        dst[sample_dst * stride_sample_dst + channel_dst * stride_channel_dst + row] = sumf;
+    }
+}
+
+static void gfx906_launch_mul_mat_vec_q8_0_warp_coop(
+        const void * vx, const void * vy, const int32_t * ids,
+        float * dst,
+        const uint32_t ncols_x, const uint3 nchannels_y,
+        const uint32_t stride_row_x,
+        const uint32_t stride_col_dst, const uint3 channel_ratio,
+        const uint32_t stride_channel_x, const uint32_t stride_channel_y,
+        const uint32_t stride_channel_dst, const uint3 sample_ratio,
+        const uint32_t stride_sample_x, const uint32_t stride_sample_y,
+        const uint32_t stride_sample_dst, const uint32_t nrows_x,
+        const uint32_t nchannels_dst, const uint32_t nsamples_dst,
+        cudaStream_t stream) {
+
+    const dim3 block_dims(64, 1, 1);
+    const dim3 block_nums((nrows_x + 1) / 2, nchannels_dst, nsamples_dst);
+
+    gfx906_mul_mat_vec_q8_0_warp_coop<<<block_nums, block_dims, 0, stream>>>(
+        vx, vy, ids, dst, ncols_x, nchannels_y, stride_row_x,
+        stride_col_dst, channel_ratio, stride_channel_x, stride_channel_y,
+        stride_channel_dst, sample_ratio, stride_sample_x, stride_sample_y,
+        stride_sample_dst, nrows_x);
+}
+
+#endif // GGML_USE_HIP
diff --git a/ggml/src/ggml-cuda/gfx906/matmul/sgemm.cuh b/ggml/src/ggml-cuda/gfx906/matmul/sgemm.cuh
new file mode 100644
index 000000000..a2e23cf9c
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/matmul/sgemm.cuh
@@ -0,0 +1,248 @@
+#pragma once
+
+// GFX906 custom SGEMM kernels - up to 2x faster than rocBLAS for small matrices
+// Optimized for: M <= 64, N <= 512, K <= 256
+// Key optimization: 2-way unrolled inner loop for better ILP on GFX906
+
+#ifdef GGML_USE_HIP
+
+#define SGEMM_M_TILE 32
+#define SGEMM_N_TILE 32
+#define SGEMM_K_TILE 64
+
+// Fast path: no bounds checking (requires M%32==0, N%32==0, K%64==0)
+__attribute__((used))
+__global__ __launch_bounds__(256, 2)
+void gfx906_sgemm_tiled_fast(
+    const float * __restrict__ A,
+    const float * __restrict__ B,
+    float * __restrict__ C,
+    const int stride_A,
+    const int stride_B,
+    const int stride_C,
+    const int K)
+{
+    __shared__ float As[SGEMM_K_TILE][SGEMM_M_TILE + 1];
+    __shared__ float Bs[SGEMM_K_TILE][SGEMM_N_TILE + 1];
+
+    const int tid = threadIdx.x;
+    const int bx = blockIdx.x;
+    const int by = blockIdx.y;
+
+    const int c_row_base = (tid % 16) * 2;
+    const int c_col_base = (tid / 16) * 2;
+
+    const int gm_base = bx * SGEMM_M_TILE;
+    const int gn_base = by * SGEMM_N_TILE;
+
+    float acc00 = 0.0f, acc01 = 0.0f, acc10 = 0.0f, acc11 = 0.0f;
+
+    for (int kt = 0; kt < K; kt += SGEMM_K_TILE) {
+        #pragma unroll
+        for (int i = 0; i < 8; i++) {
+            int idx = tid * 8 + i;
+            int m = idx % SGEMM_M_TILE;
+            int k = idx / SGEMM_M_TILE;
+            As[k][m] = __ldg(&A[(kt + k) + (gm_base + m) * stride_A]);
+        }
+
+        #pragma unroll
+        for (int i = 0; i < 8; i++) {
+            int idx = tid * 8 + i;
+            int k = idx % SGEMM_K_TILE;
+            int n = idx / SGEMM_K_TILE;
+            Bs[k][n] = __ldg(&B[(kt + k) + (gn_base + n) * stride_B]);
+        }
+
+        __syncthreads();
+
+        // 2-way unrolled inner loop for better ILP on GFX906
+        #pragma unroll
+        for (int k = 0; k < SGEMM_K_TILE; k += 2) {
+            float a0_0 = As[k][c_row_base + 0];
+            float a0_1 = As[k][c_row_base + 1];
+            float b0_0 = Bs[k][c_col_base + 0];
+            float b0_1 = Bs[k][c_col_base + 1];
+
+            float a1_0 = As[k+1][c_row_base + 0];
+            float a1_1 = As[k+1][c_row_base + 1];
+            float b1_0 = Bs[k+1][c_col_base + 0];
+            float b1_1 = Bs[k+1][c_col_base + 1];
+
+            acc00 = __fmaf_rn(a0_0, b0_0, acc00);
+            acc01 = __fmaf_rn(a0_0, b0_1, acc01);
+            acc10 = __fmaf_rn(a0_1, b0_0, acc10);
+            acc11 = __fmaf_rn(a0_1, b0_1, acc11);
+
+            acc00 = __fmaf_rn(a1_0, b1_0, acc00);
+            acc01 = __fmaf_rn(a1_0, b1_1, acc01);
+            acc10 = __fmaf_rn(a1_1, b1_0, acc10);
+            acc11 = __fmaf_rn(a1_1, b1_1, acc11);
+        }
+
+        __syncthreads();
+    }
+
+    const int out_m = gm_base + c_row_base;
+    const int out_n = gn_base + c_col_base;
+
+    C[(out_m + 0) + (out_n + 0) * stride_C] = acc00;
+    C[(out_m + 1) + (out_n + 0) * stride_C] = acc10;
+    C[(out_m + 0) + (out_n + 1) * stride_C] = acc01;
+    C[(out_m + 1) + (out_n + 1) * stride_C] = acc11;
+}
+
+// Safe path: with bounds checking for non-aligned dimensions
+__attribute__((used))
+__global__ __launch_bounds__(256, 2)
+void gfx906_sgemm_tiled(
+    const float * __restrict__ A,
+    const float * __restrict__ B,
+    float * __restrict__ C,
+    const int M, const int N, const int K,
+    const int stride_A,
+    const int stride_B,
+    const int stride_C)
+{
+    __shared__ float As[SGEMM_K_TILE][SGEMM_M_TILE + 1];
+    __shared__ float Bs[SGEMM_K_TILE][SGEMM_N_TILE + 1];
+
+    const int tid = threadIdx.x;
+    const int bx = blockIdx.x;
+    const int by = blockIdx.y;
+
+    const int c_row_base = (tid % 16) * 2;
+    const int c_col_base = (tid / 16) * 2;
+
+    const int gm_base = bx * SGEMM_M_TILE;
+    const int gn_base = by * SGEMM_N_TILE;
+
+    if (gm_base >= M) return;
+
+    float acc00 = 0.0f, acc01 = 0.0f, acc10 = 0.0f, acc11 = 0.0f;
+
+    const int num_k_tiles = (K + SGEMM_K_TILE - 1) / SGEMM_K_TILE;
+
+    for (int kt = 0; kt < num_k_tiles; kt++) {
+        const int k_base = kt * SGEMM_K_TILE;
+
+        #pragma unroll
+        for (int i = 0; i < 8; i++) {
+            int idx = tid * 8 + i;
+            int m = idx % SGEMM_M_TILE;
+            int k = idx / SGEMM_M_TILE;
+            int global_m = gm_base + m;
+            int global_k = k_base + k;
+
+            float val = 0.0f;
+            if (global_m < M && global_k < K) {
+                val = A[global_k + global_m * stride_A];
+            }
+            As[k][m] = val;
+        }
+
+        #pragma unroll
+        for (int i = 0; i < 8; i++) {
+            int idx = tid * 8 + i;
+            int k = idx % SGEMM_K_TILE;
+            int n = idx / SGEMM_K_TILE;
+            int global_n = gn_base + n;
+            int global_k = k_base + k;
+
+            float val = 0.0f;
+            if (global_k < K && global_n < N) {
+                val = B[global_k + global_n * stride_B];
+            }
+            Bs[k][n] = val;
+        }
+
+        __syncthreads();
+
+        // 2-way unrolled inner loop (bounds-safe version)
+        #pragma unroll
+        for (int k = 0; k < SGEMM_K_TILE; k += 2) {
+            float a0_0 = As[k][c_row_base + 0];
+            float a0_1 = As[k][c_row_base + 1];
+            float b0_0 = Bs[k][c_col_base + 0];
+            float b0_1 = Bs[k][c_col_base + 1];
+
+            float a1_0 = As[k+1][c_row_base + 0];
+            float a1_1 = As[k+1][c_row_base + 1];
+            float b1_0 = Bs[k+1][c_col_base + 0];
+            float b1_1 = Bs[k+1][c_col_base + 1];
+
+            acc00 = __fmaf_rn(a0_0, b0_0, acc00);
+            acc01 = __fmaf_rn(a0_0, b0_1, acc01);
+            acc10 = __fmaf_rn(a0_1, b0_0, acc10);
+            acc11 = __fmaf_rn(a0_1, b0_1, acc11);
+
+            acc00 = __fmaf_rn(a1_0, b1_0, acc00);
+            acc01 = __fmaf_rn(a1_0, b1_1, acc01);
+            acc10 = __fmaf_rn(a1_1, b1_0, acc10);
+            acc11 = __fmaf_rn(a1_1, b1_1, acc11);
+        }
+
+        __syncthreads();
+    }
+
+    const int out_m = gm_base + c_row_base;
+    const int out_n = gn_base + c_col_base;
+
+    if (out_m < M && out_n < N)
+        C[(out_m + 0) + (out_n + 0) * stride_C] = acc00;
+    if (out_m + 1 < M && out_n < N)
+        C[(out_m + 1) + (out_n + 0) * stride_C] = acc10;
+    if (out_m < M && out_n + 1 < N)
+        C[(out_m + 0) + (out_n + 1) * stride_C] = acc01;
+    if (out_m + 1 < M && out_n + 1 < N)
+        C[(out_m + 1) + (out_n + 1) * stride_C] = acc11;
+}
+
+// Dispatch function - returns true if handled by custom kernel
+// Dispatch bounds tuned via benchmarking against rocBLAS on MI50
+static bool gfx906_sgemm_custom_dispatch(
+        const float * src0,
+        const float * src1,
+        float * dst,
+        const int M, const int N, const int K,
+        const int stride_src0,
+        const int stride_src1,
+        const int stride_dst,
+        cudaStream_t stream) {
+
+    if (M < 32 || N < 32 || K < 64) {
+        return false;
+    }
+
+    if (M > 128 || N > 2048 || K > 8192) {
+        return false;
+    }
+
+    const int grid_m = (M + SGEMM_M_TILE - 1) / SGEMM_M_TILE;
+    const int grid_n = (N + SGEMM_N_TILE - 1) / SGEMM_N_TILE;
+
+    dim3 grid(grid_m, grid_n);
+    dim3 block(256);
+
+    const bool aligned = (M % SGEMM_M_TILE == 0) &&
+                         (N % SGEMM_N_TILE == 0) &&
+                         (K % SGEMM_K_TILE == 0);
+
+    if (aligned) {
+        gfx906_sgemm_tiled_fast<<<grid, block, 0, stream>>>(
+            src0, src1, dst,
+            stride_src0, stride_src1, stride_dst,
+            K
+        );
+    } else {
+        gfx906_sgemm_tiled<<<grid, block, 0, stream>>>(
+            src0, src1, dst,
+            M, N, K,
+            stride_src0, stride_src1, stride_dst
+        );
+    }
+
+    return true;
+}
+
+#endif // GGML_USE_HIP
diff --git a/ggml/src/ggml-cuda/gfx906/quantize/epilogue.cuh b/ggml/src/ggml-cuda/gfx906/quantize/epilogue.cuh
new file mode 100644
index 000000000..a11664de0
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/quantize/epilogue.cuh
@@ -0,0 +1,135 @@
+#pragma once
+
+// GFX906 Q8_1 quantization epilogue functions using DPP reductions.
+// Allows producer kernels to quantize FP32 values directly to Q8_1 format.
+
+#include "../gfx906-config.h"
+#include "../gfx906-common.cuh"
+#include "../../mmq.cuh"
+
+#ifdef GGML_USE_HIP
+
+// Quantizes 32 FP32 values across 8 threads (4 values per thread) to Q8_1.
+// Thread organization: 8 threads per group, each owns 4 consecutive values.
+// Lane 0 writes the scale/sum metadata.
+template <mmq_q8_1_ds_layout ds_layout>
+static __device__ __forceinline__ void quantize_q8_1_epilogue_32vals(
+    float vals[4],
+    int8_t* __restrict__ q_out,
+    void* __restrict__ ds_out,
+    const int lane_in_group
+) {
+    float amax = fmaxf(fmaxf(fabsf(vals[0]), fabsf(vals[1])),
+                       fmaxf(fabsf(vals[2]), fabsf(vals[3])));
+    float sum = vals[0] + vals[1] + vals[2] + vals[3];
+
+    // 8-thread DPP reduction using inline assembly for maximum performance
+    // The fused DPP operations (v_max_f32_dpp/v_add_f32_dpp) are ~4x faster than separate shuffle+op
+    int amax_i = __float_as_int(amax);
+    int sum_i = __float_as_int(sum);
+
+    if constexpr (ds_layout == MMQ_Q8_1_DS_LAYOUT_D4) {
+        int amax_tmp;
+        asm volatile(
+            "v_mov_b32 %0, %2\n"
+            "s_nop 1\n"
+            "v_mov_b32_dpp %0, %2 row_shl:4 row_mask:0xf bank_mask:0x5\n"
+            "v_mov_b32_dpp %0, %2 row_shr:4 row_mask:0xf bank_mask:0xa\n"
+            "v_max_f32 %1, %2, %0\n"
+            "s_nop 1\n"
+            "v_max_f32_dpp %1, %1, %1 quad_perm:[2,3,0,1] row_mask:0xf bank_mask:0xf\n"
+            "s_nop 1\n"
+            "v_max_f32_dpp %1, %1, %1 quad_perm:[1,0,3,2] row_mask:0xf bank_mask:0xf\n"
+            : "=&v"(amax_tmp), "=v"(amax_i)
+            : "v"(amax_i)
+            : "memory"
+        );
+        amax = __int_as_float(amax_i);
+    } else {
+        int amax_tmp, sum_tmp;
+        asm volatile(
+            "v_mov_b32 %0, %4\n"
+            "v_mov_b32 %1, %5\n"
+            "s_nop 1\n"
+            "v_mov_b32_dpp %0, %4 row_shl:4 row_mask:0xf bank_mask:0x5\n"
+            "v_mov_b32_dpp %1, %5 row_shl:4 row_mask:0xf bank_mask:0x5\n"
+            "v_mov_b32_dpp %0, %4 row_shr:4 row_mask:0xf bank_mask:0xa\n"
+            "v_mov_b32_dpp %1, %5 row_shr:4 row_mask:0xf bank_mask:0xa\n"
+            "v_max_f32 %2, %4, %0\n"
+            "v_add_f32 %3, %5, %1\n"
+            "s_nop 1\n"
+            "v_max_f32_dpp %2, %2, %2 quad_perm:[2,3,0,1] row_mask:0xf bank_mask:0xf\n"
+            "v_add_f32_dpp %3, %3, %3 quad_perm:[2,3,0,1] row_mask:0xf bank_mask:0xf\n"
+            "s_nop 1\n"
+            "v_max_f32_dpp %2, %2, %2 quad_perm:[1,0,3,2] row_mask:0xf bank_mask:0xf\n"
+            "v_add_f32_dpp %3, %3, %3 quad_perm:[1,0,3,2] row_mask:0xf bank_mask:0xf\n"
+            : "=&v"(amax_tmp), "=&v"(sum_tmp), "=v"(amax_i), "=v"(sum_i)
+            : "v"(amax_i), "v"(sum_i)
+            : "memory"
+        );
+        amax = __int_as_float(amax_i);
+        sum = __int_as_float(sum_i);
+    }
+
+    constexpr float inv_127 = 1.0f / 127.0f;
+    const float d = amax * inv_127;
+    const float d_inv = fast_rcp_f32(d);
+
+    const int q0 = __float2int_rn(vals[0] * d_inv);
+    const int q1 = __float2int_rn(vals[1] * d_inv);
+    const int q2 = __float2int_rn(vals[2] * d_inv);
+    const int q3 = __float2int_rn(vals[3] * d_inv);
+
+    const int offset = lane_in_group * 4;
+    q_out[offset + 0] = static_cast<int8_t>(q0);
+    q_out[offset + 1] = static_cast<int8_t>(q1);
+    q_out[offset + 2] = static_cast<int8_t>(q2);
+    q_out[offset + 3] = static_cast<int8_t>(q3);
+
+    if (lane_in_group == 0) {
+        if constexpr (ds_layout == MMQ_Q8_1_DS_LAYOUT_DS4) {
+            *reinterpret_cast<half2*>(ds_out) = make_half2(__float2half(d), __float2half(sum));
+        } else if constexpr (ds_layout == MMQ_Q8_1_DS_LAYOUT_D4) {
+            *reinterpret_cast<float*>(ds_out) = d;
+        }
+    }
+}
+
+// Helper to compute Q8_1 buffer size for a given shape
+static __host__ __forceinline__ size_t get_q8_1_mmq_buffer_size(
+    int64_t ncols, int64_t nrows, int cc
+) {
+    const int64_t ncols_padded = GGML_PAD(ncols, MATRIX_ROW_PADDING);
+    const size_t row_size = (ncols_padded / QK8_1) * sizeof(block_q8_1) +
+                            get_mmq_x_max_host(cc) * sizeof(block_q8_1_mmq);
+    return nrows * row_size;
+}
+
+// Variant for 128-value blocks (block_q8_1_mmq format).
+// 32 threads process 128 values, subdivided into 4 groups of 8.
+template <mmq_q8_1_ds_layout ds_layout>
+static __device__ __forceinline__ void quantize_q8_1_epilogue_128vals(
+    float vals[4],
+    block_q8_1_mmq* __restrict__ block_out,
+    const int tid_in_block
+) {
+    const int group_idx = tid_in_block / 8;
+    const int lane_in_group = tid_in_block % 8;
+
+    int8_t* q_out = block_out->qs + group_idx * 32;
+
+    void* ds_out = nullptr;
+    if constexpr (ds_layout == MMQ_Q8_1_DS_LAYOUT_DS4) {
+        ds_out = &block_out->ds4[group_idx];
+    } else if constexpr (ds_layout == MMQ_Q8_1_DS_LAYOUT_D4) {
+        ds_out = &block_out->d4[group_idx];
+    }
+
+    quantize_q8_1_epilogue_32vals<ds_layout>(vals, q_out, ds_out, lane_in_group);
+}
+
+static __device__ __forceinline__ float broadcast_scale_from_lane0(float scale, int lane_in_group) {
+    return __shfl(scale, 0, 8);
+}
+
+#endif // GGML_USE_HIP
diff --git a/ggml/src/ggml-cuda/gfx906/quantize/q8-cache.cuh b/ggml/src/ggml-cuda/gfx906/quantize/q8-cache.cuh
new file mode 100644
index 000000000..675ca09ab
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/quantize/q8-cache.cuh
@@ -0,0 +1,113 @@
+#pragma once
+
+// Q8_1 activation cache for GFX906 - reuses quantized data across MUL_MAT ops
+
+#include <unordered_map>
+#include <unordered_set>
+#include <vector>
+#include <memory>
+#include <cstdint>
+
+#if defined(GGML_USE_HIP)
+#include <hip/hip_runtime.h>
+#define Q8_CACHE_CHECK(err) do { if ((err) != hipSuccess) { fprintf(stderr, "HIP error: %s\n", hipGetErrorString(err)); abort(); } } while(0)
+#define Q8_CACHE_MALLOC(ptr, size) Q8_CACHE_CHECK(hipMalloc(ptr, size))
+#define Q8_CACHE_FREE(ptr) Q8_CACHE_CHECK(hipFree(ptr))
+#else
+#include <cuda_runtime.h>
+#define Q8_CACHE_CHECK(err) do { if ((err) != cudaSuccess) { fprintf(stderr, "CUDA error: %s\n", cudaGetErrorString(err)); abort(); } } while(0)
+#define Q8_CACHE_MALLOC(ptr, size) Q8_CACHE_CHECK(cudaMalloc(ptr, size))
+#define Q8_CACHE_FREE(ptr) Q8_CACHE_CHECK(cudaFree(ptr))
+#endif
+
+struct ggml_tensor;
+
+struct q8_cache_key {
+    const ggml_tensor* src_tensor;
+    int layout;
+    bool operator==(const q8_cache_key& other) const {
+        return src_tensor == other.src_tensor && layout == other.layout;
+    }
+};
+
+struct q8_cache_key_hash {
+    size_t operator()(const q8_cache_key& k) const {
+        return std::hash<const void*>{}(k.src_tensor) ^ (std::hash<int>{}(k.layout) << 1);
+    }
+};
+
+struct q8_cache_entry {
+    void* q8_data = nullptr;
+    size_t buffer_size = 0;
+    int64_t ne10_padded = 0;
+    int64_t ne11 = 0;
+    int64_t ne12 = 0;
+    int64_t ne13 = 0;
+};
+
+// Epoch-based buffer pool: buffers reusable after SAFE_EPOCH_DELAY graphs
+struct q8_hashmap_cache {
+    std::unordered_map<q8_cache_key, q8_cache_entry, q8_cache_key_hash> entries;
+
+    struct buffer_slot {
+        void* ptr = nullptr;
+        size_t size = 0;
+        uint64_t written_epoch = 0;
+    };
+    std::vector<buffer_slot> buffer_pool;
+
+    uint64_t current_epoch = 0;
+    static constexpr uint64_t SAFE_EPOCH_DELAY = 2;
+
+    void clear() {
+        entries.clear();
+        current_epoch++;
+    }
+
+    void* get_buffer(size_t size) {
+        for (auto& slot : buffer_pool) {
+            if (slot.size >= size && current_epoch - slot.written_epoch >= SAFE_EPOCH_DELAY) {
+                slot.written_epoch = current_epoch;
+                return slot.ptr;
+            }
+        }
+        void* ptr = nullptr;
+        Q8_CACHE_MALLOC(&ptr, size);
+        buffer_pool.push_back({ptr, size, current_epoch});
+        return ptr;
+    }
+
+    const q8_cache_entry* lookup(const ggml_tensor* tensor, int layout,
+                                  int64_t ne10p, int64_t ne11, int64_t ne12, int64_t ne13) {
+        auto it = entries.find({tensor, layout});
+        if (it != entries.end()) {
+            const auto& e = it->second;
+            if (e.ne10_padded == ne10p && e.ne11 == ne11 && e.ne12 == ne12 && e.ne13 == ne13) {
+                return &e;
+            }
+        }
+        return nullptr;
+    }
+
+    void store(const ggml_tensor* tensor, int layout, void* data, size_t size,
+               int64_t ne10p, int64_t ne11, int64_t ne12, int64_t ne13) {
+        entries[{tensor, layout}] = {data, size, ne10p, ne11, ne12, ne13};
+    }
+
+    void free_all() {
+        for (auto& slot : buffer_pool) {
+            if (slot.ptr) Q8_CACHE_FREE(slot.ptr);
+        }
+        buffer_pool.clear();
+        entries.clear();
+        current_epoch = 0;
+    }
+
+    size_t size() const { return entries.size(); }
+};
+
+// Multi-consumer fusion info
+struct prequantized_q8_info {
+    char* buffer_ptr;
+    int64_t ne10, ne11, ne12, ne13;
+};
diff --git a/ggml/src/ggml-cuda/gfx906/quantize/vecdotq.cuh b/ggml/src/ggml-cuda/gfx906/quantize/vecdotq.cuh
new file mode 100644
index 000000000..d9f731b92
--- /dev/null
+++ b/ggml/src/ggml-cuda/gfx906/quantize/vecdotq.cuh
@@ -0,0 +1,70 @@
+#pragma once
+
+// MXFP4 dequantization using v_perm_b32 for 8-entry table lookup
+// Unaligned memory loads via memcpy (compiler optimizes to flat_load)
+
+#include "../gfx906-config.h"
+
+#if defined(GGML_USE_HIP) && defined(__gfx906__)
+
+static __device__ __forceinline__ int gfx906_get_int_b1_fast(const void * x, const int & i32) {
+    const uint8_t * x8 = (const uint8_t *) x;
+    int x32;
+    memcpy(&x32, x8 + 4*i32, 4);
+    return x32;
+}
+
+static __device__ __forceinline__ int gfx906_get_int_b2_fast(const void * x, const int & i32) {
+    int x32;
+    memcpy(&x32, (const uint8_t*)x + 4*i32, 4);
+    return x32;
+}
+
+// 64-bit vectorized load - emits global_load_dwordx2 when aligned
+// Used for double-throughput memory access in MMQ tile loading
+static __device__ __forceinline__ int2 gfx906_load_int2(const void * x, const int & i32) {
+    int2 x64;
+    memcpy(&x64, (const uint8_t*)x + 4*i32, 8);
+    return x64;
+}
+
+__constant__ uint8_t gfx906_mxfp4_magnitudes[8] = { 0, 1, 2, 3, 4, 6, 8, 12 };
+
+static __device__ __forceinline__ int2 gfx906_get_int_from_mxfp4_table(const uint32_t q4) {
+    const uint32_t *mags32 = (const uint32_t *)gfx906_mxfp4_magnitudes;
+
+    const uint32_t q_even = q4;
+    const uint32_t q_odd  = q4 >> 4;
+
+    uint32_t sign_even = (q_even >> 3) & 0x01010101;
+    uint32_t sign_odd  = (q_odd  >> 3) & 0x01010101;
+
+    const uint32_t sel_even = q_even & 0x07070707;
+    const uint32_t sel_odd  = q_odd  & 0x07070707;
+
+    uint32_t mag_even = __builtin_amdgcn_perm(mags32[1], mags32[0], sel_even);
+    uint32_t mag_odd  = __builtin_amdgcn_perm(mags32[1], mags32[0], sel_odd);
+
+    const uint32_t mask_even = sign_even * 0xFFu;
+    const uint32_t mask_odd  = sign_odd  * 0xFFu;
+
+    uint32_t res_x = (mag_even ^ mask_even) + sign_even;
+    uint32_t res_y = (mag_odd  ^ mask_odd)  + sign_odd;
+
+    return make_int2(res_x, res_y);
+}
+
+#define GFX906_VEC_DOT_MXFP4_Q8_1(bq4, bq8_1, iqs, sumi) \
+    do { \
+        const int * q8 = (const int *) bq8_1->qs + iqs; \
+        const int aux_q4_0 = gfx906_get_int_b1_fast(bq4->qs, iqs + 0); \
+        const int aux_q4_1 = gfx906_get_int_b1_fast(bq4->qs, iqs + 1); \
+        const int2 v0 = gfx906_get_int_from_mxfp4_table(aux_q4_0); \
+        const int2 v1 = gfx906_get_int_from_mxfp4_table(aux_q4_1); \
+        sumi = ggml_cuda_dp4a(v0.x, q8[0], sumi); \
+        sumi = ggml_cuda_dp4a(v0.y, q8[4], sumi); \
+        sumi = ggml_cuda_dp4a(v1.x, q8[1], sumi); \
+        sumi = ggml_cuda_dp4a(v1.y, q8[5], sumi); \
+    } while(0)
+
+#endif
diff --git a/ggml/src/ggml-cuda/ggml-cuda.cu b/ggml/src/ggml-cuda/ggml-cuda.cu
index 1bcd1ab1f..96c86957b 100644
--- a/ggml/src/ggml-cuda/ggml-cuda.cu
+++ b/ggml/src/ggml-cuda/ggml-cuda.cu
@@ -58,8 +58,17 @@
 #include "ggml-cuda/pad_reflect_1d.cuh"
 #include "ggml-cuda/solve_tri.cuh"
 #include "ggml-cuda/tri.cuh"
+
+#ifdef GGML_USE_HIP
+#include "ggml-cuda/gfx906/matmul/mmf.cuh"
+#endif
 #include "ggml-cuda/cumsum.cuh"
 #include "ggml-cuda/fill.cuh"
+
+#if defined(GGML_USE_HIP) && GFX906_KVQ_MOE_CACHE_ENABLED
+#include "ggml-cuda/gfx906/fused/graph-fusion.cuh"
+#endif
+
 #include "ggml.h"
 
 #include <algorithm>
@@ -548,6 +557,10 @@ ggml_backend_cuda_context::~ggml_backend_cuda_context() {
     std::unique_lock<std::mutex> lock(ggml_cuda_lock);
     ggml_cuda_lock_cv.wait(lock, []{ return ggml_cuda_lock_counter.load(std::memory_order_relaxed) == 0; });
 
+#if defined(GGML_USE_HIP) && GFX906_KVQ_MOE_CACHE_ENABLED
+    q8_cache.free_all();
+#endif
+
     if (copy_event != nullptr) {
         CUDA_CHECK(cudaEventDestroy(copy_event));
     }
@@ -1307,7 +1320,21 @@ static void ggml_cuda_op_mul_mat_cublas(
 
         CUBLAS_CHECK(cublasSetStream(ctx.cublas_handle(id), stream));
 
-        if (GGML_CUDA_CC_IS_CDNA(cc) || GGML_CUDA_CC_IS_RDNA4(cc)) {
+        if (GGML_CUDA_CC_IS_CDNA(cc) || GGML_CUDA_CC_IS_RDNA4(cc) || GGML_CUDA_CC_IS_GCN(cc)) {
+#ifdef GGML_USE_HIP
+            // GFX906 custom FP16 GEMM for medium batch sizes (9-2048)
+            bool handled = false;
+            if (GGML_CUDA_CC_IS_GCN(cc)) {
+                handled = gfx906_mmf_dispatch(
+                    src0_ptr, src1_ptr, dst_dd_i,
+                    (int)row_diff, (int)src1_ncols, (int)ne10,
+                    (int)ne00, (int)ne10, (int)ldc,
+                    stream
+                );
+            }
+            if (!handled)
+#endif
+            {
             const float alpha = 1.0f;
             const float beta = 0.0f;
             CUBLAS_CHECK(
@@ -1318,6 +1345,7 @@ static void ggml_cuda_op_mul_mat_cublas(
                         &beta,   dst_dd_i, CUDA_R_32F, ldc,
                         CUBLAS_COMPUTE_32F,
                         CUBLAS_GEMM_DEFAULT_TENSOR_OP));
+            }
         } else {
             ggml_cuda_pool_alloc<half> dst_f16(ctx.pool(id), row_diff*src1_ncols);
 
@@ -1356,16 +1384,31 @@ static void ggml_cuda_op_mul_mat_cublas(
         const float * src0_ddf_i = src0->type == GGML_TYPE_F32 ? (const float *) src0_dd_i : src0_ddq_as_f32.get();
         const float * src1_ddf1_i = src1->type == GGML_TYPE_F32 ? (const float *) src1_ddf_i : src1_ddq_as_f32.get();
 
-        const float alpha = 1.0f;
-        const float beta = 0.0f;
+#ifdef GGML_USE_HIP
+        // GFX906 custom SGEMM for medium batch sizes
+        bool handled = false;
+        if (GGML_CUDA_CC_IS_GCN(cc)) {
+            handled = gfx906_sgemm_dispatch(
+                src0_ddf_i, src1_ddf1_i, dst_dd_i,
+                (int)row_diff, (int)src1_ncols, (int)ne10,
+                (int)ne00, (int)ne10, (int)ldc,
+                stream
+            );
+        }
+        if (!handled)
+#endif
+        {
+            const float alpha = 1.0f;
+            const float beta = 0.0f;
 
-        CUBLAS_CHECK(cublasSetStream(ctx.cublas_handle(id), stream));
-        CUBLAS_CHECK(
-            cublasSgemm(ctx.cublas_handle(id), CUBLAS_OP_T, CUBLAS_OP_N,
-                    row_diff, src1_ncols, ne10,
-                    &alpha, src0_ddf_i,  ne00,
-                            src1_ddf1_i, ne10,
-                    &beta,  dst_dd_i,    ldc));
+            CUBLAS_CHECK(cublasSetStream(ctx.cublas_handle(id), stream));
+            CUBLAS_CHECK(
+                cublasSgemm(ctx.cublas_handle(id), CUBLAS_OP_T, CUBLAS_OP_N,
+                        row_diff, src1_ncols, ne10,
+                        &alpha, src0_ddf_i,  ne00,
+                                src1_ddf1_i, ne10,
+                        &beta,  dst_dd_i,    ldc));
+        }
     }
 
     GGML_UNUSED_VARS(dst, src1_ddq_i, src1_padded_row_size);
@@ -2279,13 +2322,19 @@ static void ggml_cuda_mul_mat_id(ggml_backend_cuda_context & ctx, ggml_tensor *
     const int cc = ggml_cuda_info().devices[ggml_cuda_get_device()].cc;
 
     if (src1->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F32) {
-        if (ne2 == 1) {
+        static_assert(MMVQ_MAX_BATCH_SIZE == MMVF_MAX_BATCH_SIZE);
+        if (ne2 <= MMVQ_MAX_BATCH_SIZE) {
             if (ggml_is_quantized(src0->type)) {
-                ggml_cuda_mul_mat_vec_q(ctx, src0, src1, ids, dst);
+                if (ne2 <= 4) {
+                    ggml_cuda_mul_mat_vec_q(ctx, src0, src1, ids, dst);
+                    return;
+                }
             } else {
-                ggml_cuda_mul_mat_vec_f(ctx, src0, src1, ids, dst);
+                if (GGML_CUDA_CC_IS_AMD(cc)) {
+                    ggml_cuda_mul_mat_vec_f(ctx, src0, src1, ids, dst);
+                    return;
+                }
             }
-            return;
         }
 
         if (ggml_cuda_should_use_mmq(src0->type, cc, ne12, /*n_experts=*/ne02)) {
@@ -2888,6 +2937,17 @@ static bool ggml_cuda_graph_check_compability(ggml_cgraph * cgraph) {
 #endif
         }
 
+#ifdef GGML_USE_HIP
+        // GFX906/GCN: Disable CUDA graphs for SOLVE_TRI (batched TRSM crashes on ROCm)
+        // Qwen3-Next and other hybrid models use SOLVE_TRI for recurrent layers
+        if (node->op == GGML_OP_SOLVE_TRI) {
+            use_cuda_graph = false;
+#ifndef NDEBUG
+            GGML_LOG_DEBUG("%s: disabling CUDA graphs due to SOLVE_TRI on ROCm\n", __func__);
+#endif
+        }
+#endif
+
         if (node->op == GGML_OP_ADD &&
             node->src[1] && node->src[1]->ne[1] > 1 &&
             (node->src[0] ? node->src[0]->name != gemma3n_per_layer_proj_src0_name : true) &&
@@ -2973,8 +3033,7 @@ static bool ggml_cuda_graph_node_properties_match(ggml_tensor * node, ggml_cuda_
         }
     }
 
-    if ((node->op == GGML_OP_SCALE || node->op == GGML_OP_GLU) &&
-        memcmp(props->op_params, node->op_params, GGML_MAX_OP_PARAMS) != 0) {
+    if (memcmp(props->op_params, node->op_params, GGML_MAX_OP_PARAMS) != 0) {
         return false;
     }
 
@@ -3392,6 +3451,10 @@ static void ggml_cuda_graph_evaluate_and_capture(ggml_backend_cuda_context * cud
     // flag used to determine whether it is an integrated_gpu
     const bool integrated            = ggml_cuda_info().devices[cuda_ctx->device].integrated;
 
+#if defined(GGML_USE_HIP) && GFX906_KVQ_MOE_CACHE_ENABLED
+    cuda_ctx->clear_q8_cache();
+#endif
+
     ggml_cuda_stream_context & stream_ctx = cuda_ctx->stream_context();
     bool                         is_concurrent_event_active = false;
     ggml_cuda_concurrent_event * concurrent_event           = nullptr;
@@ -3420,6 +3483,13 @@ static void ggml_cuda_graph_evaluate_and_capture(ggml_backend_cuda_context * cud
         // Only perform the graph execution if CUDA graphs are not enabled, or we are capturing the graph.
         // With the use of CUDA graphs, the execution will be performed by the graph launch.
         if (!use_cuda_graph || cuda_graph_update_required) {
+#if defined(GGML_USE_HIP) && GFX906_KVQ_MOE_CACHE_ENABLED
+            // Sync before clearing fusion buffers (skip during graph capture)
+            if (!cuda_ctx->fusion_q8_buffers.empty() && !use_cuda_graph) {
+                CUDA_CHECK(cudaStreamSynchronize(cuda_ctx->stream()));
+            }
+            clear_fusion_state(cuda_ctx);
+#endif
             [[maybe_unused]] int prev_i = 0;
 
             if (stream_ctx.concurrent_events.size() > 0) {
@@ -3537,6 +3607,12 @@ static void ggml_cuda_graph_evaluate_and_capture(ggml_backend_cuda_context * cud
                 if (!disable_fusion) {
                     ggml_cuda_topk_moe_args args;
 
+#if defined(GGML_USE_HIP) && GFX906_KVQ_MOE_CACHE_ENABLED
+                    if (try_rms_mul_mmq_fusion(cuda_ctx, cgraph, i, use_cuda_graph, cuda_graph_update_required)) {
+                        continue;
+                    }
+#endif
+
                     if (cgraph->nodes[i]->op == GGML_OP_UNARY || cgraph->nodes[i]->op == GGML_OP_SOFT_MAX ||
                         cgraph->nodes[i]->op == GGML_OP_ARGSORT) {
                         const bool can_fuse = ggml_cuda_topk_moe_fusion(cgraph, i, args);
@@ -3854,6 +3930,16 @@ static void ggml_cuda_graph_evaluate_and_capture(ggml_backend_cuda_context * cud
                         continue;
                     }
                 }
+
+#if defined(GGML_USE_HIP) && GFX906_KVQ_MOE_CACHE_ENABLED
+                if (is_mul_handled_by_fusion(cuda_ctx, node)) {
+                    continue;
+                }
+                if (try_prequantized_mul_mat(cuda_ctx, node)) {
+                    continue;
+                }
+#endif
+
 #ifndef NDEBUG
                 assert(node->buffer->buft == ggml_backend_cuda_buffer_type(cuda_ctx->device));
                 for (int j = 0; j < GGML_MAX_SRC; j++) {
@@ -4851,6 +4937,20 @@ static bool ggml_backend_cuda_device_supports_op(ggml_backend_dev_t dev, const g
         case GGML_OP_TRI:
         case GGML_OP_DIAG:
         case GGML_OP_SOLVE_TRI:
+#if defined(GGML_USE_HIP)
+            // GFX906: limit SOLVE_TRI dimensions to avoid rocBLAS batched TRSM crashes
+            // Qwen3-Next and other hybrid models use SOLVE_TRI for recurrent layers
+            {
+                const int cc = ggml_cuda_info().devices[dev_ctx->device].cc;
+                if (GGML_CUDA_CC_IS_GCN(cc)) {
+                    // Check for NULL sources before accessing dimensions
+                    if (op->src[0] && op->src[1]) {
+                        return op->src[0]->ne[0] <= 64 && op->src[1]->ne[0] <= 32;
+                    }
+                    return false; // Fall back to CPU if sources not available
+                }
+            }
+#endif
             return true;
 
         default:
diff --git a/ggml/src/ggml-cuda/mmid.cu b/ggml/src/ggml-cuda/mmid.cu
index 3c61e4595..601e745bb 100644
--- a/ggml/src/ggml-cuda/mmid.cu
+++ b/ggml/src/ggml-cuda/mmid.cu
@@ -138,6 +138,19 @@ static void launch_mm_ids_helper(
 void ggml_cuda_launch_mm_ids_helper(
         const int32_t * __restrict__ ids, int32_t * __restrict__ ids_src1, int32_t * __restrict__ ids_dst, int32_t * __restrict__ expert_bounds,
         const int n_experts, const int n_tokens, const int n_expert_used, const int nchannels_y, const int si1, const int sis1, cudaStream_t stream) {
+
+#if defined(GGML_USE_HIP)
+    // On AMD wavefront64 GPUs (like MI50/gfx906), the optimized paths use sub-warp shuffles
+    // that don't work correctly when n_expert_used >= warp_size/2 (the sub-warp width).
+    // Fall back to generic path only for these cases.
+    const int id = ggml_cuda_get_device();
+    const int warp_size = ggml_cuda_info().devices[id].warp_size;
+    if (n_expert_used >= warp_size / 2) {
+        launch_mm_ids_helper<0>(ids, ids_src1, ids_dst, expert_bounds, n_experts, n_tokens, n_expert_used, nchannels_y, si1, sis1, stream);
+        return;
+    }
+#endif
+
     switch (n_expert_used) {
         case  2:
             launch_mm_ids_helper< 2>(ids, ids_src1, ids_dst, expert_bounds, n_experts, n_tokens, n_expert_used, nchannels_y, si1, sis1, stream);
diff --git a/ggml/src/ggml-cuda/mmq.cu b/ggml/src/ggml-cuda/mmq.cu
index 9a69f41d1..13933d10a 100644
--- a/ggml/src/ggml-cuda/mmq.cu
+++ b/ggml/src/ggml-cuda/mmq.cu
@@ -3,6 +3,10 @@
 #include "quantize.cuh"
 #include "mmid.cuh"
 
+#if defined(GGML_USE_HIP) && GFX906_KVQ_MOE_CACHE_ENABLED
+#include "gfx906/fused/gather-q8.cuh"
+#endif
+
 static void ggml_cuda_mul_mat_q_switch_type(ggml_backend_cuda_context & ctx, const mmq_args & args, cudaStream_t stream) {
     switch (args.type_x) {
         case GGML_TYPE_Q4_0:
@@ -119,22 +123,53 @@ void ggml_cuda_mul_mat_q(
     const bool use_native_mxfp4 = blackwell_mma_available(cc) && src0->type == GGML_TYPE_MXFP4;
 
     if (!ids) {
+        // Regular MUL_MAT path - uses q8_cache for cross-op reuse
         const size_t nbytes_src1_q8_1 = ne13*ne12 * ne11*ne10_padded * sizeof(block_q8_1)/QK8_1 +
             get_mmq_x_max_host(cc)*sizeof(block_q8_1_mmq);
-        ggml_cuda_pool_alloc<char> src1_q8_1(ctx.pool(), nbytes_src1_q8_1);
 
+        const int layout = static_cast<int>(mmq_get_q8_1_ds_layout(src0->type));
+
+        const char* src1_q8_1_ptr = nullptr;
+        ggml_cuda_pool_alloc<char> src1_q8_1_pool;
+
+#if defined(GGML_USE_HIP) && GFX906_KVQ_MOE_CACHE_ENABLED
+        if (!use_native_mxfp4) {
+            const q8_cache_entry* cached = ctx.q8_cache.lookup(
+                src1, layout, ne10_padded, ne11, ne12, ne13);
+
+            if (cached) {
+                src1_q8_1_ptr = static_cast<const char*>(cached->q8_data);
+            } else {
+                void* q8_data = ctx.q8_cache.get_buffer(nbytes_src1_q8_1);
+
+                const int64_t s11 = src1->nb[1] / ts_src1;
+                const int64_t s12 = src1->nb[2] / ts_src1;
+                const int64_t s13 = src1->nb[3] / ts_src1;
+                quantize_mmq_q8_1_cuda(src1_d, nullptr, static_cast<char*>(q8_data), src0->type,
+                                       ne10, s11, s12, s13, ne10_padded, ne11, ne12, ne13, stream);
+                CUDA_CHECK(cudaGetLastError());
+
+                ctx.q8_cache.store(src1, layout, q8_data, nbytes_src1_q8_1,
+                                   ne10_padded, ne11, ne12, ne13);
+                src1_q8_1_ptr = static_cast<const char*>(q8_data);
+            }
+        } else
+#endif
         {
+            // Native mxfp4 or caching disabled - use pool allocation
+            src1_q8_1_pool.alloc(ctx.pool(), nbytes_src1_q8_1);
+            src1_q8_1_ptr = src1_q8_1_pool.get();
+
             const int64_t s11 = src1->nb[1] / ts_src1;
             const int64_t s12 = src1->nb[2] / ts_src1;
             const int64_t s13 = src1->nb[3] / ts_src1;
             if (use_native_mxfp4) {
                 static_assert(sizeof(block_fp4_mmq) == 4 * sizeof(block_q8_1));
-                quantize_mmq_mxfp4_cuda(src1_d, nullptr, src1_q8_1.get(), src0->type, ne10, s11, s12, s13, ne10_padded,
-                                        ne11, ne12, ne13, stream);
-
+                quantize_mmq_mxfp4_cuda(src1_d, nullptr, const_cast<char*>(src1_q8_1_ptr), src0->type,
+                                        ne10, s11, s12, s13, ne10_padded, ne11, ne12, ne13, stream);
             } else {
-                quantize_mmq_q8_1_cuda(src1_d, nullptr, src1_q8_1.get(), src0->type, ne10, s11, s12, s13, ne10_padded,
-                                       ne11, ne12, ne13, stream);
+                quantize_mmq_q8_1_cuda(src1_d, nullptr, const_cast<char*>(src1_q8_1_ptr), src0->type,
+                                       ne10, s11, s12, s13, ne10_padded, ne11, ne12, ne13, stream);
             }
             CUDA_CHECK(cudaGetLastError());
         }
@@ -142,13 +177,13 @@ void ggml_cuda_mul_mat_q(
         // Stride depends on quantization format
         const int64_t s12 = use_native_mxfp4 ?
                                 ne11 * ne10_padded * sizeof(block_fp4_mmq) /
-                                    (8 * QK_MXFP4 * sizeof(int))  // block_fp4_mmq holds 256 values (8 blocks of 32)
+                                    (8 * QK_MXFP4 * sizeof(int))
                                 :
                                 ne11 * ne10_padded * sizeof(block_q8_1) / (QK8_1 * sizeof(int));
         const int64_t s13 = ne12*s12;
 
         const mmq_args args = {
-            src0_d, src0->type, (const int *) src1_q8_1.ptr, nullptr, nullptr, dst_d,
+            src0_d, src0->type, (const int *) src1_q8_1_ptr, nullptr, nullptr, dst_d,
             ne00, ne01, ne1, s01, ne11, s1,
             ne02, ne12, s02, s12, s2,
             ne03, ne13, s03, s13, s3,
@@ -179,6 +214,7 @@ void ggml_cuda_mul_mat_q(
         CUDA_CHECK(cudaGetLastError());
     }
 
+    // Size for gathered Q8_1 output (only the selected rows)
     const size_t nbytes_src1_q8_1 = ne12*n_expert_used*ne10_padded * sizeof(block_q8_1)/QK8_1 +
         get_mmq_x_max_host(cc)*sizeof(block_q8_1_mmq);
     ggml_cuda_pool_alloc<char> src1_q8_1(ctx.pool(), nbytes_src1_q8_1);
@@ -187,7 +223,54 @@ void ggml_cuda_mul_mat_q(
     const int64_t ne12_flat = 1;
     const int64_t ne13_flat = 1;
 
+#if defined(GGML_USE_HIP) && GFX906_KVQ_MOE_CACHE_ENABLED
+    // MoE caching: quantize full tensor once, gather rows for each expert call
+    if (!use_native_mxfp4) {
+        const int layout = static_cast<int>(mmq_get_q8_1_ds_layout(src0->type));
+
+        const q8_cache_entry* moe_cached = ctx.q8_cache.lookup(
+            src1, layout, ne10_padded, ne11, ne12, ne13);
+
+        if (!moe_cached) {
+            const size_t full_nbytes = ne13*ne12*ne11*ne10_padded * sizeof(block_q8_1)/QK8_1 +
+                get_mmq_x_max_host(cc)*sizeof(block_q8_1_mmq);
+
+            void* full_q8 = ctx.q8_cache.get_buffer(full_nbytes);
+
+            const int64_t s11 = src1->nb[1] / ts_src1;
+            const int64_t s12 = src1->nb[2] / ts_src1;
+            const int64_t s13 = src1->nb[3] / ts_src1;
+
+            quantize_mmq_q8_1_cuda(src1_d, nullptr, static_cast<char*>(full_q8), src0->type,
+                                   ne10, s11, s12, s13, ne10_padded,
+                                   ne11, ne12, ne13, stream);
+            CUDA_CHECK(cudaGetLastError());
+
+            ctx.q8_cache.store(src1, layout, full_q8, full_nbytes,
+                               ne10_padded, ne11, ne12, ne13);
+
+            moe_cached = ctx.q8_cache.lookup(src1, layout, ne10_padded, ne11, ne12, ne13);
+        }
+
+        // Gather selected rows from cached full Q8_1 tensor
+        const int64_t block_size = sizeof(block_q8_1_mmq);
+        const int64_t n_blocks = ne10_padded / (4*QK8_1);
+
+        gather_q8_1_rows_cuda(
+            moe_cached->q8_data,
+            ids_src1.get(),
+            src1_q8_1.get(),
+            block_size,
+            n_blocks,
+            ne11,
+            ne11_flat,
+            stream
+        );
+        CUDA_CHECK(cudaGetLastError());
+    } else
+#endif
     {
+        // No caching - original behavior: quantize only selected rows
         const int64_t s11 = src1->nb[1] / ts_src1;
         const int64_t s12 = src1->nb[2] / ts_src1;
         const int64_t s13 = src1->nb[3] / ts_src1;
diff --git a/ggml/src/ggml-cuda/mmq.cuh b/ggml/src/ggml-cuda/mmq.cuh
index f80f98cda..b045f9d32 100644
--- a/ggml/src/ggml-cuda/mmq.cuh
+++ b/ggml/src/ggml-cuda/mmq.cuh
@@ -9,10 +9,24 @@
 
 using namespace ggml_cuda_mma;
 
+// GFX906 MMQ optimizations (vectorized loads and prefetch)
+#ifdef GGML_USE_HIP
+    #include "gfx906/matmul/mmq.cuh"
+    #include "gfx906/gfx906-config.h"
+    #include "gfx906/matmul/mmq-prefetch.cuh"
+#endif
+
 #define MMQ_DP4A_MAX_BATCH_SIZE 64 // Max. batch size to use for dp4a MMQ kernels when FP16 tensor cores are available.
-#define MMQ_ITER_K 256
-#define MMQ_ITER_K_MXFP4_FP4    512
-#define MMQ_NWARPS 8
+
+// GFX906-optimized MMQ configuration
+#ifdef GGML_USE_HIP
+    #define MMQ_ITER_K GFX906_MMQ_ITER_K
+    #define MMQ_NWARPS GFX906_MMQ_NWARPS
+#else
+    #define MMQ_ITER_K 256
+    #define MMQ_NWARPS 8
+#endif
+#define MMQ_ITER_K_MXFP4_FP4 512
 
 typedef void (*load_tiles_mmq_t)(const char * __restrict__ x, int * x_tile, const int kbx0, const int i_max, const int stride);
 typedef void (*vec_dot_mmq_t)(const int * __restrict__ x, const int * __restrict__ y, float * __restrict__ sum, const int k00);
@@ -251,6 +265,15 @@ static constexpr __host__ __device__ int mmq_get_mma_tile_x_k(ggml_type type) {
 #define MMQ_TILE_Y_K     (MMQ_TILE_NE_K + MMQ_TILE_NE_K / QI8_1)
 #define MMQ_TILE_Y_FP4_K MMQ_TILE_Y_K
 
+// LDS stride for Y-tile - PADDING ANALYSIS RESULTS:
+// Original stride 40: 40 mod 32 = 8 → 4-way bank conflicts (9.3% LDS stalls)
+// Tested: Padded stride 41 with dst_idx = l + l/40 mapping
+// Result: -3.5% slower (1180 vs 1223 t/s) even with proper shared mem allocation
+// Root cause: Division overhead in store loop outweighs bank conflict reduction
+// The bank conflicts occur during vec_dot reads, but the overhead is in stores
+// Conclusion: Keep original stride - bank conflicts are cheaper than index math
+#define MMQ_TILE_Y_K_LDS MMQ_TILE_Y_K
+
 static int mmq_get_granularity_host(const int mmq_x, const int cc) {
     if (amd_mfma_available(cc) || amd_wmma_available(cc)) {
         return mmq_x >= 128 ? 32 : 16;
@@ -384,15 +407,20 @@ static __device__ __forceinline__ void vec_dot_q4_0_q8_1_dp4a(
 
                 int u[2*VDR_Q4_0_Q8_1_MMQ];
 
+#if defined(GGML_USE_HIP)
+                // Call GFX906-optimized vectorized load from gfx906/gfx906-mmq.cuh
+                gfx906_load_q4_0_quants_vectorized(y_qs, j*MMQ_TILE_Y_K_LDS + kyqs, QI4_0, u);
+#else
 #pragma unroll
                 for (int l = 0; l < VDR_Q4_0_Q8_1_MMQ; ++l) {
-                    u[2*l+0] = y_qs[j*MMQ_TILE_Y_K + kyqs +  l];
-                    u[2*l+1] = y_qs[j*MMQ_TILE_Y_K + kyqs + (l + QI4_0)];
+                    u[2*l+0] = y_qs[j*MMQ_TILE_Y_K_LDS + kyqs +  l];
+                    u[2*l+1] = y_qs[j*MMQ_TILE_Y_K_LDS + kyqs + (l + QI4_0)];
                 }
+#endif
 
                 sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q4_0_q8_1_impl<VDR_Q4_0_Q8_1_MMQ>
                     (&x_qs[i*(MMQ_TILE_NE_K + 1) + k0/QR4_0], u,
-                     x_df[i*(MMQ_TILE_NE_K/QI4_0) + i/QI4_0 + k0/(QR4_0*QI4_0)], y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);
+                     x_df[i*(MMQ_TILE_NE_K/QI4_0) + i/QI4_0 + k0/(QR4_0*QI4_0)], y_ds[j*MMQ_TILE_Y_K_LDS + k01/QI8_1]);
             }
         }
     }
@@ -487,15 +515,20 @@ static __device__ __forceinline__ void vec_dot_q4_1_q8_1_dp4a(
 
                 int u[2*VDR_Q4_1_Q8_1_MMQ];
 
+#if defined(GGML_USE_HIP)
+                // Call GFX906-optimized vectorized load from gfx906/gfx906-mmq.cuh
+                gfx906_load_q4_1_quants_vectorized(y_qs, j*MMQ_TILE_Y_K_LDS + kyqs, QI4_1, u);
+#else
 #pragma unroll
                 for (int l = 0; l < VDR_Q4_1_Q8_1_MMQ; ++l) {
-                    u[2*l+0] = y_qs[j*MMQ_TILE_Y_K + kyqs +  l];
-                    u[2*l+1] = y_qs[j*MMQ_TILE_Y_K + kyqs + (l + QI4_1)];
+                    u[2*l+0] = y_qs[j*MMQ_TILE_Y_K_LDS + kyqs +  l];
+                    u[2*l+1] = y_qs[j*MMQ_TILE_Y_K_LDS + kyqs + (l + QI4_1)];
                 }
+#endif
 
                 sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q4_1_q8_1_impl<VDR_Q4_1_Q8_1_MMQ>
                     (&x_qs[i*(MMQ_TILE_NE_K + 1) + k0/QR4_1], u,
-                     x_dm[i*(MMQ_TILE_NE_K/QI4_1) + i/QI4_1 + k0/(QR4_1*QI4_1)], y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);
+                     x_dm[i*(MMQ_TILE_NE_K/QI4_1) + i/QI4_1 + k0/(QR4_1*QI4_1)], y_ds[j*MMQ_TILE_Y_K_LDS + k01/QI8_1]);
             }
         }
     }
@@ -676,24 +709,46 @@ template <int mmq_y, bool need_check> static __device__ __forceinline__ void loa
     const int kbx  = txi / QI8_0;
     const int kqsx = txi % QI8_0;
 
+#if defined(GGML_USE_HIP) && defined(__gfx906__)
+    // GFX906: Software pipelining using macros from gfx906-mmq.cuh
+    constexpr int loop_iters = mmq_y / (nrows * nwarps);
+    constexpr int cache_size = loop_iters > 16 ? 16 : loop_iters;
+    int qs0_cache[cache_size];
+    int qs1_cache[cache_size];
+    int i_slot_cache[cache_size];
+
+    // Load all data into registers (async)
+    GFX906_LOAD_TILES_Q8_0_ASYNC(cache_size, nrows, nwarps, threads_per_row, need_check,
+        x, kbx0, stride, i_max, txi, kbx, kqsx, qs0_cache, qs1_cache, i_slot_cache);
+
+    // Store all to LDS
+#if defined(AMD_MFMA_AVAILABLE) || defined(TURING_MMA_AVAILABLE) || defined(AMD_WMMA_AVAILABLE)
+    GFX906_STORE_TILES_Q8_0_LDS_MMA(cache_size, x_qs, qs0_cache, qs1_cache, i_slot_cache, txi);
+#else
+    GFX906_STORE_TILES_Q8_0_LDS_LEGACY(cache_size, x_qs, qs0_cache, qs1_cache, i_slot_cache, txi);
+#endif
+#else
 #pragma unroll
     for (int i0 = 0; i0 < mmq_y; i0 += nrows*nwarps) {
-        int i = i0 + (nrows == 1 ? threadIdx.y : threadIdx.y*nrows + threadIdx.x/threads_per_row);
-
-        if (need_check) {
-            i = min(i, i_max);
-        }
+        // GFX906 optimization: Avoid LDS write conflicts in need_check path.
+        // Original code clamped i to i_max, causing all out-of-bounds threads to
+        // write to the SAME location (tile[i_max*...]) - serializing LDS writes.
+        // Fix: Each thread writes to its ORIGINAL slot; out-of-bounds write zeros.
+        const int i_slot = i0 + (nrows == 1 ? threadIdx.y : threadIdx.y*nrows + threadIdx.x/threads_per_row);
+        const int i_read = need_check ? min(i_slot, i_max) : i_slot;
+        const bool oob = need_check && (i_slot > i_max);
 
-        const block_q8_0 * bxi = (const block_q8_0 *) x + kbx0 + i*stride + kbx;
+        const block_q8_0 * bxi = (const block_q8_0 *) x + kbx0 + i_read*stride + kbx;
 
 #if defined(AMD_MFMA_AVAILABLE) || defined(TURING_MMA_AVAILABLE) || defined(AMD_WMMA_AVAILABLE)
-        x_qs[i*MMQ_MMA_TILE_X_K_Q8_0 + 0             + txi] = get_int_b2(bxi[0].qs,                   kqsx);
-        x_qs[i*MMQ_MMA_TILE_X_K_Q8_0 + MMQ_TILE_NE_K + txi] = get_int_b2(bxi[MMQ_TILE_NE_K/QI8_0].qs, kqsx);
+        x_qs[i_slot*MMQ_MMA_TILE_X_K_Q8_0 + 0             + txi] = oob ? 0 : get_int_b2(bxi[0].qs,                   kqsx);
+        x_qs[i_slot*MMQ_MMA_TILE_X_K_Q8_0 + MMQ_TILE_NE_K + txi] = oob ? 0 : get_int_b2(bxi[MMQ_TILE_NE_K/QI8_0].qs, kqsx);
 #else
-        x_qs[i*(2*MMQ_TILE_NE_K + 1) + 0             + txi] = get_int_b2(bxi[0].qs,                   kqsx);
-        x_qs[i*(2*MMQ_TILE_NE_K + 1) + MMQ_TILE_NE_K + txi] = get_int_b2(bxi[MMQ_TILE_NE_K/QI8_0].qs, kqsx);
+        x_qs[i_slot*(2*MMQ_TILE_NE_K + 1) + 0             + txi] = oob ? 0 : get_int_b2(bxi[0].qs,                   kqsx);
+        x_qs[i_slot*(2*MMQ_TILE_NE_K + 1) + MMQ_TILE_NE_K + txi] = oob ? 0 : get_int_b2(bxi[MMQ_TILE_NE_K/QI8_0].qs, kqsx);
 #endif // defined(AMD_MFMA_AVAILABLE) || defined(TURING_MMA_AVAILABLE) || defined(AMD_WMMA_AVAILABLE)
     }
+#endif
 
     constexpr int blocks_per_tile_x_row = 2*MMQ_TILE_NE_K / QI8_0;
     constexpr int rows_per_warp = warp_size / blocks_per_tile_x_row;
@@ -701,18 +756,17 @@ template <int mmq_y, bool need_check> static __device__ __forceinline__ void loa
 
 #pragma unroll
     for (int i0 = 0; i0 < mmq_y; i0 += nwarps * rows_per_warp) {
-        int i = i0 + threadIdx.y * rows_per_warp + threadIdx.x / blocks_per_tile_x_row;
-
-        if (need_check) {
-            i = min(i, i_max);
-        }
+        // Same optimization for scale loading
+        const int i_slot = i0 + threadIdx.y * rows_per_warp + threadIdx.x / blocks_per_tile_x_row;
+        const int i_read = need_check ? min(i_slot, i_max) : i_slot;
+        const bool oob = need_check && (i_slot > i_max);
 
-        const block_q8_0 * bxi = (const block_q8_0 *) x + kbx0 + i*stride + kbxd;
+        const block_q8_0 * bxi = (const block_q8_0 *) x + kbx0 + i_read*stride + kbxd;
 
 #if defined(AMD_MFMA_AVAILABLE) || defined(TURING_MMA_AVAILABLE) || defined(AMD_WMMA_AVAILABLE)
-        x_df[i*MMQ_MMA_TILE_X_K_Q8_0                 + kbxd] = bxi->d;
+        x_df[i_slot*MMQ_MMA_TILE_X_K_Q8_0                 + kbxd] = oob ? 0.0f : (float)bxi->d;
 #else
-        x_df[i*(2*MMQ_TILE_NE_K/QI8_0) + i/(QI8_0/2) + kbxd] = bxi->d;
+        x_df[i_slot*(2*MMQ_TILE_NE_K/QI8_0) + i_slot/(QI8_0/2) + kbxd] = oob ? 0.0f : (float)bxi->d;
 #endif // defined(AMD_MFMA_AVAILABLE) || defined(TURING_MMA_AVAILABLE) || defined(AMD_WMMA_AVAILABLE)
     }
 }
@@ -737,6 +791,41 @@ template <int mmq_y, bool need_check> static __device__ __forceinline__ void loa
     const int kbx  = txi / QI_MXFP4;
     const int kqsx = txi % QI_MXFP4;
 
+#if defined(GGML_USE_HIP) && defined(__gfx906__)
+    // GFX906: Software pipelining - load all data first, then dequant all
+    // Maximizes memory-level parallelism before compute
+    constexpr int loop_iters = mmq_y / (nrows * nwarps);
+    int aux_q4_cache[loop_iters > 16 ? 16 : loop_iters];
+    int i_cache[loop_iters > 16 ? 16 : loop_iters];
+
+    // Phase 1: Issue all loads
+    #pragma unroll
+    for (int iter = 0; iter < (loop_iters > 16 ? 16 : loop_iters); iter++) {
+        const int i0 = iter * nrows * nwarps;
+        int i = i0 + (nrows == 1 ? threadIdx.y : threadIdx.y*nrows + threadIdx.x/threads_per_row);
+        if (need_check) {
+            i = min(i, i_max);
+        }
+        const block_mxfp4 * bxi = (const block_mxfp4 *) x + kbx0 + i*stride + kbx;
+        aux_q4_cache[iter] = get_int_b1(bxi->qs, kqsx);
+        i_cache[iter] = i;
+    }
+
+    // Phase 2: Dequant and store
+    const int k0 = kbx * (2 * QI_MXFP4) + kqsx;
+    #pragma unroll
+    for (int iter = 0; iter < (loop_iters > 16 ? 16 : loop_iters); iter++) {
+        const int2 v = get_int_from_mxfp4_table(aux_q4_cache[iter]);
+        const int i = i_cache[iter];
+#if defined(AMD_MFMA_AVAILABLE) || defined(TURING_MMA_AVAILABLE) || defined(AMD_WMMA_AVAILABLE)
+        x_qs[i*MMQ_MMA_TILE_X_K_Q8_1 + k0 + 0]        = v.x;
+        x_qs[i*MMQ_MMA_TILE_X_K_Q8_1 + k0 + QI_MXFP4] = v.y;
+#else
+        x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0 + 0]        = v.x;
+        x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0 + QI_MXFP4] = v.y;
+#endif
+    }
+#else
 #pragma unroll
     for (int i0 = 0; i0 < mmq_y; i0 += nrows*nwarps) {
         int i = i0 + (nrows == 1 ? threadIdx.y : threadIdx.y*nrows + threadIdx.x/threads_per_row);
@@ -748,7 +837,7 @@ template <int mmq_y, bool need_check> static __device__ __forceinline__ void loa
         const block_mxfp4 * bxi = (const block_mxfp4 *) x + kbx0 + i*stride + kbx;
 
         const int aux_q4 = get_int_b1(bxi->qs, kqsx);
-        const int2 v = get_int_from_table_16(aux_q4, kvalues_mxfp4);
+        const int2 v = get_int_from_mxfp4_table(aux_q4);
         const int k0 = kbx * (2 * QI_MXFP4) + kqsx;
 
 #if defined(AMD_MFMA_AVAILABLE) || defined(TURING_MMA_AVAILABLE) || defined(AMD_WMMA_AVAILABLE)
@@ -759,6 +848,7 @@ template <int mmq_y, bool need_check> static __device__ __forceinline__ void loa
         x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0 + QI_MXFP4] = v.y;
 #endif // defined(AMD_MFMA_AVAILABLE) || defined(TURING_MMA_AVAILABLE)  || defined(AMD_WMMA_AVAILABLE)
     }
+#endif
 
     constexpr int blocks_per_tile_x_row = MMQ_TILE_NE_K / QI_MXFP4;
     constexpr int rows_per_warp = warp_size / blocks_per_tile_x_row;
@@ -851,8 +941,8 @@ static __device__ __forceinline__ void vec_dot_q8_0_q8_1_dp4a(
                 const int i = i0 + threadIdx.x;
 
                 sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q8_0_q8_1_impl<float, VDR_Q8_0_Q8_1_MMQ>
-                    (&x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k0 % MMQ_TILE_NE_K],
-                     x_df[i*(2*MMQ_TILE_NE_K/QI8_0) + i/(QI8_0/2) + k0/QI8_0], y_df[j*MMQ_TILE_Y_K + (k0/QI8_1) % (MMQ_TILE_NE_K/QI8_1)]);
+                    (&x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K_LDS + k0 % MMQ_TILE_NE_K],
+                     x_df[i*(2*MMQ_TILE_NE_K/QI8_0) + i/(QI8_0/2) + k0/QI8_0], y_df[j*MMQ_TILE_Y_K_LDS + (k0/QI8_1) % (MMQ_TILE_NE_K/QI8_1)]);
             }
         }
     }
@@ -871,7 +961,7 @@ static __device__ __forceinline__ void vec_dot_q8_0_q8_1_mma(
     constexpr int rows_per_warp = granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const float * x_df = (const float *) x_qs + 2*MMQ_TILE_NE_K;
@@ -893,14 +983,14 @@ static __device__ __forceinline__ void vec_dot_q8_0_q8_1_mma(
 #pragma unroll
         for (int j0 = 0; j0 < mmq_x; j0 += ntx*tile_C::J) {
             tile_B B;
-            load_generic(B, y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K);
+            load_generic(B, y_qs + j0*MMQ_TILE_Y_K_LDS + k01, MMQ_TILE_Y_K_LDS);
 
             float dB;
             const int j = j0 + tile_C::get_j(0);
             if (ds_layout == MMQ_Q8_1_DS_LAYOUT_D4) {
-                dB = y_df[j*MMQ_TILE_Y_K + k01/QI8_1];
+                dB = y_df[j*MMQ_TILE_Y_K_LDS + k01/QI8_1];
             } else {
-                dB = __low2float(y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);
+                dB = __low2float(y_ds[j*MMQ_TILE_Y_K_LDS + k01/QI8_1]);
             }
 
 #pragma unroll
@@ -926,7 +1016,7 @@ static __device__ __forceinline__ void vec_dot_q8_0_q8_1_mma(
     constexpr int rows_per_warp = 2 * granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const float * x_df = (const float *) x_qs + 2*MMQ_TILE_NE_K;
@@ -968,16 +1058,16 @@ static __device__ __forceinline__ void vec_dot_q8_0_q8_1_mma(
             tile_B B;
             float dB[tile_C::ne/2];
 
-            load_generic(B, y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K); // faster than load_ldmatrix
+            load_generic(B, y_qs + j0*MMQ_TILE_Y_K_LDS + k01, MMQ_TILE_Y_K_LDS); // faster than load_ldmatrix
 
 #pragma unroll
             for (int l = 0; l < tile_C::ne/2; ++l) {
                 const int j = j0 + tile_C::get_j(l);
 
                 if (ds_layout == MMQ_Q8_1_DS_LAYOUT_D4) {
-                    dB[l] =             y_df[j*MMQ_TILE_Y_K + k01/QI8_1];
+                    dB[l] =             y_df[j*MMQ_TILE_Y_K_LDS + k01/QI8_1];
                 } else {
-                    dB[l] = __low2float(y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);
+                    dB[l] = __low2float(y_ds[j*MMQ_TILE_Y_K_LDS + k01/QI8_1]);
                 }
             }
 
@@ -1093,8 +1183,8 @@ static __device__ __forceinline__ void vec_dot_q8_1_q8_1_dp4a(
                 const int i = i0 + threadIdx.x;
 
                 sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q8_1_q8_1_impl<QR5_1*VDR_Q5_1_Q8_1_MMQ>
-                    (&x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k01],
-                    x_dm[i*(MMQ_TILE_NE_K/QI5_1) + i/QI5_1 + k0/QI8_1], y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);
+                    (&x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K_LDS + k01],
+                    x_dm[i*(MMQ_TILE_NE_K/QI5_1) + i/QI5_1 + k0/QI8_1], y_ds[j*MMQ_TILE_Y_K_LDS + k01/QI8_1]);
             }
         }
     }
@@ -1113,7 +1203,7 @@ static __device__ __forceinline__ void vec_dot_q8_1_q8_1_mma(
     constexpr int rows_per_warp = granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const half2 * x_dm = (const half2 *) x_qs + 2*MMQ_TILE_NE_K;
@@ -1134,10 +1224,10 @@ static __device__ __forceinline__ void vec_dot_q8_1_q8_1_mma(
 #pragma unroll
         for (int j0 = 0; j0 < mmq_x; j0 += ntx*tile_C::J) {
             tile_B B;
-            load_generic(B, y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K);
+            load_generic(B, y_qs + j0*MMQ_TILE_Y_K_LDS + k01, MMQ_TILE_Y_K_LDS);
 
             const int j = j0 + tile_C::get_j(0);
-            const float2 dsB = __half22float2(y_dm[j*MMQ_TILE_Y_K + k01/QI8_1]);
+            const float2 dsB = __half22float2(y_dm[j*MMQ_TILE_Y_K_LDS + k01/QI8_1]);
 
 #pragma unroll
             for (int n = 0; n < ntx; ++n) {
@@ -1163,7 +1253,7 @@ static __device__ __forceinline__ void vec_dot_q8_1_q8_1_mma(
     constexpr int rows_per_warp = 2 * granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const half2 * x_dm = (const half2 *) x_qs + 2*MMQ_TILE_NE_K;
@@ -1204,13 +1294,13 @@ static __device__ __forceinline__ void vec_dot_q8_1_q8_1_mma(
             tile_B   B;
             float2 dsB[tile_C::ne/2];
 
-            load_generic(B, y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K); // faster than load_ldmatrix
+            load_generic(B, y_qs + j0*MMQ_TILE_Y_K_LDS + k01, MMQ_TILE_Y_K_LDS); // faster than load_ldmatrix
 
 #pragma unroll
             for (int l = 0; l < tile_C::ne/2; ++l) {
                 const int j = j0 + tile_C::get_j(l);
 
-                dsB[l] = __half22float2(y_dm[j*MMQ_TILE_Y_K + k01/QI8_1]);
+                dsB[l] = __half22float2(y_dm[j*MMQ_TILE_Y_K_LDS + k01/QI8_1]);
             }
 
 #pragma unroll
@@ -1256,9 +1346,9 @@ static __device__ __forceinline__ void vec_dot_q8_0_16_q8_1_dp4a(
 
                 sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q8_0_16_q8_1_impl<QI8_0>(
                     &x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0],
-                    &y_qs[j*MMQ_TILE_Y_K + k01],
+                    &y_qs[j*MMQ_TILE_Y_K_LDS + k01],
                     &x_df[i*(2*MMQ_TILE_NE_K*2/QI8_0) + i/(QI8_0/4) + k0/(QI8_0/2)],
-                    y_df[j*MMQ_TILE_Y_K + k01/QI8_1]);
+                    y_df[j*MMQ_TILE_Y_K_LDS + k01/QI8_1]);
             }
         }
     }
@@ -1279,7 +1369,7 @@ static __device__ __forceinline__ void vec_dot_q8_0_16_q8_1_mma(
     constexpr int rows_per_warp = granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const float * x_df = (const float *) x_qs + MMQ_TILE_NE_K*2;
@@ -1300,10 +1390,10 @@ static __device__ __forceinline__ void vec_dot_q8_0_16_q8_1_mma(
 #pragma unroll
         for (int j0 = 0; j0 < mmq_x; j0 += ntx*tile_C::J) {
             tile_B B[1];
-            load_generic(((tile_load *) B)[0], y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K);
+            load_generic(((tile_load *) B)[0], y_qs + j0*MMQ_TILE_Y_K_LDS + k01, MMQ_TILE_Y_K_LDS);
 
             const int j = j0 + tile_C::get_j(0);
-            const float dB = y_df[j*MMQ_TILE_Y_K + k01/QI8_1] / 2;
+            const float dB = y_df[j*MMQ_TILE_Y_K_LDS + k01/QI8_1] / 2;
 
 #pragma unroll
             for (int n = 0; n < ntx; ++n) {
@@ -1328,7 +1418,7 @@ static __device__ __forceinline__ void vec_dot_q8_0_16_q8_1_mma(
     constexpr int rows_per_warp = granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const float * x_df = (const float *) x_qs + MMQ_TILE_NE_K*2;
@@ -1349,10 +1439,10 @@ static __device__ __forceinline__ void vec_dot_q8_0_16_q8_1_mma(
 #pragma unroll
         for (int j0 = 0; j0 < mmq_x; j0 += ntx*tile_C::J) {
             tile_B B;
-            load_generic(B, y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K);
+            load_generic(B, y_qs + j0*MMQ_TILE_Y_K_LDS + k01, MMQ_TILE_Y_K_LDS);
 
             const int j = j0 + tile_C::get_j(0);
-            const float dB = y_df[j*MMQ_TILE_Y_K + k01/QI8_1];
+            const float dB = y_df[j*MMQ_TILE_Y_K_LDS + k01/QI8_1];
 
 #pragma unroll
             for (int n = 0; n < ntx; ++n) {
@@ -1378,7 +1468,7 @@ static __device__ __forceinline__ void vec_dot_q8_0_16_q8_1_mma(
     constexpr int rows_per_warp = 2 * granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const float * x_df = (const float *) x_qs + MMQ_TILE_NE_K*2;
@@ -1420,14 +1510,14 @@ static __device__ __forceinline__ void vec_dot_q8_0_16_q8_1_mma(
             float dB[tile_C::ne/2];
 
             // Here load_generic is faster than load_ldmatrix.
-            load_generic(B[0], y_qs + j0*MMQ_TILE_Y_K + (k01 + 0),         MMQ_TILE_Y_K);
-            load_generic(B[1], y_qs + j0*MMQ_TILE_Y_K + (k01 + tile_B::J), MMQ_TILE_Y_K);
+            load_generic(B[0], y_qs + j0*MMQ_TILE_Y_K_LDS + (k01 + 0),         MMQ_TILE_Y_K_LDS);
+            load_generic(B[1], y_qs + j0*MMQ_TILE_Y_K_LDS + (k01 + tile_B::J), MMQ_TILE_Y_K_LDS);
 
 #pragma unroll
             for (int l = 0; l < tile_C::ne/2; ++l) {
                 const int j = j0 + tile_C::get_j(l);
 
-                dB[l] = y_df[j*MMQ_TILE_Y_K + k01/QI8_1];
+                dB[l] = y_df[j*MMQ_TILE_Y_K_LDS + k01/QI8_1];
             }
 
 #pragma unroll
@@ -1524,7 +1614,7 @@ static __device__ __forceinline__ void vec_dot_q2_K_q8_1_dp4a(
     for (int j0 = 0; j0 < mmq_x; j0 += nwarps) {
         const int j = j0 + threadIdx.y;
 
-        y_df[j0/nwarps] = __half22float2(y_ds[j*MMQ_TILE_Y_K]);
+        y_df[j0/nwarps] = __half22float2(y_ds[j*MMQ_TILE_Y_K_LDS]);
     }
 
 #pragma unroll
@@ -1541,9 +1631,9 @@ static __device__ __forceinline__ void vec_dot_q2_K_q8_1_dp4a(
 
                 constexpr int ns = 2;
                 sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q2_K_q8_1_impl_mmq<ns>(
-                    &x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k01],
+                    &x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K_LDS + k01],
                     &x_dm[i*(MMQ_TILE_NE_K + 1) + k0/4], k01 < MMQ_TILE_NE_K/2 ? y_df[j0/nwarps].x : y_df[j0/nwarps].y,
-                    &y_ds[j*MMQ_TILE_Y_K + (1 + k01/QI8_1)]);
+                    &y_ds[j*MMQ_TILE_Y_K_LDS + (1 + k01/QI8_1)]);
             }
         }
     }
@@ -1564,9 +1654,9 @@ static __device__ __forceinline__ void vec_dot_q2_K_q8_1_dp4a(
 
                 constexpr int ns = 1;
                 sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q2_K_q8_1_impl_mmq<ns>(
-                    &x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k01],
+                    &x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K_LDS + k01],
                     &x_dm[i*(MMQ_TILE_NE_K + 1) + k0/4], k01 < MMQ_TILE_NE_K/2 ? y_df[j0/nwarps].x : y_df[j0/nwarps].y,
-                    &y_ds[j*MMQ_TILE_Y_K + (1 + k01/QI8_1)]);
+                    &y_ds[j*MMQ_TILE_Y_K_LDS + (1 + k01/QI8_1)]);
             }
         }
     }
@@ -1586,7 +1676,7 @@ static __device__ __forceinline__ void vec_dot_q2_K_q8_1_mma(
     constexpr int rows_per_warp = granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const half2 * x_dm = (const half2 *) x_qs + MMQ_TILE_NE_K*2;
@@ -1607,13 +1697,13 @@ static __device__ __forceinline__ void vec_dot_q2_K_q8_1_mma(
 #pragma unroll
         for (int j0 = 0; j0 < mmq_x; j0 += ntx*tile_C::J) {
             tile_B B[1];
-            load_generic(((tile_load *) B)[0], y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K);
+            load_generic(((tile_load *) B)[0], y_qs + j0*MMQ_TILE_Y_K_LDS + k01, MMQ_TILE_Y_K_LDS);
 
             const int j = j0 + tile_C::get_j(0);
-            const float dB = (k01 < MMQ_TILE_NE_K/2) ? __half22float2(y_ds[j*MMQ_TILE_Y_K]).x/2 : __half22float2(y_ds[j*MMQ_TILE_Y_K]).y/2;
+            const float dB = (k01 < MMQ_TILE_NE_K/2) ? __half22float2(y_ds[j*MMQ_TILE_Y_K_LDS]).x/2 : __half22float2(y_ds[j*MMQ_TILE_Y_K_LDS]).y/2;
             const float sB = (k01 >= MMQ_TILE_NE_K * 3/4) ? 0
-                                              : (((k01/4)%2) ? __half22float2(y_ds[j*MMQ_TILE_Y_K + (1 + k01/QI8_1)]).y
-                                                             : __half22float2(y_ds[j*MMQ_TILE_Y_K + (1 + k01/QI8_1)]).x);
+                                              : (((k01/4)%2) ? __half22float2(y_ds[j*MMQ_TILE_Y_K_LDS + (1 + k01/QI8_1)]).y
+                                                             : __half22float2(y_ds[j*MMQ_TILE_Y_K_LDS + (1 + k01/QI8_1)]).x);
 
             tile_C Cm;
             if (k01 >= MMQ_TILE_NE_K * 3/4) {
@@ -1652,7 +1742,7 @@ static __device__ __forceinline__ void vec_dot_q2_K_q8_1_mma(
     constexpr int rows_per_warp = granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const half2 * x_dm = (const half2 *) x_qs + MMQ_TILE_NE_K*2;
@@ -1673,13 +1763,13 @@ static __device__ __forceinline__ void vec_dot_q2_K_q8_1_mma(
 #pragma unroll
         for (int j0 = 0; j0 < mmq_x; j0 += ntx*tile_C::J) {
             tile_B B;
-            load_generic(B, y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K);
+            load_generic(B, y_qs + j0*MMQ_TILE_Y_K_LDS + k01, MMQ_TILE_Y_K_LDS);
 
             const int j = j0 + tile_C::get_j(0);
-            const float dB = (k01 < MMQ_TILE_NE_K/2) ? __half22float2(y_ds[j*MMQ_TILE_Y_K]).x : __half22float2(y_ds[j*MMQ_TILE_Y_K]).y;
+            const float dB = (k01 < MMQ_TILE_NE_K/2) ? __half22float2(y_ds[j*MMQ_TILE_Y_K_LDS]).x : __half22float2(y_ds[j*MMQ_TILE_Y_K_LDS]).y;
             const float sB = (k01 >= MMQ_TILE_NE_K * 3/4) ? 0
-                                              : (((k01/4)%2) ? __half22float2(y_ds[j*MMQ_TILE_Y_K + (1 + k01/QI8_1)]).y
-                                                             : __half22float2(y_ds[j*MMQ_TILE_Y_K + (1 + k01/QI8_1)]).x);
+                                              : (((k01/4)%2) ? __half22float2(y_ds[j*MMQ_TILE_Y_K_LDS + (1 + k01/QI8_1)]).y
+                                                             : __half22float2(y_ds[j*MMQ_TILE_Y_K_LDS + (1 + k01/QI8_1)]).x);
 
             tile_C Cm;
             if (k01 >= MMQ_TILE_NE_K * 3/4) {
@@ -1721,7 +1811,7 @@ static __device__ __forceinline__ void vec_dot_q2_K_q8_1_mma(
     constexpr int rows_per_warp = 2 * granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const half2 * x_dm = (const half2 *) x_qs + MMQ_TILE_NE_K*2;
@@ -1770,7 +1860,7 @@ static __device__ __forceinline__ void vec_dot_q2_K_q8_1_mma(
         for (int l = 0; l < tile_C::ne/2; ++l) {
             const int j = j0 + tile_C::get_j(l);
 
-            dB[l] = __half22float2(y_ds[j*MMQ_TILE_Y_K]);
+            dB[l] = __half22float2(y_ds[j*MMQ_TILE_Y_K_LDS]);
         }
 
 #pragma unroll
@@ -1778,8 +1868,8 @@ static __device__ __forceinline__ void vec_dot_q2_K_q8_1_mma(
             tile_B B[2];
 
             // Here load_generic is faster than load_ldmatrix.
-            load_generic(B[0], y_qs + j0*MMQ_TILE_Y_K + (k01 + 0),         MMQ_TILE_Y_K);
-            load_generic(B[1], y_qs + j0*MMQ_TILE_Y_K + (k01 + tile_B::J), MMQ_TILE_Y_K);
+            load_generic(B[0], y_qs + j0*MMQ_TILE_Y_K_LDS + (k01 + 0),         MMQ_TILE_Y_K_LDS);
+            load_generic(B[1], y_qs + j0*MMQ_TILE_Y_K_LDS + (k01 + tile_B::J), MMQ_TILE_Y_K_LDS);
 
             tile_C Cm[2];
             if (k01 >= MMQ_TILE_NE_K * 3/4) {
@@ -1816,7 +1906,7 @@ static __device__ __forceinline__ void vec_dot_q2_K_q8_1_mma(
             for (int l = 0; l < tile_C::ne/2; ++l) {
                 const int j = j0 + tile_C::get_j(l);
 
-                sB[l] = __half22float2(y_ds[j*MMQ_TILE_Y_K + (1 + k01/QI8_1)]);
+                sB[l] = __half22float2(y_ds[j*MMQ_TILE_Y_K_LDS + (1 + k01/QI8_1)]);
             }
 
 #pragma unroll
@@ -1964,8 +2054,8 @@ static __device__ __forceinline__ void vec_dot_q3_K_q8_1_dp4a(
                 const int8_t * scales = ((const int8_t *) (x_sc + i*(MMQ_TILE_NE_K/8) + i/8)) + k0/4;
 
                 sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q3_K_q8_1_impl_mmq(
-                    &x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k01], scales,
-                    x_df[i], y_df[j*MMQ_TILE_Y_K + k01/QI8_1]);
+                    &x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K_LDS + k01], scales,
+                    x_df[i], y_df[j*MMQ_TILE_Y_K_LDS + k01/QI8_1]);
             }
         }
     }
@@ -2118,8 +2208,8 @@ static __device__ __forceinline__ void vec_dot_q4_K_q8_1_dp4a(
                 const uint8_t * sc = (const uint8_t *) &x_sc[i * (MMQ_TILE_NE_K/8) + i/8 + k0/32] + 2*(k01/16);
 
                 sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q4_K_q8_1_impl_mmq(
-                    &x_qs[i*(MMQ_TILE_NE_K + 1) + k0/2], &y_qs[j*MMQ_TILE_Y_K + k01], sc, sc+8,
-                    x_dm[i], &y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);
+                    &x_qs[i*(MMQ_TILE_NE_K + 1) + k0/2], &y_qs[j*MMQ_TILE_Y_K_LDS + k01], sc, sc+8,
+                    x_dm[i], &y_ds[j*MMQ_TILE_Y_K_LDS + k01/QI8_1]);
             }
         }
     }
@@ -2275,8 +2365,8 @@ static __device__ __forceinline__ void vec_dot_q5_K_q8_1_dp4a(
                 const uint8_t * sc = ((const uint8_t *) &x_sc[i * (MMQ_TILE_NE_K/8) + i/8 + k00/32]) + 2*(k01/16);
 
                 sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q5_K_q8_1_impl_mmq(
-                    &x_qs[i*(QR5_K*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k01], sc, sc+8,
-                    x_dm[i], &y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);
+                    &x_qs[i*(QR5_K*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K_LDS + k01], sc, sc+8,
+                    x_dm[i], &y_ds[j*MMQ_TILE_Y_K_LDS + k01/QI8_1]);
             }
         }
     }
@@ -2396,8 +2486,8 @@ static __device__ __forceinline__ void vec_dot_q6_K_q8_1_dp4a(
                 const int8_t * sc = ((const int8_t *) &x_sc[i * (MMQ_TILE_NE_K/8) + i/8 + k0/16]);
 
                 sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q6_K_q8_1_impl_mmq(
-                    &x_qs[i*(QR6_K*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k01], sc,
-                    x_df[i*(MMQ_TILE_NE_K/QI6_K) + i/QI6_K], &y_df[j*MMQ_TILE_Y_K + k01/QI8_1]);
+                    &x_qs[i*(QR6_K*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K_LDS + k01], sc,
+                    x_df[i*(MMQ_TILE_NE_K/QI6_K) + i/QI6_K], &y_df[j*MMQ_TILE_Y_K_LDS + k01/QI8_1]);
             }
         }
     }
@@ -2417,7 +2507,7 @@ static __device__ __forceinline__ void vec_dot_q6_K_q8_1_mma(
     constexpr int rows_per_warp = granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const float * x_df = (const float *) x_qs + MMQ_TILE_NE_K*2;
@@ -2439,10 +2529,10 @@ static __device__ __forceinline__ void vec_dot_q6_K_q8_1_mma(
 #pragma unroll
         for (int j0 = 0; j0 < mmq_x; j0 += ntx*tile_C::J) {
             tile_B B[1];
-            load_generic(((tile_load *) B)[0], y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K);
+            load_generic(((tile_load *) B)[0], y_qs + j0*MMQ_TILE_Y_K_LDS + k01, MMQ_TILE_Y_K_LDS);
 
             const int j = j0 + tile_C::get_j(0);
-            const float dB = y_df[j*MMQ_TILE_Y_K + k01/QI8_1] / 2;
+            const float dB = y_df[j*MMQ_TILE_Y_K_LDS + k01/QI8_1] / 2;
 
 #pragma unroll
             for (int n = 0; n < ntx; ++n) {
@@ -2468,7 +2558,7 @@ static __device__ __forceinline__ void vec_dot_q6_K_q8_1_mma(
     constexpr int rows_per_warp = granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const float * x_df = (const float *) x_qs + MMQ_TILE_NE_K*2;
@@ -2490,10 +2580,10 @@ static __device__ __forceinline__ void vec_dot_q6_K_q8_1_mma(
 #pragma unroll
         for (int j0 = 0; j0 < mmq_x; j0 += ntx*tile_C::J) {
             tile_B B;
-            load_generic(B, y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K);
+            load_generic(B, y_qs + j0*MMQ_TILE_Y_K_LDS + k01, MMQ_TILE_Y_K_LDS);
 
             const int j = j0 + tile_C::get_j(0);
-            const float dB = y_df[j*MMQ_TILE_Y_K + k01/QI8_1];
+            const float dB = y_df[j*MMQ_TILE_Y_K_LDS + k01/QI8_1];
 
 #pragma unroll
             for (int n = 0; n < ntx; ++n) {
@@ -2519,7 +2609,7 @@ static __device__ __forceinline__ void vec_dot_q6_K_q8_1_mma(
     constexpr int rows_per_warp = 2 * granularity;
     constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.
 
-    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);
+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K_LDS);
 
     const int   * x_qs = (const int   *) x;
     const float * x_df = (const float *) x_qs + MMQ_TILE_NE_K*2;
@@ -2579,14 +2669,14 @@ static __device__ __forceinline__ void vec_dot_q6_K_q8_1_mma(
             float dB[tile_C::ne/2];
 
             // Here load_generic is faster than load_ldmatrix.
-            load_generic(B[0], y_qs + j0*MMQ_TILE_Y_K + 0         + k01, MMQ_TILE_Y_K);
-            load_generic(B[1], y_qs + j0*MMQ_TILE_Y_K + tile_B::J + k01, MMQ_TILE_Y_K);
+            load_generic(B[0], y_qs + j0*MMQ_TILE_Y_K_LDS + 0         + k01, MMQ_TILE_Y_K_LDS);
+            load_generic(B[1], y_qs + j0*MMQ_TILE_Y_K_LDS + tile_B::J + k01, MMQ_TILE_Y_K_LDS);
 
 #pragma unroll
             for (int l = 0; l < tile_C::ne/2; ++l) {
                 const int j = j0 + tile_C::get_j(l);
 
-                dB[l] = y_df[j*MMQ_TILE_Y_K + k01/QI8_1];
+                dB[l] = y_df[j*MMQ_TILE_Y_K_LDS + k01/QI8_1];
             }
 
 #pragma unroll
@@ -3375,7 +3465,7 @@ static __device__ __forceinline__ void mul_mat_q_process_tile(
 
     extern __shared__ int data_mul_mat_q[];
     int * tile_y = data_mul_mat_q + mmq_x;
-    int * tile_x = tile_y + GGML_PAD(mmq_x*MMQ_TILE_Y_K, nwarps*warp_size);
+    int * tile_x = tile_y + GGML_PAD(mmq_x*MMQ_TILE_Y_K_LDS, nwarps*warp_size);
 
 #if defined(AMD_MFMA_AVAILABLE) || defined(TURING_MMA_AVAILABLE) || defined(AMD_WMMA_AVAILABLE)
     constexpr vec_dot_mmq_t    vec_dot    = mmq_type_traits<mmq_x, mmq_y, need_check, type>::vec_dot_mma;
@@ -3404,15 +3494,24 @@ static __device__ __forceinline__ void mul_mat_q_process_tile(
         {
             const int * by0 = y + ncols_y * (kb0 * qk / ne_block) * sz;
 #pragma unroll
-            for (int l0 = 0; l0 < mmq_x * MMQ_TILE_Y_K; l0 += nwarps * warp_size) {
-                int l = l0 + threadIdx.y*warp_size + threadIdx.x;
-
+            for (int l0 = 0; l0 < mmq_x*MMQ_TILE_Y_K; l0 += nwarps*warp_size) {
+                const int l = l0 + threadIdx.y*warp_size + threadIdx.x;
                 tile_y[l] = by0[l];
             }
         }
 
         __syncthreads();
 
+// GFX906 PREFETCH: Issue AFTER barrier1, BEFORE vec_dot1
+// Warp 0: prefetch Y tile, Warp 1: prefetch X tile
+// Uses spare VGPRs to warm L2 cache for next iteration
+#if defined(GGML_USE_HIP) && defined(__gfx906__)
+        int prefetch_y = gfx906_prefetch_y_tile_v4<mmq_x, MMQ_TILE_Y_K, nwarps, warp_size>(
+            y, ncols_y, kb0, kb0_stop, qk, blocks_per_iter);
+        int prefetch_x = gfx906_prefetch_x_tile<mmq_y>(
+            x, offset_x, kb0, kb0_stop, blocks_per_iter, stride_row_x);
+#endif
+
         vec_dot(tile_x, tile_y, sum, 0);
 
         __syncthreads();
@@ -3420,9 +3519,8 @@ static __device__ __forceinline__ void mul_mat_q_process_tile(
         {
             const int * by0 = y + ncols_y * ((kb0 * qk / ne_block) * sz + sz);
 #pragma unroll
-            for (int l0 = 0; l0 < mmq_x * MMQ_TILE_Y_K; l0 += nwarps * warp_size) {
-                int l = l0 + threadIdx.y*warp_size + threadIdx.x;
-
+            for (int l0 = 0; l0 < mmq_x*MMQ_TILE_Y_K; l0 += nwarps*warp_size) {
+                const int l = l0 + threadIdx.y*warp_size + threadIdx.x;
                 tile_y[l] = by0[l];
             }
         }
@@ -3431,6 +3529,11 @@ static __device__ __forceinline__ void mul_mat_q_process_tile(
 
         vec_dot(tile_x, tile_y, sum, MMQ_TILE_NE_K);
 
+#if defined(GGML_USE_HIP) && defined(__gfx906__)
+        gfx906_prefetch_consume(prefetch_y);
+        gfx906_prefetch_consume(prefetch_x);
+#endif
+
         __syncthreads();
     }
 
diff --git a/ggml/src/ggml-cuda/mmvf.cu b/ggml/src/ggml-cuda/mmvf.cu
index 32948e4d7..d91472024 100644
--- a/ggml/src/ggml-cuda/mmvf.cu
+++ b/ggml/src/ggml-cuda/mmvf.cu
@@ -4,26 +4,48 @@
 #include "mmvf.cuh"
 #include "convert.cuh"
 
-template <typename T, typename type_acc, int ncols_dst, int block_size, bool has_fusion = false>
+template <typename T, typename type_acc, int ncols_dst, int block_size, bool has_fusion = false, bool is_multi_token_id = false>
 static __global__ void mul_mat_vec_f(
         const T * __restrict__ x, const float * __restrict__ y, const int32_t * __restrict__ ids, const ggml_cuda_mm_fusion_args_device fusion, float * __restrict__ dst,
-        const int ncols2, const int nchannels_y, const int stride_row, const int stride_col_y2, const int stride_col_dst,
+        const int ncols2, const uint3 nchannels_y, const int stride_row, const int stride_col_y2, const int stride_col_dst,
         const uint3 channel_ratio, const int stride_channel_x, const int stride_channel_y, const int stride_channel_dst,
-        const uint3 sample_ratio, const int stride_sample_x, const int stride_sample_y, const int stride_sample_dst) {
+        const uint3 sample_ratio, const int stride_sample_x, const int stride_sample_y, const int stride_sample_dst,
+        const int ids_stride) {
     const int row         = blockIdx.x;
+    // for MUL_MAT_ID - blockIdx.y = n_expert_used, blockIdx.z = ncols_dst (tokens)
     const int channel_dst = blockIdx.y;
-    const int channel_x   = ids ? ids[channel_dst]          : fastdiv((uint32_t) channel_dst, channel_ratio);
-    const int channel_y   = ids ? channel_dst % nchannels_y : channel_dst;
-    const int sample_dst  = blockIdx.z;
+    const int tid         = threadIdx.x;
+
+    int token_idx;
+    int channel_x;
+    int channel_y;
+    int sample_dst;
+
+    if constexpr (is_multi_token_id) {
+        // Multi-token MUL_MAT_ID path, adding these in the normal path causes a perf regression for n_tokens=1 case
+        token_idx  = blockIdx.z;
+        channel_x  = ids[channel_dst + token_idx * ids_stride];
+        channel_y  = fastmodulo(channel_dst, nchannels_y);
+        sample_dst = 0;
+    } else {
+        token_idx  = ids ? blockIdx.z                                          : 0;
+        channel_x  = ids ? ids[blockIdx.y + token_idx * ids_stride]            : fastdiv((uint32_t) channel_dst, channel_ratio);
+        channel_y  = ids ? fastmodulo(blockIdx.y, nchannels_y)                 : channel_dst;
+        sample_dst = ids ? 0                                                   : blockIdx.z;
+    }
+
     const int sample_x    = fastdiv((uint32_t) sample_dst, sample_ratio);
     const int sample_y    = sample_dst;
-    const int tid         = threadIdx.x;
 
     constexpr int warp_size   = ggml_cuda_get_physical_warp_size();
 
     x   += int64_t(sample_x)  *stride_sample_x   + channel_x  *stride_channel_x   + row*stride_row;
     y   += int64_t(sample_y)  *stride_sample_y   + channel_y  *stride_channel_y;
     dst += int64_t(sample_dst)*stride_sample_dst + channel_dst*stride_channel_dst;
+    if constexpr (is_multi_token_id) {
+        y   += token_idx*stride_col_y2*2;
+        dst += token_idx*stride_col_dst;
+    }
 
     bool use_gate = false;
     bool use_bias = false;
@@ -56,8 +78,10 @@ static __global__ void mul_mat_vec_f(
     if (use_gate) {
         gate_x += int64_t(sample_x)  *stride_sample_x   + channel_x  *stride_channel_x   + row*stride_row;
     }
+
+    const int channel_bias = ids ? channel_x : channel_dst;
+
     if constexpr (has_fusion) {
-        const int channel_bias = ids ? channel_x : channel_dst;
         if (use_bias) {
             x_bias += int64_t(sample_dst)*stride_sample_dst + channel_bias*stride_channel_dst;
         }
@@ -349,36 +373,36 @@ static __global__ void mul_mat_vec_f(
     }
 }
 
-template<typename T, typename type_acc, int ncols_dst, int block_size>
+template<typename T, typename type_acc, int ncols_dst, int block_size, bool is_multi_token_id = false>
 static void mul_mat_vec_f_switch_fusion(
         const T * x, const float * y, const int32_t * ids, const ggml_cuda_mm_fusion_args_device fusion, float * dst,
-        const int64_t ncols, const int64_t nrows,
+        const int64_t ncols, const uint3 nchannels_y,
         const int64_t stride_row, const int64_t stride_col_y, const int64_t stride_col_dst,
         const uint3 channel_ratio, const int stride_channel_x, const int stride_channel_y, const int stride_channel_dst,
         const uint3 sample_ratio, const int stride_sample_x, const int stride_sample_y, const int stride_sample_dst,
-        const dim3 & block_dims, const dim3 & block_nums, const int nbytes_shared, const cudaStream_t stream) {
+        const dim3 & block_dims, const dim3 & block_nums, const int nbytes_shared, const int ids_stride, const cudaStream_t stream) {
 
     const bool has_fusion = fusion.gate != nullptr || fusion.x_bias != nullptr || fusion.gate_bias != nullptr;
     if constexpr (ncols_dst == 1) {
         if (has_fusion) {
-            mul_mat_vec_f<T, type_acc, ncols_dst, block_size, true><<<block_nums, block_dims, nbytes_shared, stream>>>
-                (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,
+            mul_mat_vec_f<T, type_acc, ncols_dst, block_size, true, is_multi_token_id><<<block_nums, block_dims, nbytes_shared, stream>>>
+                (x, y, ids, fusion, dst, ncols, nchannels_y, stride_row, stride_col_y, stride_col_dst,
                 channel_ratio, stride_channel_x, stride_channel_y, stride_channel_dst,
-                sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);
+                sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride);
             return;
        }
     }
 
     GGML_ASSERT(!has_fusion && "fusion only supported for ncols_dst=1");
 
-    mul_mat_vec_f<T, type_acc, ncols_dst, block_size><<<block_nums, block_dims, nbytes_shared, stream>>>
-        (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,
+    mul_mat_vec_f<T, type_acc, ncols_dst, block_size, false, is_multi_token_id><<<block_nums, block_dims, nbytes_shared, stream>>>
+        (x, y, ids, fusion, dst, ncols, nchannels_y, stride_row, stride_col_y, stride_col_dst,
         channel_ratio, stride_channel_x, stride_channel_y, stride_channel_dst,
-        sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);
+        sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride);
 
 }
 
-template <typename T, typename type_acc, int ncols_dst>
+template <typename T, typename type_acc, int ncols_dst, bool is_multi_token_id = false>
 void launch_mul_mat_vec_f_cuda(
         const T * x, const float * y, const int32_t * ids, const ggml_cuda_mm_fusion_args_device fusion, float * dst,
         const int64_t ncols, const int64_t nrows,
@@ -386,12 +410,13 @@ void launch_mul_mat_vec_f_cuda(
         const int64_t nchannels_x, const int64_t nchannels_y, const int64_t nchannels_dst,
         const int64_t stride_channel_x, const int64_t stride_channel_y, const int64_t stride_channel_dst, const int64_t nsamples_x,
         const int64_t nsamples_dst, const int64_t stride_sample_x, const int64_t stride_sample_y, const int64_t stride_sample_dst,
-        cudaStream_t stream) {
+        const int64_t nsamples_or_ntokens, const int64_t ids_stride, cudaStream_t stream) {
     GGML_ASSERT(ncols        % 2 == 0);
     GGML_ASSERT(stride_row   % 2 == 0);
     GGML_ASSERT(stride_col_y % 2 == 0);
     GGML_ASSERT(ids || nchannels_dst % nchannels_x == 0);
     GGML_ASSERT(       nsamples_dst  % nsamples_x  == 0);
+    const uint3 nchannels_y_fd   = ids ? init_fastdiv_values(nchannels_y) : make_uint3(0, 0, 0);
     const uint3 channel_ratio_fd = ids ? make_uint3(0, 0, 0) : init_fastdiv_values(nchannels_dst / nchannels_x);
     const uint3 sample_ratio_fd  = init_fastdiv_values(nsamples_dst  / nsamples_x);
 
@@ -415,56 +440,56 @@ void launch_mul_mat_vec_f_cuda(
     const bool has_fusion = fusion.gate != nullptr || fusion.x_bias != nullptr || fusion.gate_bias != nullptr;
 
     const int nbytes_shared = warp_size*sizeof(float) + (has_fusion ? warp_size*sizeof(float) : 0);
-    const dim3 block_nums(nrows, nchannels_dst, nsamples_dst);
+    const dim3 block_nums(nrows, nchannels_dst, nsamples_or_ntokens);
     const dim3 block_dims(block_size_best, 1, 1);
     switch (block_size_best) {
         case   32: {
-            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 32>
-                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,
+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 32, is_multi_token_id>
+                (x, y, ids, fusion, dst, ncols/2, nchannels_y_fd, stride_row, stride_col_y/2, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);
+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, ids_stride, stream);
         } break;
         case   64: {
-            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 64>
-                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,
+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 64, is_multi_token_id>
+                (x, y, ids, fusion, dst, ncols/2, nchannels_y_fd, stride_row, stride_col_y/2, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);
+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, ids_stride, stream);
         } break;
         case   96: {
-            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 96>
-                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,
+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 96, is_multi_token_id>
+                (x, y, ids, fusion, dst, ncols/2, nchannels_y_fd, stride_row, stride_col_y/2, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);
+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, ids_stride, stream);
         } break;
         case  128: {
-            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 128>
-                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,
+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 128, is_multi_token_id>
+                (x, y, ids, fusion, dst, ncols/2, nchannels_y_fd, stride_row, stride_col_y/2, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);
+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, ids_stride, stream);
         } break;
         case  160: {
-            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 160>
-                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,
+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 160, is_multi_token_id>
+                (x, y, ids, fusion, dst, ncols/2, nchannels_y_fd, stride_row, stride_col_y/2, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);
+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, ids_stride, stream);
         } break;
         case  192: {
-            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 192>
-                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,
+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 192, is_multi_token_id>
+                (x, y, ids, fusion, dst, ncols/2, nchannels_y_fd, stride_row, stride_col_y/2, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);
+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, ids_stride, stream);
         } break;
         case  224: {
-            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 224>
-                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,
+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 224, is_multi_token_id>
+                (x, y, ids, fusion, dst, ncols/2, nchannels_y_fd, stride_row, stride_col_y/2, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);
+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, ids_stride, stream);
         } break;
         case  256: {
-            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 256>
-                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,
+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 256, is_multi_token_id>
+                (x, y, ids, fusion, dst, ncols/2, nchannels_y_fd, stride_row, stride_col_y/2, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);
+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, ids_stride, stream);
         } break;
         default: {
             GGML_ABORT("fatal error");
@@ -480,55 +505,88 @@ static void mul_mat_vec_f_cuda_switch_ncols_dst(
         const int64_t nchannels_x, const int64_t nchannels_y, const int64_t nchannels_dst,
         const int64_t stride_channel_x, const int64_t stride_channel_y, const int64_t stride_channel_dst, const int64_t nsamples_x,
         const int64_t nsamples_dst, const int64_t stride_sample_x, const int64_t stride_sample_y, const int64_t stride_sample_dst,
-        cudaStream_t stream) {
+        const int64_t ids_stride, cudaStream_t stream) {
+
+    const bool has_ids = ids != nullptr;
+
+    if (has_ids && ncols_dst > 1) {
+        // Multi-token MUL_MAT_ID path only - single-token goes through regular path below
+        constexpr int c_ncols_dst = 1;
+        launch_mul_mat_vec_f_cuda<T, type_acc, c_ncols_dst, true>
+            (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,
+             nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
+             stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,
+             ncols_dst, ids_stride, stream);
+        return;
+    }
+
+    if (has_ids) {
+        // Single-token MUL_MAT_ID path
+        constexpr int c_ncols_dst = 1;
+        launch_mul_mat_vec_f_cuda<T, type_acc, c_ncols_dst>
+            (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,
+             nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
+             stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,
+             ncols_dst, ids_stride, stream);
+        return;
+    }
+
     switch (ncols_dst) {
         case 1:
             launch_mul_mat_vec_f_cuda<T, type_acc, 1>
                 (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
-                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,
+                 nsamples_dst, ids_stride, stream);
             break;
         case 2:
             launch_mul_mat_vec_f_cuda<T, type_acc, 2>
                 (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
-                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,
+                 nsamples_dst, ids_stride, stream);
             break;
         case 3:
             launch_mul_mat_vec_f_cuda<T, type_acc, 3>
                 (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
-                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,
+                 nsamples_dst, ids_stride, stream);
             break;
         case 4:
             launch_mul_mat_vec_f_cuda<T, type_acc, 4>
                 (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
-                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,
+                 nsamples_dst, ids_stride, stream);
             break;
         case 5:
             launch_mul_mat_vec_f_cuda<T, type_acc, 5>
                 (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
-                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,
+                 nsamples_dst, ids_stride, stream);
             break;
         case 6:
             launch_mul_mat_vec_f_cuda<T, type_acc, 6>
                 (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
-                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,
+                 nsamples_dst, ids_stride, stream);
             break;
         case 7:
             launch_mul_mat_vec_f_cuda<T, type_acc, 7>
                 (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
-                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,
+                 nsamples_dst, ids_stride, stream);
             break;
         case 8:
             launch_mul_mat_vec_f_cuda<T, type_acc, 8>
                 (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
-                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,
+                 nsamples_dst, ids_stride, stream);
             break;
         default:
             GGML_ABORT("fatal error");
@@ -544,21 +602,21 @@ static void mul_mat_vec_f_cuda(
         const int64_t nchannels_x, const int64_t nchannels_y, const int64_t nchannels_dst,
         const int64_t stride_channel_x, const int64_t stride_channel_y, const int64_t stride_channel_dst, const int64_t nsamples_x,
         const int64_t nsamples_dst, const int64_t stride_sample_x, const int64_t stride_sample_y, const int64_t stride_sample_dst,
-        enum ggml_prec prec, cudaStream_t stream) {
+        const int64_t ids_stride, enum ggml_prec prec, cudaStream_t stream) {
 
     if constexpr(std::is_same_v<T, half>) {
         if (prec == GGML_PREC_DEFAULT) {
             mul_mat_vec_f_cuda_switch_ncols_dst<T, half>
                 (x, y, ids, fusion, dst, ncols, nrows, ncols_dst, stride_row, stride_col_y, stride_col_dst,
                 nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
-                stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             return;
         }
     }
     mul_mat_vec_f_cuda_switch_ncols_dst<T, float>
         (x, y, ids, fusion, dst, ncols, nrows, ncols_dst, stride_row, stride_col_y, stride_col_dst,
         nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
-        stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+        stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
 }
 
 void ggml_cuda_mul_mat_vec_f(ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst,
@@ -573,7 +631,7 @@ void ggml_cuda_mul_mat_vec_f(ggml_backend_cuda_context & ctx, const ggml_tensor
     const size_t ts_src1 = ggml_type_size(src1->type);
     const size_t ts_dst  = ggml_type_size(dst->type);
 
-    GGML_ASSERT(!ids || ne12 == 1); // Implementation is only correct for  batch size 1.
+    GGML_ASSERT(!ids || ne12 <= MMVF_MAX_BATCH_SIZE);
     GGML_ASSERT(ne13 == ne3);
 
     GGML_ASSERT(        nb00       == ts_src0);
@@ -626,29 +684,31 @@ void ggml_cuda_mul_mat_vec_f(ggml_backend_cuda_context & ctx, const ggml_tensor
     const int64_t ncols_dst          = ids ? ne2  : ne1;
     const int64_t nchannels_y        = ids ? ne11 : ne12;
     const int64_t nchannels_dst      = ids ? ne1  : ne2;
+    const int64_t stride_col_dst     = ids ? s2   : s1;
+    const int64_t stride_col_y       = ids ? s12  : s11;
     const int64_t stride_channel_dst = ids ? s1   : s2;
     const int64_t stride_channel_y   = ids ? s11  : s12;
 
-    GGML_ASSERT(!ids || ncols_dst == 1);
+    const int64_t ids_stride = ids ? ids->nb[1] / ggml_type_size(ids->type) : 0;
 
     switch (src0->type) {
         case GGML_TYPE_F32: {
             const float * src0_d = (const float *) src0->data;
-            mul_mat_vec_f_cuda(src0_d, src1_d, ids_d, fusion_local, dst_d, ne00, ne01, ncols_dst, s01, s11, s1,
+            mul_mat_vec_f_cuda(src0_d, src1_d, ids_d, fusion_local, dst_d, ne00, ne01, ncols_dst, s01, stride_col_y, stride_col_dst,
                 ne02, nchannels_y, nchannels_dst, s02, stride_channel_y, stride_channel_dst,
-                ne03,              ne3,           s03, s13,              s3,                 prec, ctx.stream());
+                ne03,              ne3,           s03, s13,              s3,                 ids_stride, prec, ctx.stream());
         } break;
         case GGML_TYPE_F16: {
             const half * src0_d = (const half *) src0->data;
-            mul_mat_vec_f_cuda(src0_d, src1_d, ids_d, fusion_local, dst_d, ne00, ne01, ncols_dst, s01, s11, s1,
+            mul_mat_vec_f_cuda(src0_d, src1_d, ids_d, fusion_local, dst_d, ne00, ne01, ncols_dst, s01, stride_col_y, stride_col_dst,
                 ne02, nchannels_y, nchannels_dst, s02, stride_channel_y, stride_channel_dst,
-                ne03,              ne3,           s03, s13,              s3,                 prec, ctx.stream());
+                ne03,              ne3,           s03, s13,              s3,                 ids_stride, prec, ctx.stream());
         } break;
         case GGML_TYPE_BF16: {
             const nv_bfloat16 * src0_d = (const nv_bfloat16 *) src0->data;
-            mul_mat_vec_f_cuda(src0_d, src1_d, ids_d, fusion_local, dst_d, ne00, ne01, ncols_dst, s01, s11, s1,
+            mul_mat_vec_f_cuda(src0_d, src1_d, ids_d, fusion_local, dst_d, ne00, ne01, ncols_dst, s01, stride_col_y, stride_col_dst,
                 ne02, nchannels_y, nchannels_dst, s02, stride_channel_y, stride_channel_dst,
-                ne03,              ne3,           s03, s13,              s3,                 prec, ctx.stream());
+                ne03,              ne3,           s03, s13,              s3,                 ids_stride, prec, ctx.stream());
         } break;
         default:
             GGML_ABORT("unsupported type: %s", ggml_type_name(src0->type));
@@ -695,19 +755,19 @@ void ggml_cuda_op_mul_mat_vec_f(
             const float * src0_d = (const float *) src0_dd_i;
             mul_mat_vec_f_cuda(src0_d, src1_ddf_i, nullptr, empty, dst_dd_i, ne00, row_diff, src1_ncols, stride_row, stride_col_y, stride_col_dst,
                 nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, prec, stream);
+                nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, 0, prec, stream);
         } break;
         case GGML_TYPE_F16: {
             const half * src0_d = (const half *) src0_dd_i;
             mul_mat_vec_f_cuda(src0_d, src1_ddf_i, nullptr, empty, dst_dd_i, ne00, row_diff, src1_ncols, stride_row, stride_col_y, stride_col_dst,
                 nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, prec, stream);
+                nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, 0, prec, stream);
         } break;
         case GGML_TYPE_BF16: {
             const nv_bfloat16 * src0_d = (const nv_bfloat16 *) src0_dd_i;
             mul_mat_vec_f_cuda(src0_d, src1_ddf_i, nullptr, empty, dst_dd_i, ne00, row_diff, src1_ncols, stride_row, stride_col_y, stride_col_dst,
                 nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, prec, stream);
+                nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, 0, prec, stream);
         } break;
         default:
             GGML_ABORT("unsupported type: %s", ggml_type_name(src0->type));
diff --git a/ggml/src/ggml-cuda/mmvf.cuh b/ggml/src/ggml-cuda/mmvf.cuh
index a09fbdc72..a50f7c021 100644
--- a/ggml/src/ggml-cuda/mmvf.cuh
+++ b/ggml/src/ggml-cuda/mmvf.cuh
@@ -1,5 +1,7 @@
 #include "common.cuh"
 
+#define MMVF_MAX_BATCH_SIZE 8 // Max. batch size for which to use MMVF kernels.
+
 void ggml_cuda_mul_mat_vec_f(ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst,
     const ggml_cuda_mm_fusion_args_host * fusion = nullptr);
 
diff --git a/ggml/src/ggml-cuda/mmvq.cu b/ggml/src/ggml-cuda/mmvq.cu
index d671551c1..4857bc26b 100644
--- a/ggml/src/ggml-cuda/mmvq.cu
+++ b/ggml/src/ggml-cuda/mmvq.cu
@@ -3,6 +3,13 @@
 #include "unary.cuh"
 #include "vecdotq.cuh"
 
+// GFX906-specific warp-cooperative MMVQ kernels (compile with -DGGML_HIP_GFX906)
+#if defined(GGML_HIP_GFX906)
+#include "gfx906/matmul/mmvq-q4_0.cuh"
+#include "gfx906/matmul/mmvq-q4_1.cuh"
+#include "gfx906/matmul/mmvq-q8_0.cuh"
+#endif
+
 #include <cstdint>
 
 typedef float (*vec_dot_q_cuda_t)(const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & kbx, const int & iqs);
@@ -137,15 +144,15 @@ static constexpr __host__ __device__ int calc_rows_per_block(int ncols_dst, int
     return 1;
 }
 
-// tell the compiler to use as many registers as it wants, see nwarps definition below
-template <ggml_type type, int ncols_dst, bool has_fusion>
+template <ggml_type type, int ncols_dst, bool has_fusion, bool is_multi_token_id = false>
 __launch_bounds__(calc_nwarps(ncols_dst, get_device_table_id())*ggml_cuda_get_physical_warp_size(), 1)
 static __global__ void mul_mat_vec_q(
         const void * __restrict__ vx, const void * __restrict__ vy, const int32_t * __restrict__ ids, const ggml_cuda_mm_fusion_args_device fusion, float * __restrict__ dst,
         const uint32_t ncols_x, const uint3 nchannels_y, const uint32_t stride_row_x, const uint32_t stride_col_y,
         const uint32_t stride_col_dst, const uint3 channel_ratio, const uint32_t stride_channel_x,
         const uint32_t stride_channel_y, const uint32_t stride_channel_dst, const uint3 sample_ratio,
-        const uint32_t stride_sample_x, const uint32_t stride_sample_y, const uint32_t stride_sample_dst) {
+        const uint32_t stride_sample_x, const uint32_t stride_sample_y, const uint32_t stride_sample_dst,
+        const uint32_t ids_stride) {
 
     constexpr int qk  = ggml_cuda_type_traits<type>::qk;
     constexpr int qi  = ggml_cuda_type_traits<type>::qi;
@@ -162,11 +169,25 @@ static __global__ void mul_mat_vec_q(
     const     int blocks_per_row_x = ncols_x / qk;
     constexpr int blocks_per_iter = vdr * nwarps*warp_size / qi;
 
-    // The MUL_MAT_ID code path with ids != nullptr is only implemented for ncols_dst == 1.
     const uint32_t channel_dst = blockIdx.y;
-    const uint32_t channel_x   = ncols_dst == 1 && ids ? ids[channel_dst]                     : fastdiv(channel_dst, channel_ratio);
-    const uint32_t channel_y   = ncols_dst == 1 && ids ? fastmodulo(channel_dst, nchannels_y) : channel_dst;
-    const uint32_t sample_dst  = blockIdx.z;
+
+    uint32_t token_idx = 0;
+    uint32_t channel_x;
+    uint32_t channel_y;
+    uint32_t sample_dst;
+
+    if constexpr (is_multi_token_id) {
+        // Multi-token MUL_MAT_ID path, adding these in the normal path causes a perf regression for n_tokens=1 case
+        token_idx  = blockIdx.z;
+        channel_x  = ids[channel_dst + token_idx * ids_stride];
+        channel_y  = fastmodulo(channel_dst, nchannels_y);
+        sample_dst = 0;
+    } else {
+        channel_x  = ncols_dst == 1 && ids ? ids[channel_dst]                     : fastdiv(channel_dst, channel_ratio);
+        channel_y  = ncols_dst == 1 && ids ? fastmodulo(channel_dst, nchannels_y) : channel_dst;
+        sample_dst = blockIdx.z;
+    }
+
     const uint32_t sample_x    = fastdiv(sample_dst, sample_ratio);
     const uint32_t sample_y    = sample_dst;
 
@@ -188,11 +209,11 @@ static __global__ void mul_mat_vec_q(
         active_glu    = fusion.glu_op;
     }
 
-    const uint32_t channel_bias = ids ? channel_x : channel_dst;
 
     float x_biases[ncols_dst]    = { 0.0f };
     float gate_biases[ncols_dst] = { 0.0f };
     if constexpr (has_fusion) {
+        const uint32_t channel_bias = ids ? channel_x : channel_dst;
         if (use_bias) {
             x_bias = x_bias + sample_dst*stride_sample_dst + channel_bias*stride_channel_dst + row0;
             // 1. Hide latency by prefetching bias and gate here
@@ -222,6 +243,9 @@ static __global__ void mul_mat_vec_q(
     float tmp_gate[ncols_dst][rows_per_cuda_block] = {{0.0f}};
 
     const block_q8_1 * y = ((const block_q8_1 *) vy) + sample_y*stride_sample_y + channel_y*stride_channel_y;
+    if constexpr (is_multi_token_id) {
+        y += token_idx*stride_col_y;
+    }
     const int kbx_offset = sample_x*stride_sample_x + channel_x*stride_channel_x + row0*stride_row_x;
 
     for (int kbx = tid / (qi/vdr); kbx < blocks_per_row_x; kbx += blocks_per_iter) {
@@ -275,6 +299,10 @@ static __global__ void mul_mat_vec_q(
 
     dst += sample_dst*stride_sample_dst + channel_dst*stride_channel_dst + row0;
 
+    if constexpr (is_multi_token_id) {
+        dst += token_idx*stride_col_dst;
+    }
+
     // sum up partial sums and write back result
 #pragma unroll
     for (int j = 0; j < ncols_dst; ++j) {
@@ -335,40 +363,41 @@ static __global__ void mul_mat_vec_q(
 }
 
 static std::pair<dim3, dim3> calc_launch_params(
-        const int ncols_dst, const int nrows_x, const int nchannels_y, const int nsamples_y,
+        const int ncols_dst, const int nrows_x, const int nchannels_dst, const int nsamples_or_ntokens,
         const int warp_size, const mmvq_parameter_table_id table_id) {
     const int64_t nblocks = (nrows_x + calc_rows_per_block(ncols_dst, table_id) - 1) / calc_rows_per_block(ncols_dst, table_id);
-    const dim3 block_nums(nblocks, nchannels_y, nsamples_y);
+    const dim3 block_nums(nblocks, nchannels_dst, nsamples_or_ntokens);
     const dim3 block_dims(warp_size, calc_nwarps(ncols_dst, table_id), 1);
     return {block_nums, block_dims};
 }
 
-template<ggml_type type, int c_ncols_dst>
+template<ggml_type type, int c_ncols_dst, bool is_multi_token_id = false>
 static void mul_mat_vec_q_switch_fusion(
         const void * vx, const void * vy, const int32_t * ids, const ggml_cuda_mm_fusion_args_device fusion, float * dst,
         const uint32_t ncols_x, const uint3 nchannels_y, const uint32_t stride_row_x, const uint32_t stride_col_y,
         const uint32_t stride_col_dst, const uint3 channel_ratio, const uint32_t stride_channel_x,
         const uint32_t stride_channel_y, const uint32_t stride_channel_dst, const uint3 sample_ratio,
         const uint32_t stride_sample_x, const uint32_t stride_sample_y, const uint32_t stride_sample_dst,
-        const dim3 & block_nums, const dim3 & block_dims, const int nbytes_shared, cudaStream_t stream) {
+        const dim3 & block_nums, const dim3 & block_dims, const int nbytes_shared,
+        const uint32_t ids_stride, cudaStream_t stream) {
 
     const bool has_fusion = fusion.gate != nullptr || fusion.x_bias != nullptr || fusion.gate_bias != nullptr;
     if constexpr (c_ncols_dst == 1) {
         if (has_fusion) {
-            mul_mat_vec_q<type, c_ncols_dst, true><<<block_nums, block_dims, nbytes_shared, stream>>>
+            mul_mat_vec_q<type, c_ncols_dst, true, is_multi_token_id><<<block_nums, block_dims, nbytes_shared, stream>>>
                 (vx, vy, ids, fusion, dst, ncols_x, nchannels_y, stride_row_x, stride_col_y, stride_col_dst,
                  channel_ratio, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);
+                 sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride);
             return;
         }
     }
 
     GGML_ASSERT(!has_fusion && "fusion only supported for ncols_dst=1");
 
-    mul_mat_vec_q<type, c_ncols_dst, false><<<block_nums, block_dims, nbytes_shared, stream>>>
+    mul_mat_vec_q<type, c_ncols_dst, false, is_multi_token_id><<<block_nums, block_dims, nbytes_shared, stream>>>
         (vx, vy, ids, fusion, dst, ncols_x, nchannels_y, stride_row_x, stride_col_y, stride_col_dst,
         channel_ratio, stride_channel_x, stride_channel_y, stride_channel_dst,
-        sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);
+        sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride);
 }
 
 template <ggml_type type>
@@ -379,7 +408,7 @@ static void mul_mat_vec_q_switch_ncols_dst(
         const int nchannels_x, const int nchannels_y, const int nchannels_dst,
         const int stride_channel_x, const int stride_channel_y, const int stride_channel_dst,
         const int nsamples_x, const int nsamples_dst, const int stride_sample_x, const int stride_sample_y, const int stride_sample_dst,
-        cudaStream_t stream) {
+        const int ids_stride, cudaStream_t stream) {
 
     GGML_ASSERT(ncols_x % ggml_blck_size(type) == 0);
     GGML_ASSERT(ncols_dst <= MMVQ_MAX_BATCH_SIZE);
@@ -393,8 +422,19 @@ static void mul_mat_vec_q_switch_ncols_dst(
     const mmvq_parameter_table_id table_id = get_device_table_id(ggml_cuda_info().devices[device].cc);
 
     const bool has_fusion = fusion.gate != nullptr || fusion.x_bias != nullptr || fusion.gate_bias != nullptr;
+    const bool has_ids = ids != nullptr;
+
+    if (has_ids && ncols_dst > 1) {
+        // Multi-token MUL_MAT_ID path only - single-token goes through regular path below
+        constexpr int c_ncols_dst = 1;
+        std::pair<dim3, dim3> dims = calc_launch_params(c_ncols_dst, nrows_x, nchannels_dst, ncols_dst, warp_size, table_id);
+        mul_mat_vec_q_switch_fusion<type, c_ncols_dst, true>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,
+             channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
+             sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,
+             dims.first, dims.second, 0, ids_stride, stream);
+        return;
+    }
 
-    GGML_ASSERT(!ids || ncols_dst == 1);
     switch (ncols_dst) {
         case 1: {
             constexpr int c_ncols_dst = 1;
@@ -402,7 +442,7 @@ static void mul_mat_vec_q_switch_ncols_dst(
             mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
                  sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,
-                 dims.first, dims.second, 0, stream);
+                 dims.first, dims.second, 0, ids_stride, stream);
         } break;
         case 2: {
             constexpr int c_ncols_dst = 2;
@@ -410,7 +450,7 @@ static void mul_mat_vec_q_switch_ncols_dst(
             mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
                  sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,
-                 dims.first, dims.second, 0, stream);
+                 dims.first, dims.second, 0, ids_stride, stream);
         } break;
         case 3: {
             constexpr int c_ncols_dst = 3;
@@ -418,7 +458,7 @@ static void mul_mat_vec_q_switch_ncols_dst(
             mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
                  sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,
-                 dims.first, dims.second, 0, stream);
+                 dims.first, dims.second, 0, ids_stride, stream);
         } break;
         case 4: {
             constexpr int c_ncols_dst = 4;
@@ -426,7 +466,7 @@ static void mul_mat_vec_q_switch_ncols_dst(
             mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
                  sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,
-                 dims.first, dims.second, 0, stream);
+                 dims.first, dims.second, 0, ids_stride, stream);
         } break;
         case 5: {
             constexpr int c_ncols_dst = 5;
@@ -434,7 +474,7 @@ static void mul_mat_vec_q_switch_ncols_dst(
             mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
                  sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,
-                 dims.first, dims.second, 0, stream);
+                 dims.first, dims.second, 0, ids_stride, stream);
         } break;
         case 6: {
             constexpr int c_ncols_dst = 6;
@@ -442,7 +482,7 @@ static void mul_mat_vec_q_switch_ncols_dst(
             mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
                  sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,
-                 dims.first, dims.second, 0, stream);
+                 dims.first, dims.second, 0, ids_stride, stream);
         } break;
         case 7: {
             constexpr int c_ncols_dst = 7;
@@ -450,7 +490,7 @@ static void mul_mat_vec_q_switch_ncols_dst(
             mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
                  sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,
-                 dims.first, dims.second, 0, stream);
+                 dims.first, dims.second, 0, ids_stride, stream);
         } break;
         case 8: {
             constexpr int c_ncols_dst = 8;
@@ -458,7 +498,7 @@ static void mul_mat_vec_q_switch_ncols_dst(
             mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,
                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
                  sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,
-                 dims.first, dims.second, 0, stream);
+                 dims.first, dims.second, 0, ids_stride, stream);
         } break;
         default:
             GGML_ABORT("fatal error");
@@ -474,127 +514,188 @@ static void mul_mat_vec_q_switch_type(
         const int nchannels_x, const int nchannels_y, const int nchannels_dst,
         const int stride_channel_x, const int stride_channel_y, const int stride_channel_dst,
         const int nsamples_x, const int nsamples_dst, const int stride_sample_x, const int stride_sample_y, const int stride_sample_dst,
-        cudaStream_t stream) {
+        const int ids_stride, cudaStream_t stream) {
     switch (type_x) {
         case GGML_TYPE_Q4_0:
+#if defined(GGML_HIP_GFX906)
+            // GFX906: Use warp-cooperative kernel for ncols_dst=1 (token generation) without fusion
+            {
+                const bool has_fusion = fusion.gate != nullptr || fusion.x_bias != nullptr || fusion.gate_bias != nullptr;
+
+                if (ncols_dst == 1 && !has_fusion && ncols_x <= 1024) {
+                    const uint3 nchannels_y_fd   = ids ? init_fastdiv_values(nchannels_y) : make_uint3(0, 0, 0);
+                    const uint3 channel_ratio_fd = ids ? make_uint3(0, 0, 0) : init_fastdiv_values(nchannels_dst / nchannels_x);
+                    const uint3 sample_ratio_fd  = init_fastdiv_values(nsamples_dst / nsamples_x);
+
+                    gfx906_launch_mul_mat_vec_q4_0_warp_coop(
+                        vx, vy, ids, dst,
+                        ncols_x, nchannels_y_fd, stride_row_x, stride_col_dst,
+                        channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
+                        sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,
+                        nrows_x, nchannels_dst, nsamples_dst, stream);
+                    break;
+                }
+            }
+#endif
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q4_0>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_Q4_1:
+#if defined(GGML_HIP_GFX906)
+            // GFX906: Use warp-cooperative kernel for ncols_dst=1 (token generation) without fusion
+            {
+                const bool has_fusion = fusion.gate != nullptr || fusion.x_bias != nullptr || fusion.gate_bias != nullptr;
+
+                // Use warp-coop for small matrices only (MoE experts, <= 1024 cols)
+                if (ncols_dst == 1 && !has_fusion && ncols_x <= 1024) {
+                    const uint3 nchannels_y_fd   = ids ? init_fastdiv_values(nchannels_y) : make_uint3(0, 0, 0);
+                    const uint3 channel_ratio_fd = ids ? make_uint3(0, 0, 0) : init_fastdiv_values(nchannels_dst / nchannels_x);
+                    const uint3 sample_ratio_fd  = init_fastdiv_values(nsamples_dst / nsamples_x);
+
+                    gfx906_launch_mul_mat_vec_q4_1_warp_coop(
+                        vx, vy, ids, dst,
+                        ncols_x, nchannels_y_fd, stride_row_x, stride_col_dst,
+                        channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
+                        sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,
+                        nrows_x, nchannels_dst, nsamples_dst, stream);
+                    break;
+                }
+            }
+#endif
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q4_1>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_Q5_0:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q5_0>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_Q5_1:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q5_1>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_Q8_0:
+#if defined(GGML_HIP_GFX906)
+            // GFX906: Use warp-cooperative kernel for ncols_dst=1 (token generation) without fusion
+            {
+                const bool has_fusion = fusion.gate != nullptr || fusion.x_bias != nullptr || fusion.gate_bias != nullptr;
+
+                if (ncols_dst == 1 && !has_fusion && ncols_x <= 1024) {
+                    const uint3 nchannels_y_fd   = ids ? init_fastdiv_values(nchannels_y) : make_uint3(0, 0, 0);
+                    const uint3 channel_ratio_fd = ids ? make_uint3(0, 0, 0) : init_fastdiv_values(nchannels_dst / nchannels_x);
+                    const uint3 sample_ratio_fd  = init_fastdiv_values(nsamples_dst / nsamples_x);
+
+                    gfx906_launch_mul_mat_vec_q8_0_warp_coop(
+                        vx, vy, ids, dst,
+                        ncols_x, nchannels_y_fd, stride_row_x, stride_col_dst,
+                        channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,
+                        sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,
+                        nrows_x, nchannels_dst, nsamples_dst, stream);
+                    break;
+                }
+            }
+#endif
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q8_0>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_MXFP4:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_MXFP4>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_Q2_K:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q2_K>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_Q3_K:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q3_K>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_Q4_K:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q4_K>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_Q5_K:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q5_K>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_Q6_K:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q6_K>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_IQ2_XXS:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ2_XXS>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_IQ2_XS:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ2_XS>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_IQ2_S:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ2_S>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_IQ3_XXS:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ3_XXS>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_IQ1_S:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ1_S>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_IQ1_M:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ1_M>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_IQ4_NL:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ4_NL>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_IQ4_XS:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ4_XS>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         case GGML_TYPE_IQ3_S:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ3_S>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, ids_stride, stream);
             break;
         default:
             GGML_ABORT("fatal error");
@@ -622,7 +723,7 @@ void ggml_cuda_mul_mat_vec_q(
     GGML_ASSERT(        nb0        == ts_dst);
     GGML_ASSERT(!ids || ids->nb[0] == ggml_type_size(ids->type));
 
-    GGML_ASSERT(!ids || ne12 == 1); // Implementation is only correct for batch size 1.
+    GGML_ASSERT(!ids || ne12 <= MMVQ_MAX_BATCH_SIZE);
 
     const float   * src1_d =       (const float   *) src1->data;
     const int32_t *  ids_d = ids ? (const int32_t *)  ids->data : nullptr;
@@ -693,11 +794,13 @@ void ggml_cuda_mul_mat_vec_q(
     const int64_t stride_channel_dst = ids ? s1   : s2;
     const int64_t stride_channel_y   = ids ? s11  : s12;
 
+    const int64_t ids_stride = ids ? ids->nb[1] / ggml_type_size(ids->type) : 0;
+
     mul_mat_vec_q_switch_type(
         src0->data, src0->type, src1_q8_1.get(), ids_d, fusion_local, dst_d, ne00,
         ne01,              ncols_dst,     s01, stride_col_y,     stride_col_dst,
         ne02, nchannels_y, nchannels_dst, s02, stride_channel_y, stride_channel_dst,
-        ne03,              ne3,           s03, s13,              s3,               stream);
+        ne03,              ne3,           s03, s13,              s3,               ids_stride, stream);
 }
 
 void ggml_cuda_op_mul_mat_vec_q(
@@ -726,7 +829,7 @@ void ggml_cuda_op_mul_mat_vec_q(
     ggml_cuda_mm_fusion_args_device fusion_local{};
     mul_mat_vec_q_switch_type(
         src0_dd_i, src0->type, src1_ddq_i, nullptr, fusion_local, dst_dd_i, ne00, row_diff, src1_ncols, stride_row_x, stride_col_y, nrows_dst,
-        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, stream);
+        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, stream);
 
     GGML_UNUSED_VARS(src1, dst, src1_ddf_i, src1_ncols, src1_padded_row_size);
 }
diff --git a/ggml/src/ggml-cuda/quantize.cu b/ggml/src/ggml-cuda/quantize.cu
index a8c68e44b..39a6aa963 100644
--- a/ggml/src/ggml-cuda/quantize.cu
+++ b/ggml/src/ggml-cuda/quantize.cu
@@ -1,6 +1,13 @@
 #include "quantize.cuh"
 #include <cstdint>
 
+// GFX906 optimizations: DPP-based warp reductions
+#ifdef GGML_USE_HIP
+#include "gfx906/gfx906-common.cuh"
+#endif
+// #include <vector>
+// #include <cstdio>
+
 __launch_bounds__(CUDA_QUANTIZE_BLOCK_SIZE, 1)
 static __global__ void quantize_q8_1(
         const float * __restrict__ x, void * __restrict__ vy,
@@ -204,14 +211,161 @@ static __global__ void quantize_mmq_q8_1(
     const int64_t ib  = ib0 + (i0 / (4*QK8_1))*ne1 + blockIdx.x;                    // block index in channel
     const int64_t iqs = i0 % (4*QK8_1);                                             // quant index in block
 
-    // Load 4 floats per thread and calculate max. abs. value between them:
+    // Load 4 floats per thread via 128-bit vectorized load
     const float4 xi = i0 < ne00 ? x4[(i03*s03 + i02*s02 + i01*s01 + i00)/4] : make_float4(0.0f, 0.0f, 0.0f, 0.0f);
+
+    // =========================================================================
+    // GFX906 OPTIMIZATION PATH (4 values per thread)
+    // =========================================================================
+#if defined(GGML_USE_HIP) && defined(__gfx906__)
+
+    // Compute absolute values - keep in registers for reuse
+    const float ax = fabsf(xi.x);
+    const float ay = fabsf(xi.y);
+    const float az = fabsf(xi.z);
+    const float aw = fabsf(xi.w);
+
+    // Local max using tree reduction pattern (better for ILP)
+    float amax = fmaxf(fmaxf(ax, ay), fmaxf(az, aw));
+
+    // Compute sum early (for DS4 layout) - enables interleaved reduction
+    float sum = 0.0f;
+    if constexpr (ds_layout != MMQ_Q8_1_DS_LAYOUT_D4) {
+        sum = xi.x + xi.y + xi.z + xi.w;
+    }
+
+    // =========================================================================
+    // ULTRA-FUSED WARP REDUCTIONS - Single asm block, minimal NOPs
+    // Old approach: separate function calls = 12+ NOPs total
+    // New approach: fused asm block = 3 NOPs total (75% reduction!)
+    // =========================================================================
+    if constexpr (vals_per_scale == 32 && vals_per_sum == 32) {
+        // DS4 layout: Fully fused reduction for both max and sum
+        int amax_i = __float_as_int(amax);
+        int sum_i  = __float_as_int(sum);
+        int amax_tmp, sum_tmp;
+
+        asm volatile(
+            // === XOR4: row_shl:4 + row_shr:4 with bank masks ===
+            "v_mov_b32 %0, %4\n"                                              // amax_tmp = amax
+            "v_mov_b32 %1, %5\n"                                              // sum_tmp = sum
+            "s_nop 1\n"
+            "v_mov_b32_dpp %0, %4 row_shl:4 row_mask:0xf bank_mask:0x5\n"     // amax_tmp for banks 0,2
+            "v_mov_b32_dpp %1, %5 row_shl:4 row_mask:0xf bank_mask:0x5\n"     // sum_tmp for banks 0,2
+            "v_mov_b32_dpp %0, %4 row_shr:4 row_mask:0xf bank_mask:0xa\n"     // amax_tmp for banks 1,3
+            "v_mov_b32_dpp %1, %5 row_shr:4 row_mask:0xf bank_mask:0xa\n"     // sum_tmp for banks 1,3
+            // Combine xor4 results
+            "v_max_f32 %2, %4, %0\n"                                          // amax = max(amax, amax_tmp)
+            "v_add_f32 %3, %5, %1\n"                                          // sum = sum + sum_tmp
+
+            // === XOR2: quad_perm:[2,3,0,1] - fused max/add ===
+            "s_nop 1\n"
+            "v_max_f32_dpp %2, %2, %2 quad_perm:[2,3,0,1] row_mask:0xf bank_mask:0xf\n"
+            "v_add_f32_dpp %3, %3, %3 quad_perm:[2,3,0,1] row_mask:0xf bank_mask:0xf\n"
+
+            // === XOR1: quad_perm:[1,0,3,2] - fused max/add ===
+            // Using s_nop 1 instead of 4 - the operations above provide enough distance
+            "s_nop 1\n"
+            "v_max_f32_dpp %2, %2, %2 quad_perm:[1,0,3,2] row_mask:0xf bank_mask:0xf\n"
+            "v_add_f32_dpp %3, %3, %3 quad_perm:[1,0,3,2] row_mask:0xf bank_mask:0xf\n"
+
+            : "=&v"(amax_tmp), "=&v"(sum_tmp), "=v"(amax_i), "=v"(sum_i)
+            : "v"(amax_i), "v"(sum_i)
+            : "memory"
+        );
+
+        amax = __int_as_float(amax_i);
+        sum  = __int_as_float(sum_i);
+
+    } else if constexpr (vals_per_scale == 32) {
+        // D4 layout: Only max reduction needed (no sum)
+        int amax_i = __float_as_int(amax);
+        int amax_tmp;
+
+        asm volatile(
+            // === XOR4 ===
+            "v_mov_b32 %0, %2\n"
+            "s_nop 1\n"
+            "v_mov_b32_dpp %0, %2 row_shl:4 row_mask:0xf bank_mask:0x5\n"
+            "v_mov_b32_dpp %0, %2 row_shr:4 row_mask:0xf bank_mask:0xa\n"
+            "v_max_f32 %1, %2, %0\n"
+
+            // === XOR2 ===
+            "s_nop 1\n"
+            "v_max_f32_dpp %1, %1, %1 quad_perm:[2,3,0,1] row_mask:0xf bank_mask:0xf\n"
+
+            // === XOR1 ===
+            "s_nop 1\n"
+            "v_max_f32_dpp %1, %1, %1 quad_perm:[1,0,3,2] row_mask:0xf bank_mask:0xf\n"
+
+            : "=&v"(amax_tmp), "=v"(amax_i)
+            : "v"(amax_i)
+            : "memory"
+        );
+
+        amax = __int_as_float(amax_i);
+
+    } else {
+        // D2S6 layout: Different reduction widths, use generic path
+        #pragma unroll
+        for (int offset = vals_per_scale/8; offset > 0; offset >>= 1) {
+            amax = fmaxf(amax, __shfl_xor(amax, offset, WARP_SIZE));
+        }
+        if constexpr (ds_layout != MMQ_Q8_1_DS_LAYOUT_D4) {
+            #pragma unroll
+            for (int offset = vals_per_sum/8; offset > 0; offset >>= 1) {
+                sum += __shfl_xor(sum, offset, WARP_SIZE);
+            }
+        }
+    }
+
+    // =========================================================================
+    // OPTIMIZED SCALE COMPUTATION - Eliminate double reciprocal!
+    // Old: d_inv = 127 * rcp(amax), then d = rcp(d_inv) = rcp(127*rcp(amax))
+    // New: d = amax * (1/127), d_inv = rcp(d) - saves one reciprocal!
+    // =========================================================================
+    constexpr float inv_127 = 1.0f / 127.0f;  // Compile-time constant
+    const float d = amax * inv_127;           // Just a multiply
+    const float d_inv = fast_rcp_f32(d);      // Single reciprocal
+
+    // =========================================================================
+    // QUANTIZATION - Use __float2int_rn for direct conversion
+    // =========================================================================
+    char4 q;
+    q.x = static_cast<int8_t>(__float2int_rn(xi.x * d_inv));
+    q.y = static_cast<int8_t>(__float2int_rn(xi.y * d_inv));
+    q.z = static_cast<int8_t>(__float2int_rn(xi.z * d_inv));
+    q.w = static_cast<int8_t>(__float2int_rn(xi.w * d_inv));
+
+    // Write quantized values (32-bit coalesced write)
+    char4 * yqs4 = (char4 *) y[ib].qs;
+    yqs4[iqs/4] = q;
+
+    // Write scale/sum (one thread per 32-value group)
+    if constexpr (ds_layout == MMQ_Q8_1_DS_LAYOUT_D2S6) {
+        if (iqs % 16 == 0 && iqs < 96) {
+            y[ib].d2s6[2 + iqs/16] = sum;
+        }
+        if (iqs % 64 == 0) {
+            y[ib].d2s6[iqs/64] = d;
+        }
+    } else if (iqs % 32 == 0) {
+        if constexpr (ds_layout == MMQ_Q8_1_DS_LAYOUT_DS4) {
+            y[ib].ds4[iqs/32] = make_half2(d, sum);
+        } else {
+            y[ib].d4[iqs/32] = d;
+        }
+    }
+
+#else
+    // =========================================================================
+    // GENERIC PATH (non-GFX906)
+    // =========================================================================
     float amax = fabsf(xi.x);
     amax = fmaxf(amax, fabsf(xi.y));
     amax = fmaxf(amax, fabsf(xi.z));
     amax = fmaxf(amax, fabsf(xi.w));
 
-    // Exchange max. abs. value between vals_per_scale/4 threads.
 #pragma unroll
     for (int offset = vals_per_scale/8; offset > 0; offset >>= 1) {
         amax = fmaxf(amax, __shfl_xor_sync(0xFFFFFFFF, amax, offset, WARP_SIZE));
@@ -221,7 +375,6 @@ static __global__ void quantize_mmq_q8_1(
     if (ds_layout != MMQ_Q8_1_DS_LAYOUT_D4) {
         sum = xi.x + xi.y + xi.z + xi.w;
 
-        // Calculate sums across vals_per_sum/4 threads.
 #pragma unroll
         for (int offset = vals_per_sum/8; offset > 0; offset >>= 1) {
             sum += __shfl_xor_sync(0xFFFFFFFF, sum, offset, WARP_SIZE);
@@ -235,7 +388,6 @@ static __global__ void quantize_mmq_q8_1(
     q.z = roundf(xi.z*d_inv);
     q.w = roundf(xi.w*d_inv);
 
-    // Write back 4 int8 values as a single 32 bit value for better memroy bandwidth:
     char4 * yqs4 = (char4 *) y[ib].qs;
     yqs4[iqs/4] = q;
 
@@ -243,17 +395,12 @@ static __global__ void quantize_mmq_q8_1(
         if (iqs % 16 != 0 || iqs >= 96) {
             return;
         }
-
         y[ib].d2s6[2 + iqs/16] = sum;
-
         if (iqs % 64 != 0) {
             return;
         }
-
         const float d = 1.0f / d_inv;
-
         y[ib].d2s6[iqs/64] = d;
-
         return;
     }
 
@@ -268,6 +415,7 @@ static __global__ void quantize_mmq_q8_1(
     } else {
         y[ib].d4[iqs/32]  = d;
     }
+#endif
 }
 
 void quantize_row_q8_1_cuda(
@@ -293,6 +441,47 @@ void quantize_mmq_q8_1_cuda(
     GGML_ASSERT(ne00 % 4 == 0);
     GGML_ASSERT(ne0 % (4*QK8_1) == 0);
 
+    // NOTE: Tested 2x (2 values/thread) and 8x (8 values/thread) kernels.
+    // Results: 4x=25ms, 8x=31ms, 2x=41ms. 4-value kernel is optimal due to:
+    // - Balanced register pressure vs reduction count
+    // - Vectorized 128-bit loads and 32-bit writes
+
+    // --- HOST-SIDE DEBUG VALIDATION (commented out) ---
+    // fprintf(stderr, "[quantize_mmq_q8_1] ne00=%ld s01=%ld s02=%ld s03=%ld ne0=%ld ne1=%ld ne2=%ld ne3=%ld ids=%p x=%p\n",
+    //         (long)ne00, (long)s01, (long)s02, (long)s03, (long)ne0, (long)ne1, (long)ne2, (long)ne3, (void*)ids, (void*)x);
+    //
+    // // Calculate maximum index that will be accessed
+    // int64_t max_i1_val = ne1 - 1;
+    // int64_t max_src_idx = (ne3-1)*s03 + (ne2-1)*s02 + max_i1_val*s01 + (ne0-4);
+    // fprintf(stderr, "[quantize_mmq_q8_1] Grid: (%ld, %ld, %ld) max_src_idx=%ld (assuming no ids remapping)\n",
+    //         (long)ne1, (long)((ne0 + 4*CUDA_QUANTIZE_BLOCK_SIZE_MMQ - 1) / (4*CUDA_QUANTIZE_BLOCK_SIZE_MMQ)),
+    //         (long)(ne2*ne3), (long)max_src_idx);
+    //
+    // if (ids) {
+    //     // For MoE: copy ids to host and validate
+    //     std::vector<int32_t> ids_host(ne1);
+    //     CUDA_CHECK(cudaMemcpyAsync(ids_host.data(), ids, ne1 * sizeof(int32_t), cudaMemcpyDeviceToHost, stream));
+    //     CUDA_CHECK(cudaStreamSynchronize(stream));
+    //
+    //     int64_t max_ids_val = 0;
+    //     int64_t min_ids_val = INT64_MAX;
+    //     for (int64_t i = 0; i < ne1; i++) {
+    //         if (ids_host[i] > max_ids_val) max_ids_val = ids_host[i];
+    //         if (ids_host[i] < min_ids_val) min_ids_val = ids_host[i];
+    //     }
+    //     fprintf(stderr, "[quantize_mmq_q8_1] ids range: [%ld, %ld] (ne1=%ld)\n",
+    //             (long)min_ids_val, (long)max_ids_val, (long)ne1);
+    //
+    //     max_src_idx = (ne3-1)*s03 + (ne2-1)*s02 + max_ids_val*s01 + (ne0-4);
+    //     fprintf(stderr, "[quantize_mmq_q8_1] max_src_idx with ids remapping=%ld\n", (long)max_src_idx);
+    //
+    //     if (max_ids_val * s01 > 1000000000LL) {
+    //         fprintf(stderr, "[quantize_mmq_q8_1] WARNING: max_ids_val * s01 = %ld seems too large!\n",
+    //                 (long)(max_ids_val * s01));
+    //     }
+    // }
+    // --- END DEBUG VALIDATION ---
+
     // ne1 tends to assume the highest values, therefore use it as the "x" dimension of the CUDA grid:
     const int64_t block_num_y = (ne0 + 4*CUDA_QUANTIZE_BLOCK_SIZE_MMQ - 1) / (4*CUDA_QUANTIZE_BLOCK_SIZE_MMQ);
     const dim3 num_blocks(ne1, block_num_y, ne2*ne3);
diff --git a/ggml/src/ggml-cuda/rope.cu b/ggml/src/ggml-cuda/rope.cu
index 88ed79111..da9291daf 100644
--- a/ggml/src/ggml-cuda/rope.cu
+++ b/ggml/src/ggml-cuda/rope.cu
@@ -3,6 +3,11 @@
 #include "ggml.h"
 #include "rope.cuh"
 
+#ifdef GGML_USE_HIP
+#include "gfx906/attention/rope.cuh"
+#include "gfx906/gfx906-config.h"
+#endif
+
 struct rope_corr_dims {
     float v[2];
 };
@@ -365,6 +370,17 @@ static void rope_multi_cuda(
         const int32_t * pos, const float freq_scale, const float freq_base, const float ext_factor, const float attn_factor,
         const rope_corr_dims corr_dims, const float * freq_factors, const mrope_sections sections, const bool is_imrope, cudaStream_t stream) {
     GGML_ASSERT(ne0 % 2 == 0);
+
+#if defined(GGML_USE_HIP) && defined(GFX906_ROPE_ENABLED)
+    // GFX906-optimized kernel using __sincosf and precomputed theta_power
+    const gfx906_rope_corr_dims & gfx906_corr = reinterpret_cast<const gfx906_rope_corr_dims &>(corr_dims);
+    const gfx906_mrope_sections & gfx906_sects = reinterpret_cast<const gfx906_mrope_sections &>(sections);
+
+    gfx906_rope_multi_cuda<forward, T>(
+        x, dst, ne0, ne1, ne2, s1, s2, n_dims, nr,
+        pos, freq_scale, freq_base, ext_factor, attn_factor,
+        gfx906_corr, freq_factors, gfx906_sects, is_imrope, stream);
+#else
     const dim3 block_dims(1, CUDA_ROPE_BLOCK_SIZE, 1);
     const int n_blocks_x = (ne0 + 2*CUDA_ROPE_BLOCK_SIZE - 1) / (2*CUDA_ROPE_BLOCK_SIZE);
     const dim3 block_nums(nr, n_blocks_x, 1);
@@ -380,6 +396,7 @@ static void rope_multi_cuda(
             x, dst, ne0, ne1, ne2, s1, s2, n_dims, pos, freq_scale, ext_factor,
             attn_factor, corr_dims, theta_scale, freq_factors, sections, is_imrope);
     }
+#endif
 }
 
 template<bool forward, typename T>
diff --git a/ggml/src/ggml-cuda/ssm-scan.cu b/ggml/src/ggml-cuda/ssm-scan.cu
index c1d4e2bc8..6b424381d 100644
--- a/ggml/src/ggml-cuda/ssm-scan.cu
+++ b/ggml/src/ggml-cuda/ssm-scan.cu
@@ -114,7 +114,7 @@ __global__ void __launch_bounds__(splitD, 1)
 #endif // __clang__
 
 // assumes as many threads as d_state
-template <int c_factor, int d_state>
+template <int splitH, int d_state>
 __global__ void __launch_bounds__(d_state, 1)
     ssm_scan_f32_group(
         const float * __restrict__ src0, const float * __restrict__ src1, const float * __restrict__ src2,
@@ -125,25 +125,20 @@ __global__ void __launch_bounds__(d_state, 1)
         const int src4_nb2, const int src4_nb3, const int src5_nb2, const int src5_nb3,
         const int64_t s_off, const int64_t n_head, const int64_t d_head, const int64_t n_group, const int64_t n_tok) {
 
-    const int warp     = threadIdx.x / WARP_SIZE;
-    const int lane     = threadIdx.x % WARP_SIZE;
-    const int warp_idx = blockIdx.x  * c_factor + warp;
-
-    const int head_idx =  warp_idx / d_head;
-    const int head_off = (warp_idx % d_head) * sizeof(float);
-    const int seq_idx  = blockIdx.y;
+    const int head_idx = (blockIdx.x * splitH) / d_head;
+    const int head_off = ((blockIdx.x * splitH) % d_head) * sizeof(float);
+    const int seq_idx = blockIdx.y;
 
     const int group_off = (head_idx / (n_head / n_group)) * d_state * sizeof(float);
 
-    // TODO: refactor strides to be in elements/floats instead of bytes to be cleaner and consistent with the rest of the codebase
-    const float * s0_warp = (const float *) ((const char *) src0 + src6[seq_idx] * src0_nb3 + head_idx * src0_nb2 + head_off * d_state);
-    const float * x_warp  = (const float *) ((const char *) src1 + (seq_idx * src1_nb3) + (warp_idx * sizeof(float)));
-    const float * dt_warp = (const float *) ((const char *) src2 + (seq_idx * src2_nb2) + head_idx * sizeof(float));
-    const float * A_warp  = (const float *) ((const char *) src3 + head_idx * src3_nb1);
-    const float * B_warp  = (const float *) ((const char *) src4 + (seq_idx * src4_nb3) + (group_off));
-    const float * C_warp  = (const float *) ((const char *) src5 + (seq_idx * src5_nb3) + (group_off));
-    float *       y_warp  = dst + (seq_idx * n_tok * n_head * d_head) + warp_idx;
-    float *       s_warp  = (float *) ((char *) dst + s_off + seq_idx * src0_nb3 + head_idx * src0_nb2 + head_off * d_state);
+    const float * s0_block = (const float *) ((const char *) src0 + src6[seq_idx] * src0_nb3 + head_idx * src0_nb2 + head_off * d_state);
+    const float * x_block  = (const float *) ((const char *) src1 + (seq_idx * src1_nb3) + blockIdx.x * splitH * sizeof(float));
+    const float * dt_block = (const float *) ((const char *) src2 + (seq_idx * src2_nb2) + head_idx * sizeof(float));
+    const float * A_block  = (const float *) ((const char *) src3 + head_idx * src3_nb1);
+    const float * B_block  = (const float *) ((const char *) src4 + (seq_idx * src4_nb3) + (group_off));
+    const float * C_block  = (const float *) ((const char *) src5 + (seq_idx * src5_nb3) + (group_off));
+    float *       y_block  = dst + (seq_idx * n_tok * n_head * d_head) + blockIdx.x * splitH;
+    float *       s_block  = (float *) ((char *) dst + s_off + seq_idx * src0_nb3 + head_idx * src0_nb2 + head_off * d_state);
 
     // strides across n_seq_tokens
     const int stride_x  = src1_nb2 / sizeof(float);
@@ -152,42 +147,80 @@ __global__ void __launch_bounds__(d_state, 1)
     const int stride_C  = src5_nb2 / sizeof(float);
     const int stride_y  = n_head * d_head;
 
-    float state[c_factor];
-    float state_sum = 0.0f;
+    float state[splitH];
+    // for the parallel accumulation
+    __shared__ float stateC[splitH * d_state];
 
 #pragma unroll
-    for (int j = 0; j < c_factor; j++) {
-        state[j] = s0_warp[WARP_SIZE * j + lane];
+    for (int j = 0; j < splitH; j++) {
+        state[j] = s0_block[j * d_state + threadIdx.x];
     }
 
     for (int64_t i = 0; i < n_tok; i++) {
-        // NOTE: dt_soft_plus, dA and x_dt have the same value for a warp here.
-        // Recalculation is intentional; sharing via shuffles/smem proved slower due to sync overhead.
-        const float dt_soft_plus = (dt_warp[i * stride_dt] <= 20.0f ? log1pf(expf(dt_warp[i * stride_dt])) : dt_warp[i * stride_dt]);
+        // TODO: only calculate dA and dt_soft_plus once per head instead of every splitH head elements
+        // TODO: only calculate B and C once per head group
+        // NOTE: dt_soft_plus, dA and x_dt have the same value across threads here.
+        float dt_soft_plus = dt_block[i * stride_dt];
+        if (dt_soft_plus <= 20.0f) {
+            dt_soft_plus = log1pf(expf(dt_soft_plus));
+        }
+        const float dA = expf(dt_soft_plus * A_block[0]);
+        const float B = B_block[i * stride_B + threadIdx.x];
+        const float C = C_block[i * stride_C + threadIdx.x];
 
-        state_sum = 0.0f;
-        const float dA   = expf(dt_soft_plus * A_warp[0]);
-        const float x_dt = x_warp[i * stride_x] * dt_soft_plus;
+        // across d_head
 #pragma unroll
-        for (int j = 0; j < c_factor; j++) {
-            const float B_val = B_warp[i * stride_B + WARP_SIZE * j + lane];
-            const float C_val = C_warp[i * stride_C + WARP_SIZE * j + lane];
-            state[j] = (state[j] * dA) + (B_val * x_dt);
-            state_sum += state[j] * C_val;
+        for (int j = 0; j < splitH; j++) {
+            const float x_dt = x_block[i * stride_x + j] * dt_soft_plus;
+
+            state[j] = (state[j] * dA) + (B * x_dt);
+
+            stateC[j * d_state + threadIdx.x] = state[j] * C;
         }
 
-        // parallel accumulation for output
-        state_sum = warp_reduce_sum(state_sum);
+        __syncthreads();
+
+        // parallel accumulation for stateC
+        // TODO: simplify
+        {
+            static_assert((d_state & -d_state) == d_state, "the state size has to be a power of 2");
+            static_assert((splitH & -splitH) == splitH, "splitH has to be a power of 2");
+
+            // reduce until w matches the warp size
+            // TODO: does this work even when the physical warp size is 64?
+#pragma unroll
+            for (int w = d_state; w > WARP_SIZE; w >>= 1) {
+                // (assuming there are d_state threads)
+#pragma unroll
+                for (int j = 0; j < ((w >> 1) * splitH + d_state - 1) / d_state; j++) {
+                    // TODO: check for bank conflicts
+                    const int k = (threadIdx.x % (w >> 1)) + (d_state * (threadIdx.x / (w >> 1))) + j * d_state * (d_state / (w >> 1));
+                    stateC[k] += stateC[k + (w >> 1)];
+
+                }
+                __syncthreads();
+            }
+
+            static_assert(splitH >= d_state / WARP_SIZE);
 
-        if (lane == 0) {
-            y_warp[i * stride_y] = state_sum;
+#pragma unroll
+            for (int j = 0; j < splitH / (d_state / WARP_SIZE); j++) {
+                float y = stateC[(threadIdx.x % WARP_SIZE) + d_state * (threadIdx.x / WARP_SIZE) + j * d_state * (d_state / WARP_SIZE)];
+                y = warp_reduce_sum(y);
+
+                // store the above accumulations
+                if (threadIdx.x % WARP_SIZE == 0) {
+                    const int k = threadIdx.x / WARP_SIZE + j * (d_state / WARP_SIZE);
+                    y_block[i * stride_y + k] = y;
+                }
+            }
         }
     }
 
     // write back the state
 #pragma unroll
-    for (int j = 0; j < c_factor; j++) {
-        s_warp[WARP_SIZE * j + lane] = state[j];
+    for (int j = 0; j < splitH; j++) {
+        s_block[j * d_state + threadIdx.x] = state[j];
     }
 }
 
@@ -198,24 +231,27 @@ static void ssm_scan_f32_cuda(const float * src0, const float * src1, const floa
                               const int src5_nb3, const int64_t s_off, const int64_t d_state, const int64_t head_dim,
                               const int64_t n_head, const int64_t n_group, const int64_t n_tok, const int64_t n_seq,
                               cudaStream_t stream) {
+    const int threads = 128;
     // NOTE: if you change conditions here, be sure to update the corresponding supports_op condition!
     if (src3_nb1 == sizeof(float)) {
         // Mamba-2
         if (d_state == 128) {
-            constexpr int threads   = 128;
-            constexpr int num_warps = threads/WARP_SIZE;
-
-            const dim3 blocks((n_head * head_dim + (num_warps - 1)) / num_warps, n_seq, 1);
-            ssm_scan_f32_group<128/WARP_SIZE, 128><<<blocks, threads, 0, stream>>>(
+            GGML_ASSERT(d_state % threads == 0);
+            // NOTE: can be any power of two between 4 and 64
+            const int splitH = 16;
+            GGML_ASSERT(head_dim % splitH == 0);
+            const dim3 blocks((n_head * head_dim + (splitH - 1)) / splitH, n_seq, 1);
+            ssm_scan_f32_group<16, 128><<<blocks, threads, 0, stream>>>(
                     src0, src1, src2, src3, src4, src5, src6, dst,
                     src0_nb2, src0_nb3, src1_nb2, src1_nb3, src2_nb1, src2_nb2, src3_nb1,
                     src4_nb2, src4_nb3, src5_nb2, src5_nb3, s_off, n_head, head_dim, n_group, n_tok);
         } else if (d_state == 256) { // Falcon-H1
-            constexpr int threads   = 256;
-            constexpr int num_warps = threads/WARP_SIZE;
-
-            const dim3 blocks((n_head * head_dim + (num_warps - 1)) / num_warps, n_seq, 1);
-            ssm_scan_f32_group<256/WARP_SIZE, 256><<<blocks, threads, 0, stream>>>(
+            const int threads = 256;
+            // NOTE: can be any power of two between 8 and 64
+            const int splitH = 16;
+            GGML_ASSERT(head_dim % splitH == 0);
+            const dim3 blocks((n_head * head_dim + (splitH - 1)) / splitH, n_seq, 1);
+            ssm_scan_f32_group<16, 256><<<blocks, threads, 0, stream>>>(
                     src0, src1, src2, src3, src4, src5, src6, dst,
                     src0_nb2, src0_nb3, src1_nb2, src1_nb3, src2_nb1, src2_nb2, src3_nb1,
                     src4_nb2, src4_nb3, src5_nb2, src5_nb3, s_off, n_head, head_dim, n_group, n_tok);
@@ -224,7 +260,6 @@ static void ssm_scan_f32_cuda(const float * src0, const float * src1, const floa
         }
     } else {
         // Mamba-1
-        constexpr int threads = 128;
         GGML_ASSERT(n_head % threads == 0);
         GGML_ASSERT(head_dim == 1);
         GGML_ASSERT(n_group == 1);
diff --git a/ggml/src/ggml-cuda/vecdotq.cuh b/ggml/src/ggml-cuda/vecdotq.cuh
index 6baab1176..67fda32bf 100644
--- a/ggml/src/ggml-cuda/vecdotq.cuh
+++ b/ggml/src/ggml-cuda/vecdotq.cuh
@@ -4,6 +4,11 @@
 
 #include <cstdint>
 
+// GFX906 optimizations
+#if defined(GGML_USE_HIP) && defined(__gfx906__)
+    #include "gfx906/quantize/vecdotq.cuh"
+#endif
+
 static __device__ __forceinline__ int get_int_b1(const void * x, const int & i32) {
     const uint8_t * x8 = (const uint8_t *) x;
 
@@ -15,8 +20,10 @@ static __device__ __forceinline__ int get_int_b1(const void * x, const int & i32
     return x32;
 }
 
+// GFX906: get_int_b1_fast is defined in gfx906/gfx906-vecdotq.cuh as gfx906_get_int_b1_fast
+
 static __device__ __forceinline__ int get_int_b2(const void * x, const int & i32) {
-    const uint16_t * x16 = (const uint16_t *) x; // assume at least 2 byte alignment
+    const uint16_t * x16 = (const uint16_t *) x;
 
     int x32  = x16[2*i32 + 0] <<  0;
     x32     |= x16[2*i32 + 1] << 16;
@@ -24,45 +31,59 @@ static __device__ __forceinline__ int get_int_b2(const void * x, const int & i32
     return x32;
 }
 
+// GFX906: get_int_b2_fast is defined in gfx906/gfx906-vecdotq.cuh as gfx906_get_int_b2_fast
+
 static __device__ __forceinline__ int get_int_b4(const void * x, const int & i32) {
-    return ((const int *) x)[i32]; // assume at least 4 byte alignment
+    return ((const int *) x)[i32];
+}
+
+static __device__ __forceinline__ int2 get_int_from_mxfp4_table(const uint32_t q4) {
+#if defined(GGML_USE_HIP) && defined(__gfx906__)
+    // GFX906: Use optimized lookup from gfx906-vecdotq.cuh
+    return gfx906_get_int_from_mxfp4_table(q4);
+#else
+    const int      q0_32  = (q4 >> 0) & 0x0F0F0F0F;
+    const int8_t * q0_8   = (const int8_t *) &q0_32;
+    const char4    val0_8 = make_char4(
+        kvalues_mxfp4[q0_8[0]], kvalues_mxfp4[q0_8[1]], kvalues_mxfp4[q0_8[2]], kvalues_mxfp4[q0_8[3]]);
+
+    const int      q1_32  = (q4 >> 4) & 0x0F0F0F0F;
+    const int8_t * q1_8   = (const int8_t *) &q1_32;
+    const char4    val1_8 = make_char4(
+        kvalues_mxfp4[q1_8[0]], kvalues_mxfp4[q1_8[1]], kvalues_mxfp4[q1_8[2]], kvalues_mxfp4[q1_8[3]]);
+
+    return make_int2(*((const int *) &val0_8), *((const int *) &val1_8));
+#endif
 }
 
-// q4 contains 8 indices with 4 bit each.
-// This function selects those bytes from table that are at those indices and returns them as int2.
-// The first int contains the bytes with even indices in q4, the second int contains the bytes with odd indices in q4.
 static __device__ __forceinline__ int2 get_int_from_table_16(const int & q4, const int8_t * table) {
 #if defined(GGML_USE_HIP)
-    // Load the 16-byte table into four 32-bit unsigned integers.
     const uint32_t *values = (const uint32_t *)table;
 
     const uint32_t q_even = q4;
     const uint32_t q_odd  = (q4 >> 4);
 
-    // Perform lookups in the lower half of the table (indices 0-7).
-    uint32_t v_even_low = __builtin_amdgcn_perm(values[1], values[0], q_even & 0x07070707);
-    uint32_t v_odd_low = __builtin_amdgcn_perm(values[1], values[0], q_odd & 0x07070707);
+    const uint32_t sel_even = q_even & 0x07070707;
+    const uint32_t sel_odd  = q_odd & 0x07070707;
+
+    uint32_t v_even_low = __builtin_amdgcn_perm(values[1], values[0], sel_even);
+    uint32_t v_odd_low = __builtin_amdgcn_perm(values[1], values[0], sel_odd);
+    uint32_t v_even_high = __builtin_amdgcn_perm(values[3], values[2], sel_even);
+    uint32_t v_odd_high = __builtin_amdgcn_perm(values[3], values[2], sel_odd);
+
+    uint32_t b3e = (q_even >> 3) & 0x01010101;
+    uint32_t me = b3e; me |= me << 1; me |= me << 2; me |= me << 4;
 
-    // Perform lookups in the upper half of the table (indices 8-15).
-    uint32_t v_even_high = __builtin_amdgcn_perm(values[3], values[2], q_even & 0x07070707);
-    uint32_t v_odd_high = __builtin_amdgcn_perm(values[3], values[2], q_odd & 0x07070707);
+    uint32_t b3o = (q_odd >> 3) & 0x01010101;
+    uint32_t mo = b3o; mo |= mo << 1; mo |= mo << 2; mo |= mo << 4;
 
-    // Select between the low and high results based on the MSB of each index nibble.
-    uint32_t mask_even = 0x03020100 | ((q_even & 0x08080808) >> 1);
-    uint32_t res_x = __builtin_amdgcn_perm(v_even_high, v_even_low, mask_even);
-    uint32_t mask_odd = 0x03020100 | ((q_odd & 0x08080808) >> 1);
-    uint32_t res_y = __builtin_amdgcn_perm(v_odd_high, v_odd_low, mask_odd);
+    uint32_t res_x = (v_even_high & me) | (v_even_low & ~me);
+    uint32_t res_y = (v_odd_high & mo) | (v_odd_low & ~mo);
 
     return make_int2(res_x, res_y);
 #elif !defined(GGML_USE_MUSA)
-    // CUDA does not have an instruction for selecting bytes with 4 bit indices.
-    // However, __byte_perm is an instruction that selects bytes with 3 bit indices that can be used instead.
     const uint32_t * table32 = (const uint32_t *) table;
 
-    // __byte_perm selects bytes based on the lower 16 bits in its third argument.
-    // Therefore, do 2 iterations over the 32 bits in q4 with 0 and 16 shift.
-    // To handle the fourth bit, first call _byte_perm both for the low and the high 64 bit of table, using the low 3 bits.
-    // Then, call __byte_perm again to select from the low and high bytes based on the fourth bit.
     uint32_t tmp[2];
     const uint32_t low_high_selection_indices = (0x32103210 | ((q4 & 0x88888888) >> 1));
 #pragma unroll
@@ -74,12 +95,8 @@ static __device__ __forceinline__ int2 get_int_from_table_16(const int & q4, con
         tmp[i] = __byte_perm(low, high, low_high_selection_indices >> shift);
     }
 
-    // tmp contains the bytes from tyble in the same order as the 4 bit indices in q4.
-    // However, for the result we need ints with all even/odd 4 bit indices in q4.
-    // Therefore, 2 more calls to __byte_perm to put the bytes in the correct order.
     return make_int2(__byte_perm(tmp[0], tmp[1], 0x6420), __byte_perm(tmp[0], tmp[1], 0x7531));
 #else
-    // Generic implementation.
     const int      q0_32  = (q4 >> 0) & 0x0F0F0F0F;
     const int8_t * q0_8   = (const int8_t *) &q0_32;
     const char4    val0_8 = make_char4(
@@ -94,9 +111,6 @@ static __device__ __forceinline__ int2 get_int_from_table_16(const int & q4, con
 #endif
 }
 
-// VDR = vec dot ratio, how many contiguous integers each thread processes when the vec dot kernel is called
-// MMVQ = mul_mat_vec_q, MMQ = mul_mat_q
-
 #define VDR_Q4_0_Q8_1_MMVQ 2
 #define VDR_Q4_0_Q8_1_MMQ  4
 
@@ -110,14 +124,12 @@ template <int vdr> static __device__ __forceinline__ float vec_dot_q4_0_q8_1_imp
         const int vi0 = (v[i] >> 0) & 0x0F0F0F0F;
         const int vi1 = (v[i] >> 4) & 0x0F0F0F0F;
 
-        // SIMD dot product of quantized values
         sumi = ggml_cuda_dp4a(vi0, u[2*i+0], sumi);
         sumi = ggml_cuda_dp4a(vi1, u[2*i+1], sumi);
     }
 
     const float2 ds8f = __half22float2(ds8);
 
-    // second part effectively subtracts 8 from each quant value
     return d4 * (sumi * ds8f.x - (8*vdr/QI4_0) * ds8f.y);
 }
 
@@ -134,7 +146,6 @@ template <int vdr> static __device__ __forceinline__ float vec_dot_q4_1_q8_1_imp
         const int vi0 = (v[i] >> 0) & 0x0F0F0F0F;
         const int vi1 = (v[i] >> 4) & 0x0F0F0F0F;
 
-        // SIMD dot product of quantized values
         sumi = ggml_cuda_dp4a(vi0, u[2*i+0], sumi);
         sumi = ggml_cuda_dp4a(vi1, u[2*i+1], sumi);
     }
@@ -148,9 +159,7 @@ template <int vdr> static __device__ __forceinline__ float vec_dot_q4_1_q8_1_imp
     const float2 ds8f = __half22float2(ds8);
     const float d4d8 = dm4f.x * ds8f.x;
     const float m4s8 = dm4f.y * ds8f.y;
-#endif // FAST_FP16_AVAILABLE
-
-    // scale second part of sum by QI8_1/(vdr * QR4_1) to compensate for multiple threads adding it
+#endif
     return sumi * d4d8 + m4s8 / (QI8_1 / (vdr * QR4_1));
 }
 
@@ -164,24 +173,23 @@ template <int vdr> static __device__ __forceinline__ float vec_dot_q5_0_q8_1_imp
 
 #pragma unroll
     for (int i = 0; i < vdr; ++i) {
-        int vi0 = (vl[i] >>  0) & 0x0F0F0F0F; // lower 4 qs bits, still need qh as 5th bits
-        vi0    |= (vh[i] <<  4) & 0x00000010; // 0 ->  4
-        vi0    |= (vh[i] << 11) & 0x00001000; // 1 -> 12
-        vi0    |= (vh[i] << 18) & 0x00100000; // 2 -> 20
-        vi0    |= (vh[i] << 25) & 0x10000000; // 3 -> 28
-        sumi = ggml_cuda_dp4a(vi0, u[2*i+0], sumi); // SIMD dot product of quantized values
-
-        int vi1 = (vl[i] >>  4) & 0x0F0F0F0F; // upper 4 qs bits, still need qh as 5th bits
-        vi1    |= (vh[i] >> 12) & 0x00000010; // 16 ->  4
-        vi1    |= (vh[i] >>  5) & 0x00001000; // 17 -> 12
-        vi1    |= (vh[i] <<  2) & 0x00100000; // 18 -> 20
-        vi1    |= (vh[i] <<  9) & 0x10000000; // 19 -> 28
-        sumi = ggml_cuda_dp4a(vi1, u[2*i+1], sumi); // SIMD dot product of quantized values
+        int vi0 = (vl[i] >>  0) & 0x0F0F0F0F;
+        vi0    |= (vh[i] <<  4) & 0x00000010;
+        vi0    |= (vh[i] << 11) & 0x00001000;
+        vi0    |= (vh[i] << 18) & 0x00100000;
+        vi0    |= (vh[i] << 25) & 0x10000000;
+        sumi = ggml_cuda_dp4a(vi0, u[2*i+0], sumi);
+
+        int vi1 = (vl[i] >>  4) & 0x0F0F0F0F;
+        vi1    |= (vh[i] >> 12) & 0x00000010;
+        vi1    |= (vh[i] >>  5) & 0x00001000;
+        vi1    |= (vh[i] <<  2) & 0x00100000;
+        vi1    |= (vh[i] <<  9) & 0x10000000;
+        sumi = ggml_cuda_dp4a(vi1, u[2*i+1], sumi);
     }
 
     const float2 ds8f = __half22float2(ds8);
 
-    // second part effectively subtracts 16 from each quant value
     return d5 * (sumi * ds8f.x - (16*vdr/QI5_0) * ds8f.y);
 }
 
@@ -195,19 +203,19 @@ template <int vdr> static __device__ __forceinline__ float vec_dot_q5_1_q8_1_imp
 
 #pragma unroll
     for (int i = 0; i < vdr; ++i) {
-        int vi0 = (vl[i] >>  0) & 0x0F0F0F0F; // lower 4 qs bits, still need qh as 5th bits
-        vi0    |= (vh[i] <<  4) & 0x00000010; // 0 ->  4
-        vi0    |= (vh[i] << 11) & 0x00001000; // 1 -> 12
-        vi0    |= (vh[i] << 18) & 0x00100000; // 2 -> 20
-        vi0    |= (vh[i] << 25) & 0x10000000; // 3 -> 28
-        sumi = ggml_cuda_dp4a(vi0, u[2*i+0], sumi); // SIMD dot product of quantized values
-
-        int vi1 = (vl[i] >>  4) & 0x0F0F0F0F; // upper 4 qs bits, still need qh as 5th bits
-        vi1    |= (vh[i] >> 12) & 0x00000010; // 16 ->  4
-        vi1    |= (vh[i] >>  5) & 0x00001000; // 17 -> 12
-        vi1    |= (vh[i] <<  2) & 0x00100000; // 18 -> 20
-        vi1    |= (vh[i] <<  9) & 0x10000000; // 19 -> 28
-        sumi = ggml_cuda_dp4a(vi1, u[2*i+1], sumi); // SIMD dot product of quantized values
+        int vi0 = (vl[i] >>  0) & 0x0F0F0F0F;
+        vi0    |= (vh[i] <<  4) & 0x00000010;
+        vi0    |= (vh[i] << 11) & 0x00001000;
+        vi0    |= (vh[i] << 18) & 0x00100000;
+        vi0    |= (vh[i] << 25) & 0x10000000;
+        sumi = ggml_cuda_dp4a(vi0, u[2*i+0], sumi);
+
+        int vi1 = (vl[i] >>  4) & 0x0F0F0F0F;
+        vi1    |= (vh[i] >> 12) & 0x00000010;
+        vi1    |= (vh[i] >>  5) & 0x00001000;
+        vi1    |= (vh[i] <<  2) & 0x00100000;
+        vi1    |= (vh[i] <<  9) & 0x10000000;
+        sumi = ggml_cuda_dp4a(vi1, u[2*i+1], sumi);
     }
 
 #ifdef FAST_FP16_AVAILABLE
@@ -219,9 +227,7 @@ template <int vdr> static __device__ __forceinline__ float vec_dot_q5_1_q8_1_imp
     const float2 ds8f = __half22float2(ds8);
     const float d5d8 = dm5f.x * ds8f.x;
     const float m5s8 = dm5f.y * ds8f.y;
-#endif // FAST_FP16_AVAILABLE
-
-    // scale second part of sum by QI5_1 / vdr to compensate for multiple threads adding it
+#endif
     return sumi*d5d8 + m5s8 / (QI5_1 / vdr);
 }
 
@@ -235,7 +241,6 @@ template <typename T, int vdr> static __device__ __forceinline__ T vec_dot_q8_0_
 
 #pragma unroll
     for (int i = 0; i < vdr; ++i) {
-        // SIMD dot product of quantized values
         sumi = ggml_cuda_dp4a(v[i], u[i], sumi);
     }
 
@@ -249,7 +254,6 @@ template <int vdr> static __device__ __forceinline__ float vec_dot_q8_1_q8_1_imp
 
 #pragma unroll
     for (int i = 0; i < vdr; ++i) {
-        // SIMD dot product of quantized values
         sumi = ggml_cuda_dp4a(v[i], u[i], sumi);
     }
 
@@ -262,9 +266,7 @@ template <int vdr> static __device__ __forceinline__ float vec_dot_q8_1_q8_1_imp
     const float2 ds8f = __half22float2(ds8);
     const float d8d8 = dm8f.x * ds8f.x;
     const float m8s8 = dm8f.y * ds8f.y;
-#endif // FAST_FP16_AVAILABLE
-
-    // scale second part of sum by QI8_1/ vdr to compensate for multiple threads adding it
+#endif
     return sumi*d8d8 + m8s8 / (QI8_1 / vdr);
 }
 
@@ -279,7 +281,6 @@ template <int vdr> static __device__ __forceinline__ float vec_dot_q8_0_16_q8_1_
 
 #pragma unroll
         for (int i = i0; i < i0 + QI8_0/2; ++i) {
-            // SIMD dot product of quantized values
             sumi = ggml_cuda_dp4a(v[i], u[i], sumi);
         }
 
@@ -297,17 +298,22 @@ static __device__ __forceinline__ float vec_dot_mxfp4_q8_1(
 
     const block_mxfp4 * bq4 = (const block_mxfp4 *) vbq + kbx;
 
+#if defined(GGML_USE_HIP) && defined(__gfx906__)
+    // GFX906: Use software pipelined version from gfx906-vecdotq.cuh
+    int sumi = 0;
+    GFX906_VEC_DOT_MXFP4_Q8_1(bq4, bq8_1, iqs, sumi);
+#else
     const int * q8 = (const int *) bq8_1->qs + iqs;
-
     int sumi = 0;
 #pragma unroll
     for (int l = 0; l < VDR_MXFP4_Q8_1_MMVQ; ++l) {
         const int aux_q4 = get_int_b1(bq4->qs, iqs + l);
-        const int2 v = get_int_from_table_16(aux_q4, kvalues_mxfp4);
+        const int2 v = get_int_from_mxfp4_table(aux_q4);
 
         sumi = ggml_cuda_dp4a(v.x, q8[l + 0], sumi);
         sumi = ggml_cuda_dp4a(v.y, q8[l + 4], sumi);
     }
+#endif
 
     const float d = ggml_cuda_e8m0_to_fp32(bq4->e) * 0.5f * __low2float(bq8_1->ds);
     return d * sumi;
@@ -316,7 +322,6 @@ static __device__ __forceinline__ float vec_dot_mxfp4_q8_1(
 #define VDR_Q2_K_Q8_1_MMVQ 1
 #define VDR_Q2_K_Q8_1_MMQ  4
 
-// contiguous v/x values
 static __device__ __forceinline__ float vec_dot_q2_K_q8_1_impl_mmvq(
     const int & v, const int * __restrict__ u, const uint8_t * __restrict__ scales,
     const half2 & dm2, const float * __restrict__ d8) {
@@ -330,13 +335,12 @@ static __device__ __forceinline__ float vec_dot_q2_K_q8_1_impl_mmvq(
 
         const int vi = (v >> (2*i)) & 0x03030303;
 
-        sumf_d += d8[i] * (ggml_cuda_dp4a(vi, u[i], 0) * (sc & 0xF)); // SIMD dot product
+        sumf_d += d8[i] * (ggml_cuda_dp4a(vi, u[i], 0) * (sc & 0xF));
 
-        // fill int with 4x m
         int m = sc >> 4;
         m |= m <<  8;
         m |= m << 16;
-        sumf_m += d8[i] * ggml_cuda_dp4a(m, u[i], 0); // multiply constant q2_K part with sum of q8_1 values
+        sumf_m += d8[i] * ggml_cuda_dp4a(m, u[i], 0);
     }
 
     const float2 dm2f = __half22float2(dm2);
@@ -344,7 +348,6 @@ static __device__ __forceinline__ float vec_dot_q2_K_q8_1_impl_mmvq(
     return dm2f.x*sumf_d - dm2f.y*sumf_m;
 }
 
-// contiguous v/x + u/y values
 template <int ns8>
 static __device__ __forceinline__ float vec_dot_q2_K_q8_1_impl_mmq(
     const int * __restrict__ v, const int * __restrict__ u, const half2 * dm2, const float & d8, const half2 * s8) {
@@ -399,7 +402,6 @@ static __device__ __forceinline__ float vec_dot_q2_K_q8_1_impl_mmq(
 #define VDR_Q3_K_Q8_1_MMVQ 1
 #define VDR_Q3_K_Q8_1_MMQ  2
 
-// contiguous v/x values
 static __device__ __forceinline__ float vec_dot_q3_K_q8_1_impl_mmvq(
     const int & vl, const int & vh, const int * __restrict__ u, const uint8_t * __restrict__ scales,
     const int & scale_offset, const float & d3, const float * __restrict__ d8) {
@@ -426,13 +428,13 @@ static __device__ __forceinline__ float vec_dot_q3_K_q8_1_impl_mmvq(
 
         const int vi = __vsubss4(vil, vih);
 
-        sumf += d8[i] * (ggml_cuda_dp4a(vi, u[i], 0) * sc); // SIMD dot product
+        sumf += d8[i] * (ggml_cuda_dp4a(vi, u[i], 0) * sc);
+
     }
 
     return d3 * sumf;
 }
 
-// contiguous v/x + u/y values
 static __device__ __forceinline__ float vec_dot_q3_K_q8_1_impl_mmq(
     const int * __restrict__ v, const int * __restrict__ u, const int8_t * __restrict__ scales,
     const float & d3, const float & d8) {
@@ -445,7 +447,7 @@ static __device__ __forceinline__ float vec_dot_q3_K_q8_1_impl_mmq(
 
 #pragma unroll
         for (int i = i0; i < i0 + QI8_1/2; ++i) {
-            sumi_sc = ggml_cuda_dp4a(v[i], u[i], sumi_sc); // SIMD dot product
+            sumi_sc = ggml_cuda_dp4a(v[i], u[i], sumi_sc);
         }
 
         sumi += sumi_sc * scales[i0 / (QI8_1/2)];
@@ -457,7 +459,6 @@ static __device__ __forceinline__ float vec_dot_q3_K_q8_1_impl_mmq(
 #define VDR_Q4_K_Q8_1_MMVQ 2
 #define VDR_Q4_K_Q8_1_MMQ  8
 
-// contiguous v/x values
 static __device__ __forceinline__ float vec_dot_q4_K_q8_1_impl_vmmq(
     const int * __restrict__ v, const int * __restrict__ u, const uint8_t * __restrict__ sc,
     const uint8_t * __restrict__ m, const half2 & dm4, const float * __restrict__ d8) {
@@ -470,11 +471,11 @@ static __device__ __forceinline__ float vec_dot_q4_K_q8_1_impl_vmmq(
         const int v0i = (v[0] >> (4*i)) & 0x0F0F0F0F;
         const int v1i = (v[1] >> (4*i)) & 0x0F0F0F0F;
 
-        const int dot1 = ggml_cuda_dp4a(v1i, u[2*i+1], ggml_cuda_dp4a(v0i, u[2*i+0], 0)); // SIMD dot product
-        const int dot2 = ggml_cuda_dp4a(0x01010101, u[2*i+1], ggml_cuda_dp4a(0x01010101, u[2*i+0], 0)); // sum of u
+        const int dot1 = ggml_cuda_dp4a(v1i, u[2*i+1], ggml_cuda_dp4a(v0i, u[2*i+0], 0));
+        const int dot2 = ggml_cuda_dp4a(0x01010101, u[2*i+1], ggml_cuda_dp4a(0x01010101, u[2*i+0], 0));
 
         sumf_d += d8[i] * (dot1 * sc[i]);
-        sumf_m += d8[i] * (dot2 * m[i]);  // multiply constant part of q4_K with sum of q8_1 values
+        sumf_m += d8[i] * (dot2 * m[i]);
     }
 
     const float2 dm4f = __half22float2(dm4);
@@ -482,7 +483,6 @@ static __device__ __forceinline__ float vec_dot_q4_K_q8_1_impl_vmmq(
     return dm4f.x*sumf_d - dm4f.y*sumf_m;
 }
 
-// contiguous v/x + u/y values
 static __device__ __forceinline__ float vec_dot_q4_K_q8_1_impl_mmq(
     const int * __restrict__ v, const int * __restrict__ u, const uint8_t * __restrict__ sc,
     const uint8_t * __restrict__ m, const half2 & dm4, const half2 * __restrict__ ds8) {
@@ -496,13 +496,13 @@ static __device__ __forceinline__ float vec_dot_q4_K_q8_1_impl_mmq(
 
 #pragma unroll
         for (int j = 0; j < QI8_1; ++j) {
-            sumi_d = ggml_cuda_dp4a((v[j] >> (4*i)) & 0x0F0F0F0F, u[i*QI8_1 + j], sumi_d); // SIMD dot product
+            sumi_d = ggml_cuda_dp4a((v[j] >> (4*i)) & 0x0F0F0F0F, u[i*QI8_1 + j], sumi_d);
         }
 
         const float2 ds8f = __half22float2(ds8[i]);
 
         sumf_d += ds8f.x * (sc[i] * sumi_d);
-        sumf_m += ds8f.y *   m[i]; // sum of q8_1 block * q4_K min val
+        sumf_m += ds8f.y *   m[i];
     }
 
     const float2 dm4f = __half22float2(dm4);
@@ -513,7 +513,6 @@ static __device__ __forceinline__ float vec_dot_q4_K_q8_1_impl_mmq(
 #define VDR_Q5_K_Q8_1_MMVQ 2
 #define VDR_Q5_K_Q8_1_MMQ  8
 
-// contiguous v/x values
 static __device__ __forceinline__ float vec_dot_q5_K_q8_1_impl_vmmq(
     const int * __restrict__ vl, const int * __restrict__ vh, const int * __restrict__ u, const uint8_t * __restrict__ sc,
     const uint8_t * __restrict__ m, const half2 & dm5, const float * __restrict__ d8) {
@@ -532,8 +531,8 @@ static __device__ __forceinline__ float vec_dot_q5_K_q8_1_impl_vmmq(
         const int v0i = vl0i | vh0i;
         const int v1i = vl1i | vh1i;
 
-        const int dot1 = ggml_cuda_dp4a(v0i, u[2*i+0], ggml_cuda_dp4a(v1i, u[2*i+1], 0)); // SIMD dot product
-        const int dot2 = ggml_cuda_dp4a(0x01010101, u[2*i+0], ggml_cuda_dp4a(0x01010101, u[2*i+1], 0)); // sum of u
+        const int dot1 = ggml_cuda_dp4a(v0i, u[2*i+0], ggml_cuda_dp4a(v1i, u[2*i+1], 0));
+        const int dot2 = ggml_cuda_dp4a(0x01010101, u[2*i+0], ggml_cuda_dp4a(0x01010101, u[2*i+1], 0));
 
         sumf_d += d8[i] * (dot1 * sc[i]);
         sumf_m += d8[i] * (dot2 * m[i]);
@@ -545,7 +544,6 @@ static __device__ __forceinline__ float vec_dot_q5_K_q8_1_impl_vmmq(
     return dm5f.x*sumf_d - dm5f.y*sumf_m;
 }
 
-// contiguous v/x + u/y values
 static __device__ __forceinline__ float vec_dot_q5_K_q8_1_impl_mmq(
     const int * __restrict__ v, const int * __restrict__ u, const uint8_t * __restrict__ sc,
     const uint8_t * __restrict__ m, const half2 & dm4, const half2 * __restrict__ ds8) {
@@ -559,13 +557,13 @@ static __device__ __forceinline__ float vec_dot_q5_K_q8_1_impl_mmq(
 
 #pragma unroll
         for (int j = 0; j < QI8_1; ++j) {
-            sumi_d = ggml_cuda_dp4a(v[i*QI8_1 + j], u[i*QI8_1 + j], sumi_d); // SIMD dot product
+            sumi_d = ggml_cuda_dp4a(v[i*QI8_1 + j], u[i*QI8_1 + j], sumi_d);
         }
 
         const float2 ds8f = __half22float2(ds8[i]);
 
         sumf_d += ds8f.x * (sc[i] * sumi_d);
-        sumf_m += ds8f.y *   m[i]; // sum of q8_1 block * q4_K min val
+        sumf_m += ds8f.y *   m[i];
     }
 
     const float2 dm4f = __half22float2(dm4);
@@ -576,7 +574,6 @@ static __device__ __forceinline__ float vec_dot_q5_K_q8_1_impl_mmq(
 #define VDR_Q6_K_Q8_1_MMVQ 1
 #define VDR_Q6_K_Q8_1_MMQ  8
 
-// contiguous v/x values
 static __device__ __forceinline__ float vec_dot_q6_K_q8_1_impl_mmvq(
     const int & vl, const int & vh, const int * __restrict__ u, const int8_t * __restrict__ scales,
     const float & d, const float * __restrict__ d8) {
@@ -591,15 +588,14 @@ static __device__ __forceinline__ float vec_dot_q6_K_q8_1_impl_mmvq(
 
         const int vih = ((vh >> (4*i)) << 4) & 0x30303030;
 
-        const int vi = __vsubss4((vil | vih), 0x20202020); // vi = (vil | vih) - 32
+        const int vi = __vsubss4((vil | vih), 0x20202020);
 
-        sumf += d8[i] * (ggml_cuda_dp4a(vi, u[i], 0) * sc); // SIMD dot product
+        sumf += d8[i] * (ggml_cuda_dp4a(vi, u[i], 0) * sc);
     }
 
     return d*sumf;
 }
 
-// contiguous v/x + u/y values
 static __device__ __forceinline__ float vec_dot_q6_K_q8_1_impl_mmq(
     const int * __restrict__ v, const int * __restrict__ u, const int8_t * __restrict__ sc,
     const float & d6, const float * __restrict__ d8) {
@@ -611,15 +607,15 @@ static __device__ __forceinline__ float vec_dot_q6_K_q8_1_impl_mmq(
 
 #pragma unroll
     for (int i0 = 0; i0 < VDR_Q6_K_Q8_1_MMQ; i0 += 4) {
-        int2 sumi_d = {0, 0}; // 2 q6_K scales per q8_1 scale
+        int2 sumi_d = {0, 0};
 
 #pragma unroll
         for (int i = i0; i < i0 + 2; ++i) {
-            sumi_d.x = ggml_cuda_dp4a(v[2*i+0], u[2*i+0], sumi_d.x); // SIMD dot product
-            sumi_d.x = ggml_cuda_dp4a(v[2*i+1], u[2*i+1], sumi_d.x); // SIMD dot product
+            sumi_d.x = ggml_cuda_dp4a(v[2*i+0], u[2*i+0], sumi_d.x);
+            sumi_d.x = ggml_cuda_dp4a(v[2*i+1], u[2*i+1], sumi_d.x);
 
-            sumi_d.y = ggml_cuda_dp4a(v[2*i+4], u[2*i+4], sumi_d.y); // SIMD dot product
-            sumi_d.y = ggml_cuda_dp4a(v[2*i+5], u[2*i+5], sumi_d.y); // SIMD dot product
+            sumi_d.y = ggml_cuda_dp4a(v[2*i+4], u[2*i+4], sumi_d.y);
+            sumi_d.y = ggml_cuda_dp4a(v[2*i+5], u[2*i+5], sumi_d.y);
         }
 
         sumf_d += d8[i0/4] * (sc_reg[i0/2+0]*sumi_d.x + sc_reg[i0/2+1]*sumi_d.y);
@@ -715,7 +711,11 @@ static __device__ __forceinline__ float vec_dot_q8_0_q8_1(
 
 #pragma unroll
     for (int i = 0; i < VDR_Q8_0_Q8_1_MMVQ; ++i) {
+#if defined(GGML_USE_HIP) && defined(__gfx906__)
+        v[i] = gfx906_get_int_b2_fast(bq8_0->qs, iqs + i);
+#else
         v[i] = get_int_b2(bq8_0->qs, iqs + i);
+#endif
         u[i] = get_int_b4(bq8_1->qs, iqs + i);
     }
 
@@ -757,7 +757,6 @@ static __device__ __forceinline__ float vec_dot_q3_K_q8_1(
 
     const int vl = get_int_b2(bq3_K->qs, iqs);
 
-    // invert the mask with ~ so that a 0/1 results in 4/0 being subtracted
     const int vh = ~get_int_b2(bq3_K->hmask, iqs % (QI3_K/2)) >> bq8_offset;
 
     int    u[QR3_K];
@@ -781,14 +780,8 @@ static __device__ __forceinline__ float vec_dot_q4_K_q8_1(
     int    u[2*QR4_K];
     float d8[QR4_K];
 
-    // iqs is in 0,2..30. bq8_offset = iqs/4 -> bq8_offset = 0, 2, 4, 6
     const int bq8_offset = QR4_K * ((iqs/2) / (QI8_1/2));
 
-    // iqs = 0....3 -> bq8_offset = 0, want q4_offset = 0, 4, 8, 12
-    // iqs = 4....7 -> bq8_offset = 2, want q4_offset = 32, 36, 40, 44
-    // iqs = 8...11 -> bq8_offset = 4, want q4_offset = 64, 68, 72, 76
-    // iqs = 12..15 -> bq8_offset = 6, want q4_offset = 96, 100, 104, 108
-
     const int * q4 = (const int *)(bq4_K->qs + 16 * bq8_offset + 4 * ((iqs/2)%4));
     v[0] = q4[0];
     v[1] = q4[4];
@@ -1050,7 +1043,6 @@ static __device__ __forceinline__ float vec_dot_iq3_xxs_q8_1(
 #define VDR_IQ3_S_Q8_1_MMVQ 2
 #define VDR_IQ3_S_Q8_1_MMQ  2
 
-// TODO: don't use lookup table for signs
 static __device__ __forceinline__ float vec_dot_iq3_s_q8_1(
     const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & kbx, const int & iqs) {
 
diff --git a/scripts/bench-models.sh b/scripts/bench-models.sh
old mode 100644
new mode 100755
index 744b0de35..c24101304
--- a/scripts/bench-models.sh
+++ b/scripts/bench-models.sh
@@ -7,47 +7,54 @@ ARGS_BB="-c 270336 -npp 512,4096,8192 -npl 1,2,4,8,16,32 -ntg 32"
 ARGS_B="-d 0,4096,8192,16384,32768 -p 2048 -n 32"
 
 QUICK=0
+DIO=0
 while (( "$#" )); do
-  case "$1" in
-    --quick) QUICK=1; shift ;;
-    *) shift ;;
-  esac
+    case "$1" in
+        --quick) QUICK=1; shift ;;
+        --dio) DIO=1; shift ;;
+        *) shift ;;
+    esac
 done
 
 if (( QUICK )); then
-  ARGS_BB="-c 20480 -npp 512,4096 -npl 1,2,4 -ntg 32"
-  ARGS_B="-d 0 -p 2048 -n 32"
+    ARGS_BB="-c 20480 -npp 512,4096 -npl 1,2,4 -ntg 32"
+    ARGS_B="-d 0 -p 2048 -n 32"
+fi
+
+if (( DIO )); then
+    ARGS_BB="${ARGS_BB} --no-mmap --direct-io"
+    ARGS_B="${ARGS_B} -mmp 0 -dio 1"
 fi
 
 run_model() {
-  local HFR=$1
-  local HFF=$2
+    local HFR=$1
+    local HFF=$2
 
-  printf "## ${HFR}\n" | tee -a "$RESULTS"
-  printf "\n" | tee -a "$RESULTS"
-  printf "Model: https://huggingface.co/${HFR}\n" | tee -a "$RESULTS"
-  printf "\n" | tee -a "$RESULTS"
+    printf "## ${HFR}\n" | tee -a "$RESULTS"
+    printf "\n" | tee -a "$RESULTS"
+    printf "Model: https://huggingface.co/${HFR}\n" | tee -a "$RESULTS"
+    printf "\n" | tee -a "$RESULTS"
 
-  printf -- "- \`llama-batched-bench\`\n" | tee -a "$RESULTS"
-  printf "\n" | tee -a "$RESULTS"
+    printf -- "- \`llama-batched-bench\`\n" | tee -a "$RESULTS"
+    printf "\n" | tee -a "$RESULTS"
 
-  ./bin/llama-batched-bench \
-    -hfr "${HFR}" -hff "${HFF}" \
-    -m "${HFF}" -fa 1 -ub 2048 --no-mmap \
-    ${ARGS_BB} | tee -a "$RESULTS"
+    ./bin/llama-batched-bench \
+        -hfr "${HFR}" -hff "${HFF}" \
+        -m "${HFF}" -fa 1 -ub 2048 \
+        ${ARGS_BB} | tee -a "$RESULTS"
 
-  printf "\n" | tee -a "$RESULTS"
+    printf "\n" | tee -a "$RESULTS"
 
-  printf -- "- \`llama-bench\`\n" | tee -a "$RESULTS"
-  printf "\n" | tee -a "$RESULTS"
+    printf -- "- \`llama-bench\`\n" | tee -a "$RESULTS"
+    printf "\n" | tee -a "$RESULTS"
 
-  ./bin/llama-bench \
-    -m "${HFF}" -fa 1 -ub 2048 -mmp 0 \
-    ${ARGS_B} | tee -a "$RESULTS"
+    ./bin/llama-bench \
+        -m "${HFF}" -fa 1 -ub 2048 \
+        ${ARGS_B} | tee -a "$RESULTS"
 
-  printf "\n" | tee -a "$RESULTS"
+    printf "\n" | tee -a "$RESULTS"
 
-  printf "\n"
+    printf "\n"
 }
 
 run_model "ggml-org/gpt-oss-20b-GGUF"                       "gpt-oss-20b-mxfp4.gguf"
@@ -55,6 +62,7 @@ run_model "ggml-org/gpt-oss-120b-GGUF"                      "gpt-oss-120b-mxfp4-
 run_model "ggml-org/Qwen3-Coder-30B-A3B-Instruct-Q8_0-GGUF" "qwen3-coder-30b-a3b-instruct-q8_0.gguf"
 run_model "ggml-org/Qwen2.5-Coder-7B-Q8_0-GGUF"             "qwen2.5-coder-7b-q8_0.gguf"
 run_model "ggml-org/gemma-3-4b-it-qat-GGUF"                 "gemma-3-4b-it-qat-Q4_0.gguf"
+run_model "ggml-org/GLM-4.7-Flash-GGUF"                     "GLM-4.7-Flash-Q8_0.gguf"
 
 if [[ -f models-extra.txt ]]; then
     while read -r HFR HFF; do
